---
title: "Temporal variability in Lake Champlain basins"
author: "Rosalie Bruel"
date: "24/01/2019"
output: 
  html_document: 
    df_print: paged
    fig_caption: yes
    toc: yes
    toc_depth: 3
    toc_float: true
    number_sections: true
editor_options: 
  chunk_output_type: console
  df_print: paged
fontsize: 11pt
---

_Last updated: `r Sys.Date()`_


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

A significant advantage to using Lake Champlain to compare food web models is that the lake is divided (by causeways) into four trophically different basins. The **Main Lake** is deep (122 m) and meso-oligotrophic, **Malletts Bay** is moderately deep (50 m) and oligotrophic, and the **Inland Sea** is moderately deep (35 m) and meso-eutrophic, whereas **Missisquoi Bay** is shallow (7 m) and highly eutrophic. Therefore, comparisons can be made among models and among different trophic systems within the same lake. For example, coldwater fish species avoid Missisquoi Bay, and stocked lake trout avoid the Inland Sea in summer, resulting in lower pressures on the forage base and different upper food webs compared to the Main Lake. Forage fish monitoring has been conducted in all of the basins except Missisquoi Bay; long-term ecological monitoring stations are present in all basins.


```{r load packages and data, include=FALSE}
# Load packages
# Give the path of a specific script I want to load
pkgTest <- function(x)
{
  if (!require(x,character.only = TRUE))
  {
    install.packages(x,dep=TRUE)
    if(!require(x,character.only = TRUE)) stop("Package not found")
  }
}

pkgTest("lubridate") # Handle different date time format
# also used to get the day number in a year, e.g. January 1st is day 1
# https://stackoverflow.com/questions/25463523/convert-variable-with-mixed-date-formats-to-one-format
pkgTest("vegan");pkgTest("ade4") # PCA
pkgTest("reshape2") # to unmelt data
pkgTest("ggplot2")
pkgTest("dplyr")
pkgTest("rgdal") # to read bathymetry
pkgTest("rnaturalearth")
pkgTest("rnaturalearthdata")
pkgTest("ggspatial")
#load spatial packages
pkgTest("raster")
pkgTest("rgeos")
pkgTest("scales")

#function which user
getpath4data <- function() {
  if(Sys.getenv("USER")=="Rosalie") return("/Volumes/-/Script R/LCM_GitHub_Data_LCM/")
  if(Sys.getenv("USER")=="alexnaccarato") return("~/Desktop/Food-Web 2018-2019/LCM_GitHub_Data/")
  if(Sys.getenv("USER")!="Rosalie"|Sys.getenv("USER")!="alexnaccarato") stop("You need to download the data from the VTDEC website and place them in a folder we can reach")
}

myconstant <- function() {
  if(Sys.getenv("USER")=="Rosalie") return(16)
  if(Sys.getenv("USER")=="alexnaccarato") return(21)
  if(Sys.getenv("USER")!="Rosalie"|Sys.getenv("USER")!="alexnaccarato") stop("You need to download the data from the VTDEC website and place them in a folder we can reach")
}

# Load data ####
# There should be 15 monitoring points on the lake
(Sys.glob(paste0(getpath4data(),"LakeMonitoringPoints/","*.txt")))
filenames <- list.files(paste0(getpath4data(),"LakeMonitoringPoints"), pattern="*.txt", full.names=TRUE)
ldf <- lapply(filenames, read.csv, header=T, sep="\t")
res <- lapply(ldf, summary)
# The function below substr() will allow us to extract the number of the lake
#    monitoring site in order to rename our new dataframes
# The way data are formated in the folder, the site code is the two first characters
#    of the file name
# Because path changes for different users, we'll access it by getting the length
#    of the path, and then adding 2 (to get pass the change in folder to the first
#    character)
location_name <- nchar(paste0(getpath4data(),"LakeMonitoringPoints"))+2

names(res) <- paste("LCM",substr(filenames, location_name, location_name+1), sep="")
for (i in 1:length(ldf))
  assign(paste0("LCM",substr(filenames, location_name, location_name+1))[i], ldf[[i]]) # Create individual dataframes

# Clean data - the date-time format varies ####
for (i in 1:length(ldf)) {
  temporary <- ldf[[i]] 
  temporary$VisitDate <- parse_date_time(x = temporary$VisitDate, orders = c("m/d/y", "m/d/Y"))
  assign(paste("LCM",substr(filenames, location_name, location_name+1), sep="")[i], temporary)
  ldf[[i]] <- temporary
  rm(temporary)
}
names(ldf) <- names(res)
# Create one global data set ####
total <- NULL
for (i in 1:length(ldf)) {
  total <- rbind(total,ldf[[i]])
}
summary(total)

# transform the data to average per year
total_year <- total
total_year$VisitDate <- year(total_year$VisitDate )

# transform the datasets
total <- dcast(data = total,formula = VisitDate + StationID~Test,value.var = "Result",fun.aggregate = mean, na.rm = TRUE)
total_year <- dcast(data = total_year,formula = VisitDate + StationID~Test,value.var = "Result",fun.aggregate = mean, na.rm = TRUE)
dim(total)
dim(total_year)

```

# Materials and methods

## Lake Champlain location

```{r}
world <- ne_countries(scale = "medium", returnclass = "sf")

p1 <- ggplot() +  
  geom_sf(data = world) +
  coord_sf(xlim = c(-165, -50), ylim = c(10, 80))  + 
  geom_point(aes(x=-73.212074, y=44.475883)) + 
  geom_label(aes(x=-73.212074, y=44.475883, label="Lake Champlain"),hjust=1, vjust=0, label.size = 0.15)  + 
  theme_set(theme_bw()) +
  annotation_scale(location = "bl", width_hint = 0.5) +
  annotation_north_arrow(location = "bl", which_north = "true",
                         pad_x = unit(0.75, "in"), pad_y = unit(0.5, "in"),
                         style = north_arrow_fancy_orienteering) +
  xlab("Longitude") + ylab("Latitude") 
p1

#ggsave(paste0(getwd(),"/Output/Figures/1-descriptive/GeneralMap_NorthAmerica.pdf"), p1,  width = 5.6, height = 4.4)



```


## Prep data
I edited the data and will work from mainly two datasets:
- a data set where intra-annual variability has been preserved (each measurement is still linked to its measurement date),
- a data set where I averaged the parameter per year.

```{r view datasets I will work with}
total[1:5,1:7]
total_year[1:5,1:7]
```

## Check outliers
Running the code below takes a lot of time, so summary of outliers given below. Uncomment the 'dotchart()' to see individual plots. 
```{r, echo=FALSE}
par(mfrow=c(ceiling((ncol(total)-1)/5),5))
for (i in 3:ncol(total)) {
  #dotchart(total[,i], main=colnames(total[i]))
}
par(mfrow=c(1,1))
```

It seems that there might be an outlier for the Net Phytoplankton, Chlorophyta biovolume + density (one value is 4 times higher than the previous max). The outlier is for station 4 (South Lake) on 2006-09-27 - not a site I'm using so not too bad.

Outlier too for Chrysophyta biovolume, station 50 (Missisquoi Bay) on 2014-09-19.

Max value (but doesn't behave as an outlier) for Cyanobacteria on 2013-07-30, at station 51 (Missiquoi Bay).

No outlier for Pyrrophyta biovolume either, max value on 2008-06-27, station 25 (Malletts Bay). Max values for Pyrrophyta density on 2007-07-16	for stations 34 and 40 (Northeast Arm, Inland Sea).

For Net phytoplankton, total biovolume, max value on 2014-09-19 for station 50 (Missisquoi Bay). For Net phytoplankton, total density, max value on 2013-07-30 for station 51 (Missisquoi Bay).

For Ortho-Phosphorus, max value for station 2 on 1994-07-27 (most likely an outlier).

Total Phosphorus, max value for station 2 on 2004-08-17	(235 Âµg/L).

Max Total Suspended solids for station 2 (South Lake, southern most station) on 2004-08-17.

```{r check outliers}
total[order(total$`Net phytoplankton, Chlorophyta biovolume`, decreasing = T),][1:5,1:2]
total[order(total$`Net phytoplankton, Chlorophyta density`, decreasing = T),][1:5,1:2]
total[order(total$`Net phytoplankton, Chrysophyta biovolume`, decreasing = T),][1:5,1:2]
total[order(total$`Net phytoplankton, Cyanobacteria biovolume`, decreasing = T),][1:5,1:2]
total[order(total$`Net phytoplankton, Cyanobacteria density`, decreasing = T),][1:5,1:2]
total[order(total$`Net phytoplankton, Pyrrophyta biovolume`, decreasing = T),][1:5,1:2]
total[order(total$`Net phytoplankton, Pyrrophyta density`, decreasing = T),][1:5,1:2]
total[order(total$`Net phytoplankton, total biovolume`, decreasing = T),][1:5,1:2]
total[order(total$`Net phytoplankton, total density`, decreasing = T),][1:5,1:2]

total[order(total$`Ortho-Phosphorus`, decreasing = T),][1:5,1:2]
total[order(total$`Total Phosphorus`, decreasing = T),][1:5,1:2]
total[order(total$`Total Suspended Solids`, decreasing = T),][1:5,1:2]

```


## Write a function to do PCAs
Creating a function to repeat the PCA more easily across station.
```{r prep dataset for PCA function}
# Parameters of the function should be:
#  mstationID= the number of the monitoring site, e.g., 2= South Lake B
#  replaceNas= logical argument to indicate whether NAs should be replaced by average mean. Default to TRUE
mpca_data <- function(mstationID, replaceNAs=T)
{
  mdata <- total_year[total_year$StationID==mstationID,]

  # First, deleting the columns that have only NAs (could happen when some parameters where never measured at the specific station)
  mdata <- mdata[colSums(!is.na(mdata)) > 0]
  
  # NAs cannot be handled by the PCA. 
  # I'm replacing the NAs by the column average, so they won't get any weight in the analysis
  if (replaceNAs) for (i in 3:ncol(mdata)) {
    mdata[is.na(mdata[,i]),i] <- mean(mdata[,i], na.rm=T)
  }
  
  return(mdata)
}

```



# Results
## Main Lake
The Main Lake is deep (122 m) and meso-oligotrophic. The monitoring point 19 is in the middle of the main lake. 

```{r PCA for Main Lake}
mydata <- mpca_data(mstationID = 19, replaceNAs = TRUE)
 
  acpR<-dudi.pca(mydata[,-c(1,2)], scannf = FALSE)
  summary(acpR)
  s.corcircle(acpR$co, clab=0.8)
  s.label(acpR$l1, clabel=0.6, label = mydata$VisitDate)
#  legend("topleft", legend = c(paste("PC1:",round(acpR$eig[1]/sum(acpR$eig)*100,2),"%",sep=""), paste("PC2:",round(acpR$eig[2]/sum(acpR$eig)*100,2),"%"),sep=""), cex=.4)
  #s.arrow(acpR$co, add.p=TRUE)
  
```

## Mallets Bay
Malletts Bay is moderately deep (50 m) and oligotrophic

```{r PCA for Mallets Bay}
mydata <- mpca_data(mstationID = 25, replaceNAs = TRUE)
 
acpR<-dudi.pca(mydata[,-c(1,2)], scannf = FALSE)
summary(acpR)
s.corcircle(acpR$co, clab=0.8)
s.label(acpR$l1, clabel=0.6, label = mydata$VisitDate)

  
```

## Inland Sea
The Inland Sea is moderately deep (35 m) and meso-eutrophice. 

```{r PCA for Inland Sea}
mydata <- mpca_data(mstationID = 34, replaceNAs = TRUE)
 
acpR<-dudi.pca(mydata[,-c(1,2)], scannf = FALSE)
summary(acpR)
s.corcircle(acpR$co, clab=0.8)
s.label(acpR$l1, clabel=0.6, label = mydata$VisitDate)

  
```

# Mapping

## Visualize the monitoring stations

```{r load files for map, message=FALSE, warning=FALSE, include=FALSE}
bathy<-readOGR(paste0(getpath4data(),"GIS/LakeChamplain_Shapefile/LakeChamplain.shp"))
lcgroupmap=c("lake",rep("island", 84562))

world <- ne_countries(scale = "medium", returnclass = "sf")

class(bathy)
crs(bathy)
crs(world)
bathy <- spTransform(bathy,crs(world))
crs(bathy)

gps_monitor <- read.delim(paste0(getpath4data(),"GIS/GPS_station_LCM.txt"))
gps_monitor_df <- gps_monitor
coordinates(gps_monitor)<-~Longitude+Latitude
proj4string(gps_monitor)<-CRS("+proj=longlat +datum=WGS84")
gps_monitor <- spTransform(gps_monitor,crs(world))
gps_monitor_df$Longitude <- gps_monitor$Longitude
gps_monitor_df$Latitude <- gps_monitor$Latitude
gps_monitor <- gps_monitor_df
gps_monitor_df <- gps_monitor_df[gps_monitor_df$CoreStation==TRUE&gps_monitor_df$WaterbodyType=="Lake",]
```

Over the years, `r nrow(gps_monitor)` monitoring stations have been monitored as part of the Lake Champlain monitoring program. `r nrow(gps_monitor[gps_monitor_df$CoreStation==TRUE,])` of these stations are core stations, and `r nrow(gps_monitor_df[gps_monitor_df$CoreStation==TRUE&gps_monitor_df$WaterbodyType=="Lake",])` are core lake stations.

```{r map, echo=FALSE, message=FALSE, warning=FALSE}
p <- ggplot() +  geom_polygon(data=bathy, aes(x=long, y=lat, group=group, fill=lcgroupmap), show.legend = FALSE) +
  scale_fill_manual(values = c("white","darkgrey")) +
  geom_point(data=gps_monitor, aes(x=Longitude, y=Latitude, color=CoreStation)) +
  xlab("Longitude") + ylab("Latitude") + theme_minimal()  +
  theme_set(theme_bw()) +
  coord_equal(ratio=1)

p

```

## Plot some parameters and see the average per sites

This is an interactive tool if you go back to the script. Here, the visualization is for the following parameters:

```{r}
parameter <- "Total Phosphorus"

min_yr    <- 1990
max_yr    <- 2018
mysubset <- total[which(as.numeric(substring(total$VisitDate, 1, 4))>=min_yr & as.numeric(substring(total$VisitDate, 1, 4))<=max_yr),]
```

Some parameters were not measured for the whole period. __`r length(which(!is.na(mysubset[,parameter])))`__ are available for the above parameters.

Parameters to chose from are:  
   -`r paste(names(total)[-c(1:2)],collapse = "</br>   -")`
   
```{r match gps data, include=FALSE}
n <- length(which(!is.na(mysubset[,parameter])))

if(n>1) min_yr    <- min(as.numeric(substring(mysubset$VisitDate, 1, 4)))
if(n>1) max_yr    <- max(as.numeric(substring(mysubset$VisitDate, 1, 4)))

summ_subset <- with(mysubset, 
                    tapply(mysubset[,parameter],list("Sites"=as.factor(mysubset$StationID)), mean, na.rm=T))
summ_subset <- as.data.frame(summ_subset)
summ_subset$StationID <- as.numeric(rownames(summ_subset))

summ_subset$lon <- rep(NA, nrow(summ_subset))
summ_subset$lat <- rep(NA, nrow(summ_subset))
for (i in 1:nrow(summ_subset)) {
    summ_subset$lon[i] <- gps_monitor_df$Longitude[which(as.numeric(gps_monitor_df$StationID)==summ_subset$StationID[i])]
    summ_subset$lat[i] <- gps_monitor_df$Latitude[which(as.numeric(gps_monitor_df$StationID)==summ_subset$StationID[i])]
}

```
   
Plot that on the map.


```{r map one selected parameter, echo=FALSE, message=FALSE, warning=FALSE}
#muted("red") creates some color that are nicer looking
p <- ggplot() +  geom_polygon(data=bathy, aes(x=long, y=lat, group=group, fill=lcgroupmap), show.legend = FALSE) +
  scale_fill_manual(values = c("white","darkgrey")) +
  geom_point(data=summ_subset, aes(x=lon, y=lat,size = summ_subset)) +
  #scale_colour_gradient2(low = muted("black"), mid = "white", high = "red", midpoint = mean(summ_subset$summ_subset, na.rm=T), space = "Lab",na.value = "grey50", guide = "colourbar", aesthetics = "colour") +
  xlab("Longitude") + ylab("Latitude") + theme_minimal()  +
  ggtitle(paste0(parameter, " (",min_yr,"-",max_yr,")")) +
  theme_set(theme_bw()) +
  coord_equal(ratio=1) 

p
ggsave(paste0(getwd(),"/Output/Figures/1-descriptive/Map_",gsub(" ", "_",parameter),"_",min_yr,"-",max_yr,".pdf"), p)

```
 
 
# Weather data

## Read data

```{r message=FALSE, warning=FALSE, include=FALSE}
weather <- read.delim(paste0(getpath4data(), "Weather/allWeatherBTV_join.txt"))
head(weather)

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
weather$date <- parse_date_time(weather$date,orders = c("d/m/Y"))
ggplot(weather,aes(x = date, y= AvgApparentTemperatureCelsius)) + geom_line() + geom_smooth()

```

