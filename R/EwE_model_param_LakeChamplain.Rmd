---
title: "Model parameter EwE Lake Champlain"
author: "Rosalie Bruel"
date: "created on 2019-07-11 -- last update: `r Sys.Date()`"
output:
  html_document:
    df_print: paged
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
fontsize: 11pt
editor_options:
  chunk_output_type: console
  df_print: paged
always_allow_html: yes
---

_Last update: `r Sys.Date()`_

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = dirname(getwd()))
knitr::opts_chunk$set(echo = TRUE)
# tmp <- lessismore(packages = .packages(), path2file = "R/EwE_model_param_LakeChamplain.Rmd", plot_output = T)
# cat(tmp$summary)
# tmp$functions_non_matched
# tmp$packages_used
# tmp$packages_non_used
# tmp$summary_functions_plot
library(ggplot2)
library(plotly)
library(plyr) #for ddply
library(dplyr)
library(lubridate) # for date time
library(changepoint) #changepoint analysis
library(reshape2) # for melt
library(Hmisc) # for err bar plot
library(FSA);library(FSAdata) # Fisheries stock assessment methods and data
library(leaflet)
library(janitor)
library(rgdal)
library(raster)
library(rnaturalearth)
library(RColorBrewer)
library(grid)
library(gridExtra) # get two plots side by side
library(knitr) # for kable
library(stringr) # for str_replace()
library(wesanderson) # colors
library(ggraph) # for nodes plot
library(igraph) # probably better for nodes graph because can also assign weigth
library(tidygraph) # to transform dataframe in node datasets
library(scales) # Get full number in x/y scales with scale_x_continuous(labels = comma)/scale_y_continuous(labels = comma) instead of 1.00e09 for example
library(concurve) # plot p-value and s-value, see recommandations in Chow and Sander, 2019
library(psych) # for pairs-panels (scaterplot matrix)


# Captions with library captioner
# https://community.rstudio.com/t/avoiding-repetitive-latex-codes-in-r-markdown/7834/12
# https://cran.r-project.org/web/packages/captioner/vignettes/using_captioner.html 
fig_cap <- captioner::captioner()
tab_cap <- captioner::captioner("Table")

getpath4data <- function() {
  if(Sys.getenv("USER")=="Rosalie") return("/Volumes/-/Script R/LCM_GitHub_Data_LCM/")
  if(Sys.getenv("USER")=="alexnaccarato") return("~/Desktop/Food-Web 2018-2019/LCM_GitHub_Data/")
  if(Sys.getenv("USER")!="Rosalie"|Sys.getenv("USER")!="alexnaccarato") stop("You need to get the data.")
}

getpath4data <- function() {
  if(Sys.getenv("USER")=="Rosalie") return("/Volumes/-/Script R/LCM_GitHub_Data_LCM/")
  if(Sys.getenv("USER")=="alexnaccarato") return("~/Desktop/Food-Web 2018-2019/LCM_GitHub_Data/")
    if(Sys.getenv("USER")=="YOUR USER NAME") return("YOUR PATH")
  if(Sys.getenv("USER")!="Rosalie"|Sys.getenv("USER")!="alexnaccarato") stop("You need to get the data.")
}

# Source functions for fulton cond factor, relative condition, and relative weight
# Compute_FultonK
source("https://raw.githubusercontent.com/rosalieb/miscellaneous/master/R/Compute_FultonK.R")
# RelCond_Fisheries
source("https://raw.githubusercontent.com/rosalieb/miscellaneous/master/R/RelCond_Fisheries.R")
# RelWeight_Fisheries
source("https://raw.githubusercontent.com/rosalieb/miscellaneous/master/R/RelWeight_Fisheries.R")

# turn this to true so some stuffs are not run when I'm knitting (and vice versa)
R_U_KNITTING = TRUE 
```

The objective of this document is to gather all in one place the decisions made for the parameters of the model.
I'm starting this document on July 16th, 2019, having entered some parameters over the past 4 months and realizing I already forgot where I got them from... Hopefully I'll find my notes back, otherwise, having everything in one place might be easier.

The model and connection we're assuming in a first place are represented below.

```{r plot foodweb with nodes, echo=FALSE, message=FALSE, warning=FALSE}
foodweb <- data.frame(
  "predator" = c("Sea Lamprey","Sea Lamprey","Sea Lamprey","AtSalmon","AtSalmon","LT",  "LT",  "LT",  "LT",     "Cisco","Cisco","Cisco","Cisco","Whitefish","Whitefish", "Trout Perch","ALW", "ALW", "ALW", "Smelt", "Smelt", "Smelt","Sculpin",           "Sculpin",  "MYS", "MYS", "ZOO"),
  "prey"     = c("Burbot", "LT",     "AtSalmon","ALW",     "Smelt",   "MYS","ALW", "Smelt","Sculpin","ALW","Smelt","ZOO","BcCp",   "BcCp","Benthic Invertebrates", "ZOO"  , "MYS", "ZOO", "BcCp", "MYS", "ZOO", "BcCp",    "Benthic Invertebrates", "MYS","PHY", "Detritus","PHY")
)

foodweb$level <- rep(1, nrow(foodweb))
foodweb$level <- rep(NA, nrow(foodweb))
for (i in 1:nrow(foodweb)) {
  if (foodweb$predator[i] %in% c("Detritus")) foodweb$level[i] = 0.5
  if (foodweb$predator[i] %in% c("PHY","Benthic Invertebrates")) foodweb$level[i] = 1
  if (foodweb$predator[i] %in% c("ZOO","MYS")) foodweb$level[i] = 2
  if (foodweb$predator[i] %in% c("BcCp")) foodweb$level[i] = 3
  if (foodweb$predator[i] %in% c("Sculpin","Trout Perch", "Alewife","Smelt")) foodweb$level[i] = 4
  if (foodweb$predator[i] %in% c("Burbot","AtSalmon", "LT","Cisco","Whitefish")) foodweb$level[i] = 5
  if (foodweb$predator[i] %in% c("Sea Lamprey")) foodweb$level[i] = 6
}

graph <- as_tbl_graph(foodweb)
ggraph(graph, "grid") + 
  geom_edge_link() + 
  geom_node_point() +
  geom_node_label(aes(label = names(graph[1])), repel = TRUE)

```
<br>`r fig_cap("node plot1", caption="Lake Champlain food web – species of interest")`

```{r plot food web with igraph, echo=FALSE, message=FALSE, warning=FALSE}
ewediet <- read.delim(paste0(getpath4data(),"EwE_params/EwE_diet.txt"))
colnames(ewediet)[-c(1:2)] <- as.character(ewediet$predator[1:c(ncol(ewediet)-2)])
rownames(ewediet) <- as.character(ewediet$predator)
ewediet <- ewediet[1:c(ncol(ewediet)),-c(1)]

ewediet2 <- melt(ewediet)
colnames(ewediet2) <- c("prey","predator","weigth")
#ewediet2 <- ewediet2[,c("predator","prey","weigth")]
ewediet2 <- rbind(ewediet2,c("Smelt","Smelt",0.1))

ewediet2 <- ewediet2[ewediet2[,3]>0,]

ewediet2$prey <- as.character(ewediet2$prey)
ewediet2$predator <- as.character(ewediet2$predator)

ewediet_nodes <- data.frame(
  "id"=as.character(rownames(ewediet)),
  "data_source"=as.character(c("VTFWS", "VTFWS/UVM","VTFWS/UVM","NA","NA","NA","UVM","UVM","UVM","UVM","UVM","UVM","VTDEC","VTDEC","VTDEC","UVM","VTDEC","calculated")),
  "level"=c(-2,0,3,0,0,0,1,0,5,4,2,5,7,6,7,6.5,8,9)
)

ewediet_nodes <- ewediet_nodes[order(ewediet_nodes$level),]

net <- graph_from_data_frame(d=ewediet2, vertices=ewediet_nodes, directed=T) 

#set colors
colrs <- wes_palette("Darjeeling1", n=length(unique(V(net)$level)), type="continuous")
V(net)$color <- colrs[V(net)$level+6]
colrs <- wes_palette("Darjeeling1", n=length(V(net)$name), type="continuous")
V(net)$color <- colrs

edge.start <- ends(net, es=E(net), names=F)[,2]
edge.col <- V(net)$color[edge.start]

E(net)$weight <- as.numeric(paste(ewediet2$weigth))
E(net)$width <- as.numeric(paste(E(net)$weight))*10


# par(mfrow=c(1,1), mar=c(1,12,1,1))
# plot(net, edge.color=edge.col, edge.curved=.1, layout=do.call("layout_in_circle", list(net))) 
# par(xpd=T)
# legend(x=-2, y=1.1, c(V(net)$name), pch=21,col="#777777", pt.bg=colrs, pt.cex=2, cex=.8, bty="n", ncol=1)
# par(xpd=F)


# mylayout should have 2 columns, x and y, and number of rows of the number of groups: length(V(net)$name) == 18

mylayout <- ewediet_nodes %>% mutate(
  # x positionning 
  x = case_when(id == "Sea lamprey"      ~ 0,
                id == "Adult lake trout" ~ 0,
                id == "Atlantic salmon"  ~ 2,
                id == "Burbot"           ~ 4,
                id == "Walleye"          ~ -4,
                id == "Whitefish"        ~ -2,
                id == "Cisco"            ~ -1,
                id == "Alewife"          ~ 1,
                id == "Smelt"            ~ 4,
                id == "Juvenile lake trout" ~ -3,
                id == "Trout-perch"      ~  2,
                id == "Sculpin"          ~  4,
                id == "Zooplankton (predacious)"    ~  -1,
                id == "Mysis"            ~  -3,
                id == "Benthic invertebrates" ~ 2,
                id == "Zooplankton (grazers)"      ~ 0,
                id == "Phytoplankton"    ~ -2,
                id == "Detritus"         ~ 0),
  # Label positioning (1= bottom, 2=left, 3=top, 4= right) 
  x2 = case_when(id == "Sea lamprey"     ~ 3,
                id == "Adult lake trout" ~ 3,
                id == "Atlantic salmon"  ~ 3,
                id == "Burbot"           ~ 3,
                id == "Walleye"          ~ 3,
                id == "Whitefish"        ~ 3,
                id == "Cisco"            ~ 3,
                id == "Alewife"          ~ 3,
                id == "Smelt"            ~ 2,
                id == "Juvenile lake trout" ~ 3,
                id == "Trout-perch"      ~  3,
                id == "Sculpin"          ~  2,
                id == "Zooplankton (predacious)"    ~  3,
                id == "Mysis"            ~  4,
                id == "Benthic invertebrates" ~ 2,
                id == "Zooplankton (grazers)"      ~ 1,
                id == "Phytoplankton"    ~ 4,
                id == "Detritus"         ~ 1))
mylayout <- matrix(c(mylayout$x,mylayout$level,mylayout$x2),ncol=3)

E(net)$label <- paste0(round(as.numeric(paste(E(net)$weight)),2), "\n")
E(net)$label <- NA

V(net)$label <- paste0(ewediet_nodes$id,"\n")
V(net)$label <- NA

if(!R_U_KNITTING) pdf(paste0(getwd(),"/Output/Figures/5-Summary/summary_interaction4.pdf"),width = 8, height = 8)
par(mar=c(0,4,0,7))
plot.igraph(net,edge.color=adjustcolor(edge.col,alpha.f = .8), edge.curved=.1,
                  layout=-as.matrix(mylayout)[,1:2])
par(xpd=T)
TeachingDemos::shadowtext(-norm_coords(mylayout[,1:2], -1, 1, -1, 1), label=ewediet_nodes$id,pos=mylayout[,3], offset=1.3, col="black",bg = "white",  theta = (1:8/4)*pi, r1 = 0.06, r2 = 0.04)
par(xpd=F)
if(!R_U_KNITTING) dev.off()

```


For the Lake Champlain monitoring data, I'm using the data I processed for the Stats-a-thon. The idea was to get something a bit homogeneous in term of depths (hypolimnion and epilimnion data when lake stratified, and hypolimnion=epilimnion when lake mixed).

```{r read data from LTM}
dtlcm <- read.delim(paste0(getpath4data(),"LCM_unique_param_step4.txt"))
names(dtlcm)
dtlcm$Year <- as.numeric(paste(year(as.Date(dtlcm$VisitDate, format="%d/%m/%Y"))))

```


```{r read detailed plankton database, message=FALSE, warning=FALSE, include=FALSE}
plktn_pre2010  <- read.delim(paste0(getpath4data(),"LCM_bio_PeteStangel/Plankton data pre 2010.txt"))
plktn_post2010 <- read.delim(paste0(getpath4data(),"LCM_bio_PeteStangel/Plankton data after 2010.txt"))
sites_raw      <- read.delim(paste0(getpath4data(),"LCM_bio_PeteStangel/Plankton data stations.txt"))
sites          <- sites_raw[sites_raw$LocationID %in% as.factor(plktn_pre2010$LocationID),]

plktn <- rbind(plktn_pre2010,plktn_post2010)

plktn$VisitDate <- parse_date_time(x = plktn$PlanktonData.VisitDate, orders = c("d-b-y","d/m/Y"))

plktn$StationID  <- sites[match(plktn$LocationID, sites$LocationID), "StationID"]

names(plktn)
plktn$PlanktonType[plktn$PlanktonType=="phyto"] <- "Phyto"
summary(plktn$PlanktonType)

summary(plktn$ResultType[plktn$PlanktonType=="Phyto"])
summary(plktn$ResultType[plktn$PlanktonType=="Zoo"])
summary(plktn$SampleType[plktn$PlanktonType=="Zoo"])

# Reshape the dataframe with the type of plankton+name as column name
plktn$Type_SpeciesID <- paste(plktn$PlanktonType, plktn$SpeciesID, sep="_")
plktn2 <- dcast(plktn,  VisitDate + StationID ~ Type_SpeciesID,value.var = "Result",fun.aggregate = sum, na.rm = TRUE )
head( plktn2)
dim(plktn2)
names(plktn2)

phyto <- plktn2[,c(1, 2, grep("Phyto_", names(plktn2)))]
zoo   <- plktn2[,c(1, 2, grep("Zoo_",   names(plktn2)))]

dim(zoo)
dim(phyto)

names(zoo)
rowSums(phyto[,-c(1:2)])

# Often, when we get 0 for zoo, we get value for phyto, and vice versa.
# no overlap of sampling? Are zoo and phyto samples taken on different days?
# There are only 69 days for which we got values both for zoo and phyto.
summary(which(rowSums(zoo[,-c(1,2)])!=0) %in% which(rowSums(phyto[,-c(1,2)])!=0))
summary(which(rowSums(phyto[,-c(1,2)])!=0) %in% which(rowSums(zoo[,-c(1,2)])!=0))


```


```{r read target and bycatch data here because they could be used at several steps, message=FALSE, warning=FALSE, include=FALSE}
tg <- read.delim(paste0(getpath4data(),"data_from_Pascal/target_catch_2016-2018.txt"))
#head(tg)
byc <- read.delim(paste0(getpath4data(),"data_from_Pascal/bycatch_2016-2018.txt"))
#head(byc)

# Convert date format to something we'll use more easily
tg$date <- as.Date(tg$date, format = "%d/%m/%Y")
byc$date <- as.Date(byc$date, format = "%d/%m/%Y")

```
Here are also the number of observations collected for different species during the trawls (`r min(tg$year,na.rm=T)`-`r max(tg$year,na.rm=T)`).

```{r summary target/bycatch, message=FALSE, warning=FALSE, include=FALSE}
summ_tg <- with(tg, tapply(rep(1,nrow(tg)),list("species#"=species,"Year#"=year), sum))
summ_by <- with(byc, tapply(rep(1,nrow(byc)),list("species#"=species,"Year#"=year), sum))
summ_tgby <- as.data.frame(rbind(summ_tg,summ_by))
summ_tgby$df_origin <- c(rep("targetcatch",nrow(summ_tg)),rep("bycatch",nrow(summ_by)))
summ_tgby <- summ_tgby[order(rownames(summ_tgby)),]
summ_tgby
```


# Detritus

We use Pauly et al (1993) equation to calculate the biomass of detritus:  
$Log10(D) = 0.954 * log10(PP) + 0.863 * log10(E) – 2.41$,

with E= depth of the euphotic zone.
I'm calculating the depth of the euphotic zone based on the secchi depth.

## Secchi Depth

__Data__:  
* Long-term monitoring dataset

`r fig_cap("secchi", caption = "Secchi depths across all sites and years", display=FALSE)`

The secchi depth across all sites and years is ploted below (`r fig_cap("secchi", display="cite")`).

```{r get secchi depth from LTM, echo=FALSE, message=FALSE, warning=FALSE}
secchi_depth_p <- dtlcm %>% ggplot(mapping = aes(x = as.factor(StationID), y = -Secchi.Depth, fill = as.factor(StationID))) +
  geom_boxplot() +
  xlab("Station ID") +
  ylab("Secchi depth (m)") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle(paste0("Secchi depths across all sites and years")) +
  scale_fill_discrete(name = "Stations")
secchi_depth_p
```
<br> `r fig_cap("secchi", display="full")`

It would seems like years 2000 have generally deeper secchi depths. There's also some kind of oscillations: ± 2 m in secchi depth. Teleconnections with some ocean cycles?
```{r plot secchi depth per sites with facet wrap, echo=FALSE, message=FALSE, warning=FALSE}
secchi_depth_p2 <- dtlcm %>% ggplot(mapping = aes(x = as.factor(year(as.Date(dtlcm$VisitDate, format="%d/%m/%Y"))), y = -Secchi.Depth, fill = as.factor(year(as.Date(dtlcm$VisitDate, format="%d/%m/%Y"))))) +
  geom_boxplot() +
  facet_wrap(~as.factor(StationID)) + 
  xlab("Station ID") +
  ylab("Secchi depth (m)") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none") +
  ggtitle(paste0("Evolution of secchi depths per site and years (",min(year(as.Date(dtlcm$VisitDate, format="%d/%m/%Y"))),"-",max(year(as.Date(dtlcm$VisitDate, format="%d/%m/%Y"))),")")) +
  scale_fill_discrete(name = "Stations")
secchi_depth_p2

```
<br>`r fig_cap("secchi2", caption = "Evolution of sechi depths across all sites and years (see Hilt et al 2010 were secchi depths suddenly change).")`

The study of Gaiser _et al_ (2009, _Limnology and Oceanography_) shows some teleconnection between Lake Annie transparency and AMO index. Does Lake Champlain also exhibit teleconnection with AMO or ENSO?

Multivariate ENSO Index version 2 (MEI) data retrieved from NOAA (<a href="https://www.esrl.noaa.gov/psd/enso/mei/">Source for MEI.v2 Values</a>), and compare them with our secchi depths (i.e., reproduce Fig. 3 in Gaiser _et al_, 2009).

```{r meiv index with secchi, echo=FALSE, message=FALSE, warning=FALSE}
meiv <- read.delim(paste0(getpath4data(),"meiv2.data.txt"))
dtlcm_secchi <- dtlcm[,c("StationID", "VisitDate", "Secchi.Depth","Total.Phosphorus_E")]
dtlcm_secchi$VisitDate <- as.Date(dtlcm_secchi$VisitDate, format="%d/%m/%Y")
dtlcm_secchi <- dtlcm_secchi[complete.cases(dtlcm_secchi),]
dtlcm_secchi$Year <- year(dtlcm_secchi$VisitDate)

dtlcm_secchi_summ <- as.data.frame(
  with(dtlcm_secchi, tapply(dtlcm_secchi$Secchi.Depth
  ,list("Year"=as.factor(Year),"StationID"=as.factor(StationID)), mean, na.rm=T))
  )

dtlcm_TP_summ <- as.data.frame(
  with(dtlcm_secchi, tapply(dtlcm_secchi$Total.Phosphorus_E
  ,list("Year"=as.factor(Year),"StationID"=as.factor(StationID)), mean, na.rm=T))
  )

pairs.panels(cbind(as.numeric(paste(rownames(dtlcm_secchi_summ))),dtlcm_secchi_summ[,c("2","4","7","9")]), 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
pairs.panels(cbind(as.numeric(paste(rownames(dtlcm_secchi_summ))),dtlcm_secchi_summ[,c("16","21","19","25","33","36","46")]), 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
pairs.panels(cbind(as.numeric(paste(rownames(dtlcm_secchi_summ))),dtlcm_secchi_summ[,c("34","40","50","51")]), 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )

dtlcm_secchi_summ2 <- data.frame(melt(dtlcm_secchi_summ), "Year"=as.numeric(paste(rep(rownames(dtlcm_secchi_summ), times=ncol(dtlcm_secchi_summ)))))

dtlcm_TP_summ2 <- data.frame(melt(dtlcm_TP_summ), "Year"=as.numeric(paste(rep(rownames(dtlcm_TP_summ), times=ncol(dtlcm_TP_summ)))))

dtlcm_secchi_summ2$TP <- dtlcm_TP_summ2$value
dtlcm_secchi_summ2$meiv_summer <- rep(NA, nrow(dtlcm_secchi_summ2))
dtlcm_secchi_summ2$meiv_winter <- rep(NA, nrow(dtlcm_secchi_summ2))

for (i in min(dtlcm_secchi_summ2$Year):max(dtlcm_secchi_summ2$Year)) {
  dtlcm_secchi_summ2$meiv_summer[dtlcm_secchi_summ2$Year==i] <- rowMeans(meiv[meiv$YEAR==i,7:11])
  dtlcm_secchi_summ2$meiv_winter[dtlcm_secchi_summ2$Year==i] <- rowMeans(meiv[meiv$YEAR==i,2:5])
}

# dtlcm_secchi <- dtlcm_secchi %>%
#   mutate(meiv = case_when(VisitDate == 2000 ~ rowMeans(meiv[,-1])[meiv$YEAR==2000]))

```

```{r plot meiv index with secchi, echo=FALSE, message=FALSE, warning=FALSE}
p1 <- ggplot(dtlcm_secchi_summ2[dtlcm_secchi_summ2$Year!=2015,],aes(x=meiv_summer, y=value/log(TP), col=Year)) +
  geom_point() + stat_smooth(method = "lm") +
  #geom_point(aes(x=meiv_min, y=value, col="min meiv"))+
  #geom_point(aes(x=meiv_max, y=value, col="max meiv"))+
  facet_wrap(~as.factor(variable)) + ggtitle("Summer ENSO index (direct)")

p2 <- ggplot(dtlcm_secchi_summ2[dtlcm_secchi_summ2$Year!=2015,],aes(x=meiv_winter, y=value, col=Year)) +
  geom_point() + stat_smooth(method = "lm") +
  facet_wrap(~as.factor(variable)) + ggtitle("Winter ENSO index (legacy)")

grid.arrange(p1,p2,nrow=1)
```
<br>`r fig_cap("secchi3", caption = "Secchi depth against Multivariate ENSO Index for each monitoring station.")`

Same but with the Atlantic Multidecadal Oscillation (AMO, <a href="https://www.esrl.noaa.gov/psd/data/timeseries/AMO/"> data source NOAA</a>).

```{r AMO index}
amo <- read.delim(paste0(getpath4data(),"AMO_index.txt"))

dtlcm_secchi_summ2$amo_summer <- rep(NA, nrow(dtlcm_secchi_summ2))
dtlcm_secchi_summ2$amo_winter <- rep(NA, nrow(dtlcm_secchi_summ2))

for (i in min(dtlcm_secchi_summ2$Year):max(dtlcm_secchi_summ2$Year[dtlcm_secchi_summ2$Year %in% amo$YEAR])) {
  dtlcm_secchi_summ2$amo_summer[dtlcm_secchi_summ2$Year==i] <- rowMeans(amo[amo$YEAR==i,6:10])
  dtlcm_secchi_summ2$amo_winter[dtlcm_secchi_summ2$Year==i] <- rowMeans(amo[amo$YEAR==i,2:5])
}

p3 <- ggplot(dtlcm_secchi_summ2[dtlcm_secchi_summ2$Year!=2015,],aes(x=amo_summer, y=value, col=Year)) +
  geom_point() + stat_smooth(method = "lm") +
  facet_wrap(~as.factor(variable)) + ggtitle("Summer AMO index (direct)")

p4 <- ggplot(dtlcm_secchi_summ2[dtlcm_secchi_summ2$Year!=2015,],aes(x=amo_winter, y=value, col=Year)) +
  geom_point() + stat_smooth(method = "lm") +
  facet_wrap(~as.factor(variable)) + ggtitle("Winter AMO index (legacy)")

grid.arrange(p3,p4,nrow=1)
```
<br>`r fig_cap("secchi4", caption = "Secchi depth against AMO (summer and winter).")`

```{r}
head(dtlcm_secchi_summ2)
library(mgcv)
gam1 <- gam(value ~ s(meiv_summer) + variable, data=dtlcm_secchi_summ2)
summary(gam1)
plot(gam1);abline(h=0,lty=2)
gam2 <- gam(value ~ s(meiv_winter) + variable, data=dtlcm_secchi_summ2)
summary(gam2)
plot(gam2)
gam3 <- gam(value ~ s(amo_winter) + variable, data=dtlcm_secchi_summ2)
summary(gam3)
plot(gam3)
gam4 <- gam(value ~ s(amo_winter) + variable, data=dtlcm_secchi_summ2)
summary(gam4)
plot(gam4)
# Alternatively, we could fit a smoother to each station, as below
gam5 <- gam(value ~ s(meiv_summer, by = as.numeric(variable==2)) +
              s(meiv_summer, by = as.numeric(variable==4)) +
              s(meiv_summer, by = as.numeric(variable==7)) +
              s(meiv_summer, by = as.numeric(variable==9)) +
              s(meiv_summer, by = as.numeric(variable==16)) +
              s(meiv_summer, by = as.numeric(variable==19)) +
              s(meiv_summer, by = as.numeric(variable==21)) +
              s(meiv_summer, by = as.numeric(variable==25)) +
              s(meiv_summer, by = as.numeric(variable==33)) +
              s(meiv_summer, by = as.numeric(variable==34)) +
              s(meiv_summer, by = as.numeric(variable==36)) +
              s(meiv_summer, by = as.numeric(variable==40)) +
              s(meiv_summer, by = as.numeric(variable==46)) +
              s(meiv_summer, by = as.numeric(variable==50)) +
              s(meiv_summer, by = as.numeric(variable==51)), data=dtlcm_secchi_summ2)
summary(gam5)
# Good R2, but looking at the plot, there is no crossing of the intercept, except for 9 and 46.
#plot(gam5)
```

With GLS:

Residuals are not normally distributed.

```{r}
library(nlme)
M0 <- gls(value ~ amo_winter + variable + Year, data=dtlcm_secchi_summ2, na.action = na.omit)
summary(M0)

#plot the standardized residuals vs. fitted
plot(M0)

```


**Compound symmetry** covariance matrix assumes that whatever the distance in time between two observations, their residual correaltion is the same (i.e., it doesn't change with increasing/decreasing time between observations). Constant covariance.
+ Often too simplistic for time series data.
```{r}

M1<-gls(value ~ amo_winter + variable + Year, na.action = na.omit, correlation = corCompSymm(form=~Year|variable), data=dtlcm_secchi_summ2)


#look at the model output
summary(M1)
```

**AR1: Auto-regressive model of order 1** Models the residual at time s as a function of the residual of time s-1 along with noise.

The parameter for this correlation function, $\rho$, is unknown and has to be estimated from the data. The further away two residuals are separated in time, the lower their correlation. 
```{r}
#fit the model with AR-1 correlation function, corAR1
M2<-gls(value ~ amo_winter + variable + Year, na.action = na.omit, correlation = corAR1(form=~Year|variable), data=dtlcm_secchi_summ2)

#look at the output
summary(M2)
#the parameter estimate is much higher. Residuals separated by one year have a correlation of 0.31. Residuals separated by two years have a correlation of 0.31^2=.10. Residuals separated by two years have a correlation of 0.77^3=.02, etc. 

#compare the three models with AIC
AIC(M0,M1,M2)

```

**ARMA Error Structures**
More complex structures based on the AR-1, using an auto-regressive moving average (ARMA) model for residuals. ARMA has two parameters defining it's order: the number of auto-regressive parameters (p) and the number of moving average parameters (q). So, ARMA(1,0) refers to AR-1. 

Realize that the p and q parameters have to be estimated from the data - using values of p or q larger than 2 or 3 tend to give error messages related to convergence problems (i.e., the larger p and/or q are, the more parameters there will be to estimate, which can give you convergence problems). 

```{r}
#here, we'll define a bunch of ARMA(p,q) structures
#Zuur says that it's kind of a black box. We're starting with these values, but we don't really know why.
cs1 <- corARMA(c(0.2), p = 1, q = 0) #0.2 is a chosen starting value for p
cs2 <- corARMA(c(0.3, -0.3), p = 2, q = 0) #0.3 and -0.3 are chosen starting values for 2 p's 
cs3 <- corARMA(c(0.3, -0.3, -0.3), p = 3, q = 0) #starting values for 3 p's

#fit models using all of the correlation structures above
M3arma1<-gls(value ~ amo_winter + variable + Year, correlation = cs1, na.action = na.omit, data=dtlcm_secchi_summ2) #equivalent to AR1
M3arma2<-gls(value ~ amo_winter + variable + Year, correlation = cs2, na.action = na.omit, data=dtlcm_secchi_summ2)


M3arma3<-gls(value ~ amo_winter + variable + Year,na.action=na.omit,
            correlation=cs3,data=dtlcm_secchi_summ2)

AIC(M0,M1,M2,M3arma1,M3arma2,M3arma3)


```

These models are not all nested, so we need to compare them using AIC (we cannot use likelihood ratio tests).
Model below not working (no convergence).

```{r}
BM1<-gamm(value ~ amo_winter +  variable + s(Year, by=variable),
          weights = varIdent(form = ~1|variable), data=dtlcm_secchi_summ2)
```

The `varIdent(form=~1|variable)` should in theory allows each time series (i.e., each station) to have a different variance. The `s(Year, by=variable)` is here to ensure that the smoother is applied over each station's time series.

The problem with this model is that the p-values assume independence, and because thet data are time series, this assumption may be violated. Adding an AR-1 residual auto-correlation structure can help with that.

```{r}

#fit the same model with AR-1 temporal correlation
BM2<-gamm(value ~ s(amo_winter) + variable,
          correlation = corAR1(form=~Year|variable),
          data=dtlcm_secchi_summ2)



#Use AIC to compare the two models
#AIC(BM1$lme,BM2$lme)
#look at output for the best model
summary(BM2$gam)
summary(BM2$lme)

anova(BM2$gam)

plot(BM2$gam)



```

The `anova()` output is the most straightforward. Only the Oahu time series have signifcant long-term trends and rainfall effects. The Maui time series are only affected by rainfall. 


## Setting the euphotic zone
```{r set euphtotic zone}
eu_z = 4.9
```
<br>
We set the euphotic zone for the main lake (StationID= 16 & 19) at `r eu_z` m.

The following code is not run yet, we need a value for the primary production.

```{r calculate detritus, eval=FALSE}
D = 10^(0.954 * log10(PP) + 0.863 * log10(eu_z) - 2.41)
D
```


# Phytoplankton

__Data__:  
* Long-term monitoring dataset

Phytoplankton data are collected since 2006. The field methodology described by the program says: 2x secchi by integrated hose or by 63 um net tow, lugols preserved samples, and are then counted on settling chambers or Sedgewick Rafter cells.

Biovolume are given in µm<sup>3</sup>/l.

We need: <br>
* The dimensions of the net <br>
* The depth sampled (use the secchi depth) <br>

```{r set diameter phytoplankton net}
diameter_net <- .8 #in meter
```

Create a special dataframe for phytoplankton.

```{r create a special dataframe for phytoplankton, echo=FALSE, message=FALSE, warning=FALSE}
phyto <- dtlcm[,c("StationID", "VisitDate","Secchi.Depth", "Net.phytoplankton..total.biovolume")]

n1 = nrow(phyto)
phyto <- phyto[!is.na(phyto$Secchi.Depth),]
n2 = nrow(phyto)
phyto <- phyto[!is.na(phyto$Net.phytoplankton..total.biovolume),]
n3 = nrow(phyto)
head(phyto)

phyto$Volume.sampled <- pi*(diameter_net/2)^2*(2*phyto$Secchi.Depth)

head(phyto)

```

`r n1-n2` rows deleted because Secchi.Depth=NA. <br>
`r n2-n3` rows deleted because Net.phytoplankton..total.biovolume=NA<br>

Conversion of µm<sup>3</sup>/l in t/km<sup>2</sup>:<br>
<center>
1 µm<sup>3</sup> = 1*10<sup>-15</sup> l    <br>
1 l = 1*10<sup>-6</sup> t
</center>

# Zooplankton

See script 'plankton_data.Rmd'.

Organisms density available (#/m<sup>3</sup>). To convert them to biomass, we use McCauley (1984), Chapter 7. The Estimation of the Abundance and Biomass of Zooplankton in Samples.
He established Weight/Length relationships for zooplankton.
We used average length of organisms collected from pictures found online, use the equations in McCauley (1984) to get an estimation of the weight, and converted #/m<sup>3</sup> to g/<sup>3</sup> that way.

```{r calculate weight from size, echo=FALSE, message=FALSE, warning=FALSE}
# Size data, from McCauley 1984 (Chapter 7)
size_zoop      <- read.delim(paste0(getpath4data(),"plankton/Zoop_size.txt"))
# Calculate average weight to get biomass
size_zoop$weight <- exp(size_zoop$a +  size_zoop$b*log(size_zoop$Average_Size))
size_zoop
```

Rotifers are not listed in the McCauley chapter, so we used a multiplying factor from <a href= "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.595.698&rep=rep1&type=pdf"> </a>.
> Dry Weights of the Zooplankton of Lake Mikri Prespa (Macedonia, Greece), Evangelia Michaloudi (2005)

But first, we need to group the species into larger group.

## Group zoop in larger groups

```{r grouping zoop species, message=FALSE, warning=FALSE}
# Group the data by at least genus

#names(zoo)
whatsleft <- 
  names(zoo[,-c(grep("aphni",     names(zoo)),
              grep("osmin",       names(zoo)),
              grep("epto",        names(zoo)),
              grep("otifer|Anuraeopsis|Ascomorpha|Asplanchna|Brachi|Collotheca|Conochilus|Cupelopagis|Euchlanis|Filinia|Kellicottia|Keratella|Lecane|Monostyla|Nothalca|Ploesoma|Polyarthra|Synchaeta|Trichocerca",    names(zoo)),
              grep("iaphanoso",               names(zoo)),
              grep("yclop",                   names(zoo)),
              grep("alanoid|Limnocalanus",    names(zoo)),
              grep("opepo|Epischura|Harpacticoid",names(zoo)))])
zoo$BOSMINA      <- rowSums(zoo[,grep("osmin",names(zoo))])     # small grazer
zoo$DAPHNIA      <- rowSums(zoo[,grep("aphni",names(zoo))])     # medium grazer
zoo$DIAPHANOSOMA <- rowSums(zoo[,grep("iaphanoso",names(zoo))]) # medium grazer
zoo$LEPTO        <- rowSums(zoo[,grep("epto", names(zoo))])      # predator
zoo$CYCLOP       <- rowSums(zoo[,grep("yclop",names(zoo))])     # copepod
zoo$CALANOID     <- rowSums(zoo[,grep("alanoid|Limnocalanus",
                                              names(zoo))])   # copepod
zoo$ROTIFER      <- rowSums(zoo[,grep("otifer|Anuraeopsis|Ascomorpha|Asplanchna|Brachi|Collotheca|Conochilus|Cupelopagis|Euchlanis|Filinia|Kellicottia|Keratella|Lecane|Monostyla|Nothalca|Ploesoma|Polyarthra|Synchaeta|Trichocerca",names(zoo))])    # small grazer
zoo$COPEPOD      <- rowSums(zoo[,grep("opepo|Epischura|Harpacticoid",names(zoo))])    # large grazer


# Group by very large groups (i.e. grazers, predators)
zoo$PREDATOR    <- rowSums(zoo[,grep("Polyphemus|LEPTO|Holopedium",names(zoo))])   
zoo$GRAZER_sm   <- rowSums(zoo[,grep("BOSMINA|Alon|Chyd|ROTIF|leurox|ladocer",names(zoo))])  
zoo$GRAZER_lr   <- rowSums(zoo[,grep("DAPHNIA|Sida|DIAPHA|CYCLOP|CALANO|opep|Eurycercus|aupl",names(zoo))])  
zoo$PARASITIC   <- zoo[,grep("Ergasilus",names(zoo))]
  
# There should be no species left here if we selected all species (only VisitDate and StationID)
whatsleft[-c(grep("Polyphemus|LEPTO|Holopedium",whatsleft),
            grep("BOSMINA|Sida|Alon|Chyd|ROTIF|leurox|ladocer",whatsleft),
            grep("DAPHNIA|DIAPHA|CYCLOP|CALANO|COPEPOD|Eurycercus|aupl",whatsleft),
            grep("Ergasilus",whatsleft))]

# Add explanatory variables to my dataframe
zoo$Year         <- as.numeric(format(as.Date(zoo$VisitDate, "%Y-%m-%d"),"%Y"))
zoo$Month        <- as.numeric(format(as.Date(zoo$VisitDate, "%Y-%m-%d"),"%m"))
zoo$yday         <- as.numeric(format(as.Date(zoo$VisitDate, "%Y-%m-%d"),"%j"))

```

## Number of organisms per groups over the years

This is the number of organisms per group over the years:

```{r visualize abundance over years, echo=FALSE, message=FALSE, warning=FALSE}
zoo_summ <- melt(zoo[,c("VisitDate", "StationID","LEPTO", "DAPHNIA", "BOSMINA", "PARASITIC")], id.vars = c("VisitDate", "StationID"))
pzoo <- ggplot(zoo_summ, aes(VisitDate,value,color=variable)) + geom_point() +
  stat_smooth() +
  xlab("Visit Date") + ylab("# organisms per m3")

ggplotly(pzoo)
```
`r fig_cap("zoop_evol", caption = "Evolution of zooplankton abundance in Lake Champlain")`

```{r View abundance per station, fig.cap = c(cap1,cap2,cap3),echo=FALSE, message=FALSE, warning=FALSE}
cap1 = fig_cap("zoop_pred", caption = "Evolution of predators zooplankton abundance in Lake Champlain")
ggplot(data = zoo, aes(x = VisitDate, y = PREDATOR)) + geom_point() +
  facet_wrap(~as.factor(zoo$StationID)) + ylim(0,20000)

cap2=fig_cap("zoop_lrgraz", caption = "Evolution of large grazers zooplankton abundance in Lake Champlain")
ggplot(data = zoo, aes(x = VisitDate, y = GRAZER_lr)) + geom_point() +
  facet_wrap(~as.factor(zoo$StationID)) + ylim(0,100000)

cap3=fig_cap("zoop_smgraz", caption = "Evolution of small grazers zooplankton abundance in Lake Champlain")
ggplot(data = zoo, aes(x = VisitDate, y = GRAZER_sm)) + geom_point() +
  facet_wrap(~as.factor(zoo$StationID)) + ylim(0,250000)

```

## Biomass

Calculate biomass 
```{r get biomass, message=FALSE, warning=FALSE}
# Biomass values were obtained from the size_zoop table
# When multiplier = 0, we're still working on collecting the data
# Biomass values      <- old zoop counts  ug/m3    g/m3      g/km3        t/km3
zoo$BOSMINA_biom      <- zoo$BOSMINA      * 1.76   / 1000000 * 1000000000 / 1000000
zoo$DAPHNIA_biom      <- zoo$DAPHNIA      * 32.15  / 1000000 * 1000000000 / 1000000
zoo$DIAPHANOSOMA_biom <- zoo$DIAPHANOSOMA * 6.32   / 1000000 * 1000000000 / 1000000
zoo$LEPTO_biom        <- zoo$LEPTO        * 113.31 / 1000000 * 1000000000 / 1000000
zoo$Polyphemus_biom <- 
         zoo$`Zoo_Polyphemus pediculus`   * 12.84  / 1000000 * 1000000000 / 1000000
zoo$Holopedium_biom <- 
         zoo$`Zoo_Holopedium gibberum`    * 158.15 / 1000000 * 1000000000 / 1000000
zoo$CYCLOP_biom       <- zoo$CYCLOP       * 16.52  / 1000000 * 1000000000 / 1000000
zoo$CALANOID_biom     <- zoo$CALANOID     * 0.00   / 1000000 * 1000000000 / 1000000
zoo$ROTIFER_biom      <- zoo$ROTIFER      * 0.2556 / 1000000 * 1000000000 / 1000000 
zoo$COPEPOD_biom      <- zoo$COPEPOD      * 1.58   / 1000000 * 1000000000 / 1000000
zoo$PREDATOR_biom     <- zoo$PREDATOR     * 94.77  / 1000000 * 1000000000 / 1000000
zoo$GRAZER_sm_biom    <- zoo$GRAZER_sm    * 1.86   / 1000000 * 1000000000 / 1000000
zoo$GRAZER_lr_biom    <- zoo$GRAZER_lr    * 23.59  / 1000000 * 1000000000 / 1000000
zoo$PARASITIC_biom    <- zoo$PARASITIC    * 0.00   / 1000000 * 1000000000 / 1000000

# Group by very large groups (i.e. grazers, predators)
zoo$PREDATOR_biom    <- rowSums(zoo[,grep("Polyphemus_biom|LEPTO_biom|Holopedium_biom",names(zoo))]) 
zoo$GRAZER_sm_biom   <- rowSums(zoo[,grep("BOSMINA_biom|Alon_biom|Chyd_biom|ROTIFER_biom|leurox_biom|ladocer_biom",names(zoo))])  
zoo$GRAZER_lr_biom   <- rowSums(zoo[,grep("DAPHNIA_biom|Sida|DIAPHANOSOMA_biom|CYCLOP_biom|CALANOID_biom|opep_biom|Eurycercus_biom|aupl_biom",names(zoo))])  


# Focus on main lake
zoo19 <- zoo[zoo$StationID==19,c("VisitDate", "StationID", "Year", "Month", "yday",
                                   "PREDATOR_biom", "GRAZER_sm_biom", "GRAZER_lr_biom", "PARASITIC_biom")]


plot(zoo19$VisitDate[zoo19$PREDATOR != 0],zoo19$PREDATOR[zoo19$PREDATOR != 0], pch=20, col=adjustcolor("black", alpha.f = .3) , xlab="Visit Date", ylab="Biomass, t/km2")
points(zoo19$VisitDate[zoo19$GRAZER_lr != 0],zoo19$GRAZER_lr[zoo19$GRAZER_lr != 0], pch=20, col=adjustcolor("blue", alpha.f = .3) )
points(zoo19$VisitDate[zoo19$GRAZER_sm != 0],zoo19$GRAZER_sm[zoo19$GRAZER_sm != 0], pch=20, col=adjustcolor("red", alpha.f = .3) )
points(zoo19$VisitDate[zoo19$PARASITIC != 0],zoo19$PARASITIC[zoo19$PARASITIC != 0], pch=20, col=adjustcolor("green", alpha.f = .3) )


#head(plktn)
#head(zoo19)

zoo19 <- zoo19[rowSums(zoo19[,c("PREDATOR_biom", "GRAZER_sm_biom", "GRAZER_lr_biom", "PARASITIC_biom")])>0,]
zoo19_month <- aggregate(zoo19[,c("PREDATOR_biom", "GRAZER_sm_biom", "GRAZER_lr_biom", "PARASITIC_biom")], list(format(zoo19$VisitDate, "%Y-%m")), mean)
#print(zoo19_month)
zoo19_month$Year   <- as.numeric(substr(zoo19_month$Group.1,1,4))
zoo19_month$Month  <- as.numeric(substr(zoo19_month$Group.1,6,7))
zoo19_month_Predator <- dcast(zoo19_month, Year ~ Month, value.var = "PREDATOR_biom", fun.aggregate = mean)
zoo19_month_Grazer_sm <- dcast(zoo19_month, Year ~ Month, value.var = "GRAZER_sm_biom", fun.aggregate = mean)
zoo19_month_Grazer_lr <- dcast(zoo19_month, Year ~ Month, value.var = "GRAZER_lr_biom", fun.aggregate = mean)

zoo19_month_Predator
```



# Mysis

`r tab_cap("mysis", caption = "Head of the table with mysis data")`
```{r load datasets, echo=FALSE, message=FALSE, warning=FALSE}
mysis <- read.delim(paste0(getpath4data(), "data_from_Jason/mysis_long_term.txt"))
mysis$Date <- parse_date_time(mysis$Date, orders = c("d/m/Y"))
head(mysis)
```

Mysis data are available from `r paste(min(mysis$Year),"to",max(mysis$Year))`, usually from months `r paste(min(month(mysis$Date)),"to",max(month(mysis$Date)))`, for `r length(unique(mysis$Station))` (`r paste(unique(mysis$Station),sep="", collapse=", ")`).

Lake Champlain Mysid net tows = 0.5 m net, whole water column tows. 
To calculate the net area, we use the formula for the volume of a cylinder:<br>

$V = r^2 * \pi $
<br>

```{r get net area fo mysis tow}
diameter_tow = 0.5
net_area = (diameter_tow/2)^2*pi
```

The #/m<sup>2</sup> is obtained that by dividing the number counted in the tow by the volume.

```{r recalculate the number of mysis per m2}
mysis$number.m2 = mysis$Total/net_area
```

## Calculate average per year.


```{r average mysis per year, echo=FALSE, message=FALSE, warning=FALSE}
mysis_summ_d <- as.data.frame(with(mysis, tapply(number.m2
  ,list("Year"=Year,"depth_round"=round(mysis$Depth/10)*10), mean, na.rm=T)))
mysis_summ_d <- data.frame(melt(mysis_summ_d), "Year"=rep(unique(mysis$Year), times=ncol(mysis_summ_d)))

ggplot(mysis_summ_d, aes(variable, Year)) + geom_tile(aes(fill = value),colour = "white") + scale_fill_gradient(low = "white", high = "steelblue") + ggtitle("Total number of mysis per m2") + xlab("Average depth (rounded to 10 m)")

```
<br>
`r fig_cap("mysis vs years", caption="Total number of mysis per m2")`

`r tab_cap("mysis stats", caption="Average number and weight of mysis per year (with standard deviation)")`
```{r summary biomass per year, echo=FALSE, message=FALSE, warning=FALSE}
mysis_summ <- data.frame(
  "Year"=unique(mysis$Year),
  "mean"=as.vector(with(mysis, tapply(number.m2
  ,list("Year"=Year), mean, na.rm=T))),
  "sd"=as.vector(with(mysis, tapply(number.m2
  ,list("Year"=Year), sd, na.rm=T))))

mysis_summ

```


```{r view data}
with (data=mysis_summ
      , expr = errbar(Year,mean,mean+sd/2,mean-sd/2, pch=16, cap=.01, xlab="Year",ylab="mean density (# mysis/m2)", cex=.8)
        )
```
<br>
`r fig_cap("mysis abundance 1", caption="Average (and sd) mysis density per year")`

## Changepoint analysis

```{r do changepoint analysis}
#library(changepoint)
ans=cpt.mean(as.vector(mysis_summ$mean))
summary(ans)
#mysis_summ$Year[cpts(ans)]
```

Changepoint detected in `r mysis_summ$Year[cpts(ans)]`.

```{r plot changepoint on graph}
lm1 <- lm(mysis_summ$mean[1:cpts(ans)]~mysis_summ$Year[1:cpts(ans)])
lm2 <- lm(mysis_summ$mean[-c(1:cpts(ans))]~mysis_summ$Year[-c(1:cpts(ans))])


with (data=mysis_summ
      , expr = errbar(Year,mean,mean+sd/2,mean-sd/2, pch=16, cap=.01, xlab="Year",ylab="mean density (# mysis/m2)", cex=.8)
        )



lines(mysis_summ$Year[1:cpts(ans)], c(lm1$coefficients[1]+mysis_summ$Year[1:cpts(ans)]*lm1$coefficients[2]), lwd=2)

lines(mysis_summ$Year[-c(1:cpts(ans))], c(lm2$coefficients[1]+mysis_summ$Year[-c(1:cpts(ans))]*lm2$coefficients[2]), lwd=2)
                            


```
<br>
`r fig_cap("mysis abundance 2", caption="Average (and sd) mysis density per year. Linear regression for the two periods detected.")`


## Get biomass in grams
Using mysis weight data from Chesapeake Bay (given by Rosie) to get an estimate of average/median weigth.

```{r read mysis data}
mysis_w <- read.delim(paste0(getpath4data(),"data_from_Rosie/ChesapeakeBay_Mysis_oxygen_consumption_data_RC_AN.txt"))
```

```{r plot mysis weigth, echo=FALSE, message=FALSE, warning=FALSE}
p1<- ggplot(data=mysis_w, aes(mysis_w$Wet_wt, fill=Sex)) + 
  geom_histogram(col="black") + xlab("Mysis wet weight (mg)") + theme(legend.position = "none")
p2<- ggplot(data=mysis_w, aes(mysis_w$Dry_wt..mg., fill=Sex)) + 
  geom_histogram(col="black") + xlab("Mysis dry weight (mg)")

grid.arrange(p1,p2,nrow=1, widths=c(1,1.3))

mysis_w_mean    = mean(mysis_w$Wet_wt, na.rm=T)
mysis_w_median  = median(mysis_w$Wet_wt, na.rm=T)
```
`r fig_cap("mysis dry and wet weigth", caption="Distribution of mysis dry and wet weigth")`


The distribution is not normal, so median value should be chosen over mean. <br>
* mean: `r mysis_w_mean` <br
* median: `r mysis_w_median`<br>

We multiply the mysis count per this biomass:
```{r calculate mean mysis biomass and visualise}
mysis_summ$mean_w <- mysis_summ$mean*mysis_w_median
mysis_summ$sd_w <- mysis_summ$sd*mysis_w_median

with (data=mysis_summ
      , expr = errbar(Year,mean_w,mean_w+sd_w/2,mean_w-sd_w/2, pch=16, cap=.01, xlab="Year",ylab="mean biomass (mg mysis/m2)", cex=.8)
        )
```
<br>`r fig_cap("mysis biomass 1", caption="Average (and sd) mysis biomass per year")`

## Get biomass in t/km2
1 g.m<sup>-2</sup> = 1,000,000 t.m<sup>-2</sup> = 1,000,000/1e-6 t.km<sup>-2</sup> = 1t.km<sup>-2</sup> 


# Benthic invertebrates

__Data:__ <br>
  * Monitoring data initiated during the initial colonization

From the <a href="https://dec.vermont.gov/watershed/lakes-ponds/aquatic-invasives/monitoring/zebra-mussels">VTDEC website</a>:
"The Vermont Department of Environmental Conservation (VTDEC), in cooperation with the Lake Champlain Basin Program, initiated the Lake Champlain Zebra Mussel Monitoring Program in 1994 to track the zebra mussel's distribution through the lake. Reports are provided annually.

The efficient combination of the Zebra Mussel Monitoring Program with the Long-term Water Quality and Biological Monitoring Program provides a nationally unique lake database. Information on veliger and juvenile densities monitored consistently since the initial colonization is obtained concurrently with comprehensive water quality data. This information is critical to determining the effects of zebra mussels on the Lake Champlain ecosystem and the potential risk and impact of zebra mussel colonization of other water bodies.

Zebra mussel monitoring includes veliger (larvae), settled veliger (juvenile), and adult life stages at open-water and nearshore lake stations, lake tributaries and inland lakes. Greater emphasis is placed on veliger monitoring, as it is in their pelagic stage that zebra mussels are most easily spread and sampled in Lake Champlain.

Zebra mussels in Lake Champlain continued to reproduce and settle successfully during 2018. The range expansion in the Northeast Lake is now complete.  Zebra mussel adults have been well established in the South, Central, and Northwest Lake since 1996. Season settling plates retrieved at nearshore stations in these areas in 2018 confirm continued reproductive success, and similar growth rates as in years past. 
"

__Other resources:__ <br
<a href="https://dec.vermont.gov/sites/dec/files/wsm/lakes/images/lp_zm-sitemap.gif">Map of sampling locations</a>
  
__STA19__ in the veligers dataset is the station in the Main Lake. I'm going to extract these density to start with (veligers/m3)

```{r read ZM data from state, message=FALSE, warning=FALSE, include=FALSE}
ZMjuv <- read.delim(paste0(getpath4data(),"VTDEC_zebra_mussel/Juvenile_ZM.txt"))
ZMvel <- read.delim(paste0(getpath4data(),"VTDEC_zebra_mussel/Veliger_ZM.txt"))
ZMjuv$Date <- as.Date(ZMjuv$Date, format="%d/%m/%Y")
ZMvel$Date <- as.Date(ZMvel$Date, format="%d/%m/%Y")
head(ZMjuv)
tail(ZMvel)
unique(ZMjuv$Lake.Sta)
unique(ZMvel$LakeSta)


```

```{r get summary per year veligers station 19}
ZMvel_summ <- as.data.frame(with(ZMvel, tapply(ZMvel[,"Density"],
  list("Year"=Expr1,"station"=LakeSta), mean, na.rm=T)))
ZMvel_summ <- ZMvel_summ[,colSums(ZMvel_summ,na.rm=T)>0]

plot(rownames(ZMvel_summ),ZMvel_summ$STA19, xlab="Year", ylab="Seasonal mean density (veligers/m3)", type="l")

#Data extracted from the graph using WebPlotDigitizer on Google Chrome because excel is missing some lines
#Source plot:
# https://dec.vermont.gov/sites/dec/files/wsm/lakes/ans/images/1994-2018%20NE%20Seasnly-int%20graphs1.pdf
#log scale
ZMvel_STA19 <- data.frame("Year"=c(1994:2005,2011:2018), "Density"=c(0,167,4734,4973,12056,11477,20713,19718,6359,7017,8133,17871,2051,7750,1953,2623,5487,8974,10926,7371))
#lines(ZMvel_STA19$Year,ZMvel_STA19$Density, col="pink")
#==> not exactly the same data, what's wrong? Ask Pete Stangel maybe

```



# Sculpin

# Trout-perch

__Data: __
<br> 
We can calculate CPUE from trawling (bycatch data, 2016-2018). A length distribution for trout perch collected in Lake Champlain is available from measurements collected during the WFB 161 lab (fall 2018). Finally, total length for fish aged `r paste0(min(TroutperchLM1$age),"-",max(TroutperchLM1$age))` (fish aged using scales) is available through the FSAdata library (D. Ogle).


## Data from Ellen -- bycatch from the LT recruitment survey

### Sample locations for trout-perch data 
Data for trout perch (`r nrow(byc[byc$species=="trout-perch",])` observations) were collected and are summarized in the bycatch dataframe. Observations on `r length(summary(byc$site[byc$species=="trout-perch"]))` sites -- sites with higher number of observation are listed below.

```{r where are trout perch found, echo=FALSE, message=FALSE, warning=FALSE}
summ_tp <- summary(byc$site[byc$species=="trout-perch"])
summ_tp[order(summ_tp, decreasing = T)][1:5]

```


```{r trout perch subset, message=FALSE, warning=FALSE, include=FALSE}
#trout perch subset
tps <- byc[byc$species=="trout-perch" & byc$site=="burlington bay",]
tps <- byc[byc$species=="trout-perch" ,]
tps$tote..fullness.[tps$tote..fullness.=="na"] <- NA
tps$est_number[tps$est_number=="na"] <- NA
tps$est_number <- as.numeric(paste(tps$est_number))

# when info is given in tote..fullness., sometimes it says only the fraction, sometimes x/x tote full, sometimes x/x fish tote.
# to qa/qc, remove all mention of tote, keep only the fraction
tps$tote..fullness.[grep("tote", tps$tote..fullness.)] <- substr(tps$tote..fullness.[grep("tote", tps$tote..fullness.)],1,3)
gsub(" ",NA,tps$tote..fullness.)

#View(tps[,c("est_number","tote..fullness.")])
#View(byc[,c("species","est_number","tote..fullness.")])

# Convert to character the 3 columns with fullness info to ease the cleaning process
tps$tote..fullness. <- as.character(tps$tote..fullness.)
tps$X..of.tote      <- as.character(tps$X..of.tote)
tps$est_number      <- as.character(tps$est_number)

```

Unfortunately, there's only one correspondance between 'tote fulness' and 'estimated number': 100 (est. number) = 1 layer.
Talking to Ellen and Pascal, they told me it was too high. It's most likely there's only 40 or so. Talk to fisheries biologist working on other lakes maybe?
For the sake of getting a bit more estimates, trying to convert tote fullness into number.


```{r how many layers of trout perch in 1/4 of a tote}
# 1 layer = 100? 35? how many? find out.
# It's easier (at least for me) to count layers over 1/4 of a fish tote
# I'm assuming there's about 8 layers in 1/4 of a tote
# Meaning that in a tote, there are
#                       number layer in 1/4  * bring back to 1/1
layer_tote_troutperch = 8                    *        4
# tote_troutperch = number in layer *  number layer tote
( tote_troutperch =      40         * layer_tote_troutperch)
```


Now that we somewhat have an estimation of number per tote, convert that back and get an idea of the average abundance. <br>

Note that right now, we are trusting __estimated number__ over __tote fullness__ over __percentage of tote__. <br>

```{r convert percentage and fullness into number of fish}
tps$fish_count <- rep(NA, nrow(tps))

for (i in 1:nrow(tps)) {
  # Turn all to false at the beginning of each loops
  C1=F;C2=F;C3=F
  if(i==1) {n1=0;n2=0;n3=0;nbis1=0;nbis2=0;nbis3=0}
  
  # Case 1: percentage is available
  if(!is.na(tps$X..of.tote[i]) & tps$X..of.tote[i]!="na" & tps$X..of.tote[i]!="") {
    n1=n1+1
    C1=T
    options(warn=-1)
    tps$fish_count[i] = as.numeric(gsub("[\\%,]", "", tps$X..of.tote[i]))
    options(warn=0)
    # Sometimes there's some fraction in the percent column -- consider this as an error
    if(is.na(tps$fish_count[i])) {n1=n1-1}
  }
  
  # Case 2: tote fullness are available
  if(!is.na(tps$tote..fullness.[i]) & tps$tote..fullness.[i]!="") {
    n2=n2+1
    C2=T
    # This is the most complicated case, because sometimes it says layers, sometimes it's a fraction, sometimes it's just a multiplying of layers
    if(tps$tote..fullness.[i]=="full") {
      tps$fish_count[i] = tote_troutperch
    } else {
      if (any(c("layer","layers","single") %in% unlist(strsplit(tps$tote..fullness.[i]," ")))) {
      # If there's a layer, we'll reuse or estimate of number of layers per tote
      nb_layer = layer_tote_troutperch
      tps$fish_count[i] = gsub("layer|layers", "", tps$tote..fullness.[i])
      if ("single" %in% unlist(strsplit(tps$fish_count[i]," "))) {
       tps$fish_count[i] = gsub("single ", "1", tps$fish_count[i])
      }
    } else {
      # if no "layer" in the line, the fraction refers to the full tote, so we're defaulting nb_layer to 1
      nb_layer = 1
      tps$fish_count[i] = tps$tote..fullness.[i]
    }
      
      # If there's a space at the beginning, it can mess up everything. 
      # Removing all first spaces
      while(unlist(strsplit(tps$fish_count[i],""))[1]==" ") tps$fish_count[i] = paste(unlist(strsplit(tps$fish_count[i],""))[-1], sep = "",collapse = "")
      
      # if it's a fraction, we'll have a "/"
    if ("/" %in% unlist(strsplit(tps$fish_count[i],""))) {
      if(" " %in% unlist(strsplit(tps$fish_count[i],split = ""))) {
        tps$fish_count[i] = tote_troutperch *
       as.numeric(unlist(strsplit(tps$fish_count[i],split = "/"))[1]) /
       as.numeric(unlist(strsplit(unlist(strsplit(tps$fish_count[i],split = "/"))[2]," "))[1]) *
       as.numeric(unlist(strsplit(unlist(strsplit(tps$fish_count[i],split = "/"))[2]," "))[2]) /
       nb_layer
      } else {
                tps$fish_count[i] = tote_troutperch *
       as.numeric(unlist(strsplit(tps$fish_count[i],split = "/"))[1]) /
       as.numeric(unlist(strsplit(tps$fish_count[i],split = "/"))[2])  /
       nb_layer
      }
    } else {
      tps$fish_count[i] = tote_troutperch * as.numeric(tps$fish_count[i]) / nb_layer
  }
    }
    
    } 
  
  
  # Case 3: estimated numbers are available
  if(!is.na(tps$est_number[i])) {
    n3=n3+1
    C3=T
    tps$fish_count[i] = tps$est_number[i]
  }
  
  # Were indicators present twice?
  if(C1&C2) nbis1 = nbis1 + 1
  if(C1&C3) nbis2 = nbis2 + 1
  if(C2&C3) nbis3 = nbis3 + 1
  
  if(all(is.na(tps$fish_count[i]) & any(C1|C2|C3))) stop(print(i))
  
  if(i==nrow(tps)) message(paste0(" ✓ There are ",n1," lines with info on percentage fullness. \n ✓ There are ",n2," lines with info on tote fullness. \n ✓ There are ",n3," lines with info on estimated number. \n ✕ There were ",nrow(tps)-n1-n2-n3, " lines with no info on how many fish are present.\n\n Note that:\n   - both percent and tote fullness were available for ",nbis1, " lines,\n   - both percent and est. number were available for ",nbis2, " lines,\n   - both est.number and tote fullness were available for ",nbis3, " lines."))
}

tps$fish_count <- as.numeric(tps$fish_count)
```

We now have estimated number of trout perch for `r length(which(!is.na(tps$fish_count)))` sampling event out of `r nrow(tps)` total events with trout perch collected (i.e., `r round(length(which(!is.na(tps$fish_count)))/nrow(tps)*100)`% of the events).

It'd be easier to assign the GPS coordinates to each date to be sure there's not a big spot with / without trout-perch. <br>
[Code hidden to save plot on the .html, go to .Rmd to see it]

```{r match gps coordinates and duration trawls for trout perch surveys, message=FALSE, warning=FALSE, include=FALSE}
# Read LTeff compiled
melo_effort <- read.delim(paste0(getpath4data(),"data_from_Pascal/LTeff_compiled_RB.txt"))
tps$start.lon.dd <- rep(NA, nrow(tps))
tps$end.lon.dd <- rep(NA, nrow(tps))
tps$start.lat.dd <- rep(NA, nrow(tps))
tps$end.lat.dd <- rep(NA, nrow(tps))
tps$duration <- rep(NA, nrow(tps))
for (i in 1:nrow(tps)) {
  if(i==1) {n1=0;n2=0;whichn1=NULL}
  narrow2day <- melo_effort[as.character(melo_effort$date)==as.character(tps$date[i]),]
  narrow2trawl <- narrow2day[as.numeric(paste(narrow2day$netID))==as.numeric(paste(tps$netID[i])),]
  if(nrow(narrow2trawl)==1 & narrow2trawl$start.lat.dd != "#VALUE!" && as.numeric(paste(narrow2trawl$start.lat.dd)) > 0) {
    tps$start.lon.dd[i] <- as.numeric(paste(narrow2trawl$start.lon.dd))
    tps$end.lon.dd[i]   <- as.numeric(paste(narrow2trawl$end.lon.dd))
    tps$start.lat.dd[i] <- as.numeric(paste(narrow2trawl$start.lat.dd))
    tps$end.lat.dd[i]   <- as.numeric(paste(narrow2trawl$end.lat.dd))
    tps$duration[i]     <- substr(narrow2trawl$duration,4,5)
    n1 <- n1+1 # for info message
  } else {
      n2 <- n2+1 # for info message
      whichn1 <- c(whichn1,i)
      }
  # Info message
  if(i==nrow(tps)) message(paste0(" ✓ Found matching GPS coordinates for ", n1," of the trawls","\n ✕ No or several potential GPS coordinates were found for ", n2, " of the trawls (call 'whichn1' in the console to know which rows of the dataframe were not matched, or find the rows with NAs)." ))
}

tps$avg.lon.dd <- (tps$start.lon.dd+tps$end.lon.dd)/2
tps$avg.lat.dd <- (tps$start.lat.dd+tps$end.lat.dd)/2

```


`r fig_cap("troutperch number lake", caption="Circle size shows the number of trout-perch collected per sampling event. Colour scale refers to day of the year. Rectangle identify Burlington Bay. [you can zoom and unzoom on the map if you're reading the html version]", display = FALSE)`

`r fig_cap("troutperch number lake", display="cite")` shows the number of trout perch collected against the center location of the trawl. The radius show the number of fish captured. Note that we have CPUE (1 unit effort = 1 minute of trawling).

```{r get CPUE trout perch}
tps$fish_count_CPUE <- tps$fish_count/as.numeric(tps$duration)
```


```{r map the number of trout perch caught per lake location, echo=FALSE, message=FALSE, warning=FALSE}
# p1 <- ggplot(tps[!is.na(tps$fish_count) & tps$site=="burlington bay",], aes(x = avg.lon.dd, y = avg.lat.dd, size = fish_count, colour=yday)) + geom_point() + labs(x="Longitude", y="Latitude")
# 
# ggplotly(p1)

# create color palette and shapes
colfunc <- colorRampPalette(c("yellow", "red"))
pal <- colfunc(max(tps$yday)-min(tps$yday))

addLegendCustom <- function(map, colors, labels, sizes, opacity = 0.5){
      colorAdditions <- paste0(colors, "; width:", sizes, "px; height:", sizes, "px")
      labelAdditions <- paste0("<div style='display: inline-block;height: ", sizes, "px;margin-top: 4px;line-height: ", sizes, "px;'>", labels, "</div>")

      return(addLegend(map, colors = colorAdditions, labels = labelAdditions, opacity = opacity))
    }

troutperch_map <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  setView(lng = -73.255, lat = 44.465, zoom = 12) %>% 
  addCircleMarkers(
    lng = tps$avg.lon.dd,
    lat=tps$avg.lat.dd,
    radius = ifelse(!is.na(tps$fish_count_CPUE),as.numeric(tps$fish_count_CPUE)/5, 2),
    color = pal[as.numeric(tps$yday)-min(tps$yday)],
    stroke = FALSE, fillOpacity = 0.5
  )  %>%
  addRectangles(
    lng1=-73.290, lat1=44.45,
    lng2=-73.231, lat2=44.48,
    fillColor = "transparent"
  ) %>%
  addLegend("topright", title = "Day of the year",colors = pal[seq(1,(max(tps$yday)-min(tps$yday)),by = 30)],values = seq(1,(max(tps$yday)-min(tps$yday)),by = 30),
            labels=(min(tps$yday):(max(tps$yday)))[seq(1,(max(tps$yday)-min(tps$yday)),by = 30)]) 

troutperch_map

```
<br>`r fig_cap("troutperch number lake", display="full")`


`r fig_cap("troutperch per yday", caption="(a) Trout-perch CPUE across the year, (b) number of sampling events per day of the year and (c) number of sampling events with CPUE reported.", display = FALSE)`

__Question for Ellen__: CPUE are always lower during summer (see `r fig_cap("troutperch per yday", display="cite")`), could it be because there's more work with Lake Trout, so data are only recorded when CPUEs are low? 

```{r trout perch count vs. day, echo=FALSE, message=FALSE, warning=FALSE}
p1 <- ggplot(tps, aes(yday, fish_count_CPUE )) + geom_point() + ggtitle("a. CPUE per day of the year") + xlab("Day of the year") + ylab("CPUE (UE= 1 minute of bottom trawling)")
p2 <- ggplot() + geom_histogram(data=tps, aes(yday,fill="All sampling events")) + stat_bin(bins = 30) + geom_histogram(data=tps[!is.na(tps$fish_count_CPUE),], aes(yday, fill="Sampling events with \nCPUE data reported")) + stat_bin(bins = 30) + ggtitle("b. Sampling events per day of the year") + theme(legend.title = element_blank())  + xlab("Day of the year") + ylab("Number of sampling events")
grid.arrange(p1,p2,nrow=1, widths=c(2.5,4))
```
<br>`r fig_cap("troutperch per yday", display="full")`

## Data from WFB 161 - Fall 2018

```{r read data from WFB 161 fall 2018, message=FALSE, warning=FALSE, include=FALSE}
# Read data
data <- read.delim("Input/WFB161_2018_Lab3_data.txt", header = T)
metadata <- read.delim("Input/WFB161_2018_Lab3_metadata_2.txt", header = T)

length <- read.delim("Input/WFB161_2018_Lab3_length.txt")
length_sp <- length[1,]
length <- length[-1,]
indx <- sapply(length, is.factor)
length[indx] <- lapply(length[indx], function(x) as.numeric(as.character(x)))

str(length)

troutperch <- read.delim("Input/WFB161_2018_Lab3_length_troutperch.txt", header = T)
alewife <- read.delim("Input/WFB161_2018_Lab3_length_alewife.txt", header = T)

```
 
Trawl on September 17th (Monday lab) and 19th (Wednesday lab), 2018, as part of Fisheries Biology and Techniques (UVM/WFB 161). We met at 1pm with the first group, conducted 2 trawls, and then met at 3.30pm with the second group for the 2nd trip (2 more trawls). That makes a total of 8 trawls (4 trawls per afternoon). Metadata collected are presented below: <br>

`r metadata`


`r fig_cap("trout perch trawl", caption="Start and end depth of trawls conducted in 2018 for WFB 161.", display=FALSE)`

Trawls were done deeper and deeper (`r fig_cap("trout perch trawl", display="cite")`).

```{r depth trawls wfb 161, echo=FALSE, message=FALSE, warning=FALSE}
plot(1:8,colMeans(metadata[5:6,-1]), pch=20, ylim=c(min(metadata[5:6,-1]), max(metadata[5:6,-1])), xlab="# trawls", ylab="depth (m)")
for (i in 1:8) {
  lines(c(i,i),c(metadata[5,i+1], metadata[6,i+1]), col="grey")
}
points(1:8,metadata[5,-1], pch=20, col="grey", cex=.8)
points(1:8,metadata[6,-1], pch=15, col="brown1")
points(1:8,colMeans(metadata[5:6,-1]), cex=1.4, pch=20)
legend("bottomrigh", legend = c("Start", "End", "Average depth"), pch=c(20,15,20), col=c("grey", "brown1","black"), bty='n')

```
<br>
`r fig_cap("trout perch trawl", display="full")`


Length were collected for `r nrow(troutperch)` trout-perch (and `r nrow(alewife)` alewifes). Note that we collected more trout-prech for each trawl but stopped measuring after 115-125 individuals. Trawling locations show different size distribution.

```{r histogram trout perch length, fig.cap=c(cap1,cap2,cap3), echo=FALSE, message=FALSE, warning=FALSE}
cap1=fig_cap("size distrib trout perch 1", caption="size distribution of trout perch")
hist(troutperch$Length_trout_perch, breaks = c(seq(0,130,2)), col="grey", xlab="length class (mm)", main="")
# hist(troutperch$Length_trout_perch[troutperch$Trawl==1], breaks = c(seq(0,130,5)), col="grey", ylim = c(0,100))
# par(new=T)
cap2=fig_cap("size distrib trout perch 2", caption="size distribution of trout perch - shallower and deeper sites evidenced")
hist(troutperch$Length_trout_perch[troutperch$Trawl==1|troutperch$Trawl==2|troutperch$Trawl==3], breaks = c(seq(0,130,5)), col=adjustcolor(col = "blue",alpha.f = .3), ylim = c(0,100), main='', xlab='length class (mm)')
# par(new=T)
# hist(troutperch$Length_trout_perch[troutperch$Trawl==4], breaks = c(seq(0,130,5)), col=adjustcolor(col = "blue",alpha.f = .3))
par(new=T)
hist(troutperch$Length_trout_perch[troutperch$Trawl==5|troutperch$Trawl==6|troutperch$Trawl==7|troutperch$Trawl==8], breaks = c(seq(0,130,5)), col=adjustcolor(col = "red",alpha.f = .3), ylim = c(0,100), main='', xlab='')
legend("topleft", legend = c("Trawls #1-3", "Trawls #5-8"), fill =c(adjustcolor(col = "blue",alpha.f = .3),adjustcolor(col = "red",alpha.f = .3)), bty='n')
cap3=fig_cap("size distrib trout perch 3", caption="size distribution of trout perch")
boxplot(troutperch$Length_trout_perch~troutperch$Trawl , xlab="trawls", ylab="length (mm)")

```

Size between trout-perch caught in trawls 1-3 and trawls 5-8 is significantly different (t-test, t = `r t.test(troutperch$Length_trout_perch[troutperch$Trawl==1|troutperch$Trawl==2|troutperch$Trawl==3|troutperch$Trawl==4],troutperch$Length_trout_perch[troutperch$Trawl==5|troutperch$Trawl==6|troutperch$Trawl==7|troutperch$Trawl==8])$statistic`, p-value = `r t.test(troutperch$Length_trout_perch[troutperch$Trawl==1|troutperch$Trawl==2|troutperch$Trawl==3|troutperch$Trawl==4],troutperch$Length_trout_perch[troutperch$Trawl==5|troutperch$Trawl==6|troutperch$Trawl==7|troutperch$Trawl==8])$p.value`).

Below is a more detailed characterisation of each trawl.

```{r various stats of each trawl trout perch WFB161, message=FALSE, warning=FALSE}
library(psych)
describeBy(troutperch$Length_trout_perch, group = troutperch$Trawl)

```

We stopped measuring for each trawl after 115-125 individuals -- below is a code I used to show the students that if we had sample less fish, we would have still find the same mean for each trawl.

```{r resample trout perch data, echo=FALSE, message=FALSE, warning=FALSE}
# resample data
# independent 2-group t-test
# Principle: t.test(y1,y2) # where y1 and y2 are numeric
par(mfrow=c(1,2), mar=c(8.5,4.1,3.8,2.1))
plot(c(10,100), c(0,1), xlab="number of individuals measured", ylab="p-value", pch=NA)
rect(xleft = 0,ybottom = -0.1,xright = 120,ytop = 0.05, col=adjustcolor("grey", alpha.f = .5), border=NA)
mycol <- wes_palette("Darjeeling1", 8, type="continuous")
mtext("a. Mean comparison (independent\n     2-group t-test)", font=2, side = 3, line = .8, at = 5, adj = 0)
for (i in 1:8) {
  for (j in seq(10,100,10)){
    myttest <- NULL
    for (k in 1:100) {
      mysample <- sample(troutperch$Length_trout_perch[troutperch$Trawl==i],size = j, replace = F)
      ttest <- t.test(mysample,troutperch$Length_trout_perch[troutperch$Trawl==i])
      myttest <- c(myttest,ttest$p.value)
    }
    points(j, mean(myttest), pch=20, col=mycol[i])
  }
}

plot(c(10,100), c(0,1), xlab="number of individuals measured", ylab="p-value", pch=NA)
rect(xleft = 0,ybottom = -0.1,xright = 120,ytop = 0.05, col=adjustcolor("grey", alpha.f = .5), border = NA)
mycol <- wes_palette("Darjeeling1", 8, type="continuous")
mtext("b. Variance comparison (independent\n     2-group var-test)", font=2, side = 3, line = .8, at = 5, adj = 0)
for (i in 1:8) {
  for (j in seq(10,100,10)){
    myvartest <- NULL
    for (k in 1:100) {
      mysample <- sample(troutperch$Length_trout_perch[troutperch$Trawl==i],size = j, replace = F)
      vartest <- var.test(mysample,troutperch$Length_trout_perch[troutperch$Trawl==i], alternative = "two.sided")
      myvartest <- c(myvartest,vartest$p.value)
    }
    points(j, mean(myvartest), pch=20, col=mycol[i])
  }
}

par(xpd=T)
legend(x = -5, y=-.39, legend = paste("trawl", 1:3), col=mycol[1:3], pch=20, bty='n')
legend(x = 30, y=-.39, legend = paste("trawl", 4:6), col=mycol[4:6], pch=20, bty='n')
legend(x = 65, y=-.39, legend = paste("trawl", 7:8), col=mycol[7:8], pch=20, bty='n')
par(xpd=F)
par(mfrow=c(1,1), mar=c(5.1,4.1,4.1,2.1))

```
<br>
`r fig_cap("trout perch sample size stats", caption="Testing robustness of our results by changing sample size")`

## Data from Lake Michigan

The assigned ages (by scales), total lengths (mm), and sexes of Troutperch (Percopsis omsicomaycus) captured in southeastern Lake Michigan, from the library _FSAdata_.

```{r data trout perch from lake michigan }
?TroutperchLM1
head(TroutperchLM1)
```


```{r plot data trout perch from lake michigan, echo=FALSE, message=FALSE, warning=FALSE}
# make the plot average into a function so I don't have to repeat the code twice
plot_mean_WFB_troutperch <- function() {
  polygon(x=c(-2,12,12,-2), y=  quantile(troutperch$Length_trout_perch[troutperch$Trawl<4], c(.25, .25, .75, .75)), col=adjustcolor("cadetblue",alpha=.2), border = NA)
  abline(h=mean(troutperch$Length_trout_perch[troutperch$Trawl<4]), lty=2, col="cadetblue")
  polygon(x=c(-2,12,12,-2), y=  quantile(troutperch$Length_trout_perch[troutperch$Trawl>=4], c(.25, .25, .75, .75)), col=adjustcolor("coral2",alpha=.2), border = NA)
  abline(h=mean(troutperch$Length_trout_perch[troutperch$Trawl>=4]), lty=2, col="coral2")
  legend("bottomright", title = "Trout-perch average length \n(and 25-75th percentiles)", legend = c("in trawls 1-3","in trawls 4-8"), lty=2,col=c("cadetblue","coral2"), cex=.8, bty='n')
}

# Actual plot
op <- par(mfrow=c(1,2),pch=19)
plot(tl~age,data=TroutperchLM1,subset=sex=="f",main="female", ylim=c(min(TroutperchLM1$tl), max(TroutperchLM1$tl)))
plot_mean_WFB_troutperch()
plot(tl~age,data=TroutperchLM1,subset=sex=="m",main="male", ylim=c(min(TroutperchLM1$tl), max(TroutperchLM1$tl)))
plot_mean_WFB_troutperch()
par(op)
?FSAdata
```
<br>
`r fig_cap("trout perch data FSA package", caption="Trout-perch total length per age class from the FSA package (Lake Michigan, House and Wells, 1973). Average length of trout-perch collected as part of the WFB 161 lab (fall 2018) is represented.")`

`r fig_cap("trout perch data FSA package", display="cite")` suggests that the trawls conducted during the lab mainly caught 2 years-old fish.


## Get an average biomass

Get average CPUE: `r round(mean(tps$fish_count_CPUE, na.rm=T))`/trout-perch/minute trawled. A trout-perch weight on average 
What is the average volume sampled per minute of trawl? __Ask Steve__.

According to fish base, a and b parameters are respectively: 0.00398 and 3.15


# Smelt
For fish in general, we need to get the a and b parameters of the $ W = a L^{b} $ equation.

__Data:__ 
<br>

```{r read smelt data, message=FALSE, warning=FALSE, include=FALSE}
smelt <- read.csv(file = paste0(getpath4data(),"data_from_Ellen/MasterFileSmeltBio1984-2015-2-24-16.csv"))
smelt <- smelt[smelt$Species == "Rainbow Smelt",]
smelt$Station <- gsub("-"," ",smelt$Station)
head(smelt)
```

The data were provided by Ellen Marsden. It contains `r nrow(smelt)` observations of smelt. Each row has some metadata associated (date and basin sampled) as well as individuals characteristics (length, weight, year class, condition).

```{r map smelt data origin, message=FALSE, warning=FALSE, include=FALSE}

smelt_coord <- read.csv(file = paste0(getpath4data(), "smelt_coord.csv"))

xIcon <- makeIcon(
  iconUrl = "https://cdn4.iconfinder.com/data/icons/defaulticon/icons/png/256x256/cancel.png",
  iconWidth = 20, iconHeight = 20)
```

## Metadata / general considerations about the sampling

### Sampling stations
Here, you can see rough coordinates of the stations sampled for rainbow smelt throughout the years. For ambiguous locations such as Main Lake, Main Lake North, Main Lake South, etc., we just coordinates roughly in the center of the area. You can click on the icons to see the stations specific coordinates. 

```{r map smelt sampling sites, echo=FALSE, message=FALSE, warning=FALSE}
# Creation of the map with each station. Includes name of station, latitude, and longitude
coord_map <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers(icon = xIcon, lng = smelt_coord$Longitude, lat = smelt_coord$Latitude,
             popup = paste("<b>Station:</b>", smelt_coord$Station, "<br>",
                           "<b>Latitude:</b>", smelt_coord$Latitude, "<br>",
                           "<b>Longitude:</b>", smelt_coord$Longitude))

coord_map
```

### Number of fish caught per year

Note that there are no data for 1986, 1988, and 1989.

```{r number smelt caught per yr, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=c(cap1,cap2)}
# Data frame which includes the number of fish sampled per year
summ_smelt_catches1 <- as.data.frame(summary(as.factor(smelt$Year)))
summ_smelt_catches1 <- cbind(rownames(summ_smelt_catches1), summ_smelt_catches1)
rownames(summ_smelt_catches1) <- c()
colnames(summ_smelt_catches1) <- c("Year", "num_fish")

# Bar plot which displays the number of fish sampled per year
ggplot(data = summ_smelt_catches1, mapping = aes(x = Year, y = num_fish)) + 
  geom_bar(stat = "summary") + 
  labs(x = "Year", y = "Number of Fish Sampled") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
cap1 <- fig_cap("number smelt sampled per year", caption="Number of rainbow smelt sampled per year.")

# Table which displays the number of fish sampled each year at each station 
station_number_sampled <- as.data.frame(with(smelt, tapply(rep(1,nrow(smelt))
  ,list("Year#"=Year,"station"=Station), sum, na.rm=T)))
station_number_sampled[is.na(station_number_sampled)] <- 0
cap2 <- tab_cap("number smelt sampled per year per station", caption="Number of rainbow smelt sampled per year and per station.")
kable(station_number_sampled)
```

### Evolution of sampled stations

The table of 0's and 1's indicates whether or not a station was sampled for a particular year.

```{r stations sampled per yr, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=c(cap1)}
# Data frame which includes the years, list of the stations sampled during that year, and number of stations sampled that year
summ_smelt_catches2 <- smelt %>% 
  group_by(Year) %>% 
  summarise(list_stations = list(unique(Station)), num_stations = length(unlist(list_stations)))

# Bar plot showing the number of stations sampled per year
ggplot(data = summ_smelt_catches2, mapping = aes(x = as.factor(Year), y = num_stations)) +
  geom_bar(stat = "summary") +
  labs(x = "Year", y = "Number of Stations Sampled") + 
  scale_y_continuous(breaks = c(seq(1, 10, 1))) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
cap1 <- fig_cap("number station sampled smelt per year", caption = "Number of stations sampled for rainbow smelt per year.")

# Table which displays which stations were sampled when
station_which_sampled <- as.data.frame(with(smelt, tapply(rep(1,nrow(smelt))
  ,list("Year#"=Year,"station"=Station), sum, na.rm=T)))
station_which_sampled[is.na(station_which_sampled)] <- 0
station_which_sampled[station_which_sampled > 0] <- 1

# Put in red the sites that were sampled to make it easier to see which sites were sampled when
library(huxtable)
ht <- as_hux(station_which_sampled)
ht <- set_background_color(ht, where(ht == 1), "pink")
ht <- huxtable::add_colnames(ht, colnames = colnames(station_which_sampled))
ht <- huxtable::add_rownames(ht, rownames = rownames(station_which_sampled))
col_width(ht) <- 0.5
wrap(ht) <- TRUE
ht



# Table which displays the total number of fish sampled at each station across all years
station_total_sampled <- station_number_sampled %>% summarise_all(list(sum))
kable(station_total_sampled)
```

## Length/weigth/condition evolution

### Weigth/length relationship
```{r smelt weight length, echo=FALSE, message=FALSE, warning=FALSE}
# Test of a fit without assumptions
p1 <- ggplot(aes(x=Length, y=Weight), data=smelt )+
  geom_point() + stat_smooth()

# per station
p2 <- ggplot(aes(y=Weight, x=Length), data=smelt )+
  geom_point()+ geom_smooth() +
  facet_wrap(~Station)

grid.arrange(p1,p2,nrow=1)
```
<br>
`r fig_cap("WL relationship smelt", caption="Smelt Weigth/length relationship by monitoring station.")`

### Condition of rainbow smelt over time by lake station 

It appears that the condition of rainbow smelt has stayed realtively constant over time in Lake Champlain. The trendlines show an average condition of roughly 0.5 to 0.7 over time.

```{r condition by station, warning = FALSE}
# Grid of plots which display the condition of rainbow smelt over time at each station
ggplot(aes(y=condition, x=Year), data=smelt )+
  ylim(0,6)+
  geom_point()+
  #geom_line() +
  geom_smooth(method="gam", formula = y ~ s(x)) +
  facet_wrap(~Station)
```
<br>
`r fig_cap("smelt condition over year", caption="Evolution of smelt condition in Lake Champlain, by monitoring station.")`


### Rainbow smelt per year class in Lake Champlain

```{r catch per year, warning = FALSE, message = FALSE}
# Data frame which includes the number of fish obtained per year class
summ_smelt_ageclass <- as.data.frame(summary(as.factor(smelt$Age)))
summ_smelt_ageclass <- cbind(rownames(summ_smelt_ageclass), summ_smelt_ageclass)
rownames(summ_smelt_ageclass) <- c()
colnames(summ_smelt_ageclass) <- c("year_class", "num_fish")
summ_smelt_ageclass

# Bar plot which displays the number of fish obtained per year class
ggplot(data = summ_smelt_ageclass[3:12,], aes(x = year_class, y = num_fish)) + 
  geom_bar(stat = "summary") + 
  labs(x = "Year Class", y = "Number of Fish")
```
<br>
`r fig_cap("number of smelt per year class", caption="Number of smelt per year class in our sample.")`

## Exploring growth parameters for rainbow smelt

### Weight-length relationship

Using Rosalie's code from LT analysis and Derek Ogle's tutorial.

```{r smelt weigth length relationship basic plot, message = FALSE, warning = FALSE}
# Two plots here--the first is the weight-length relationship of rainbow smelt without a log-transformation. The second plot is a log-transformed plot of this relationship with a line of best fit
par(mfrow=c(1,2))
plot(smelt$Weight ~ smelt$Length ,xlab="Total Length (mm)", ylab = "Weight (g)", main = "", pch=20)

smelt$logW <- log(smelt$Weight)
smelt$logL <- log(smelt$Length)

lm1 <- lm(logW~logL,data = smelt)
fitPlot(lm1,xlab="Log Total Length (mm)", ylab = "Log Weight (g)", main="")

```

The relationship is not linear without log-transforming the data. Here, we have the coefficients of the log-transformed data. 

```{r summary model smelt W ~ L, message = FALSE, warning = FALSE}
summary(lm1)
# test the concurve package
# randomframe <- curve_gen(lm1, "logL")
# tibble::tibble(randomframe)
# ggconcurve(type = "consonance", randomframe)

# Try a model with a random distribution to see how the range of values vary
# smelt2 <- smelt
# smelt2$logL <- smelt$logL+rep(1:10,length.out = nrow(smelt))
# lm2 <- lm(logW~logL,data = smelt2)
# fitPlot(lm2,xlab="Log Total Length (mm)", ylab = "Log Weight (g)", main="")
# randomframe2 <- curve_gen(lm2, "logL")
# summary(lm2)
# randomframe2 <- curve_gen(lm1, "logL")
# ggconcurve(type = "consonance", randomframe2)


```

### Test whether rainbow smelt exhibit isometric or allometric growth

A test of whether the fish in a population exhibit isometric growth or not can be obtained by noting that b is the estimated slope from fitting the transformed length-weight model. The slope is generically labeled with β such that the test for allometry can be translated into the following statistical hypotheses:
* H0: β=3 ⇒ H0 :"Isometricgrowth"
* HA: β≠3 ⇒ HA :"Allometricgrowth"
(All taken from Derek Ogle tutorial. Go back there for more details).

A test, and confidence interval for b, of whether rainbow smelt from Lake Champlain exhibited allometric growth or not is constructed with:

```{r smelt allo or isometric growth, message = FALSE, warning = FALSE}
hoCoef(lm1,2,3)
confint(lm1)
```

These results show that LT exhibit allometric growth (p = 1.3e-93) with an exponent parameter (b) between 2.87 and 2.89, with 95% confidence.

### Prediction on original scale
Again, from Derek Ogle tutorial: "Predictions of the mean value of the response variable given a value of the explanatory variable can be made with predict(). In the length-weight regression, the value predicted is the mean log of weight. Most often, of course, the researcher is interested in predicting the mean weight on the original scale. An intuitive and common notion is that the log result can simply be back-transformed to the original scale by exponentiation. However, back-transforming the mean value on the log scale in this manner underestimates the mean value on the original scale. This observation stems from the fact that the back-transformed mean value from the log scale is equal to the geometric mean of the values on the original scale. The geometric mean is always less than the arithmetic mean and, thus, the back-transformed mean always underestimates the arithmetic mean from the original scale."

We want to extract the sigma, and then get the correction factor:

```{r get growth parameters smelt original scale, message = FALSE, warning = FALSE}
syx <- summary(lm1)$sigma
(cf <- exp((syx^2)/2))
```

(1) Predict log weight of a rainbow smelt of size 130 mm
(2) Biased prediction on original scale
(3) Corrected prediction on original scale

```{r check error and bias, message = FALSE, warning = FALSE}
(pred.log <- predict(lm1,data.frame(logL=log(130)),interval="c")) ##(1)
(bias.pred.orig <- exp(pred.log)) ##(2)
(pred.orig <- cf*bias.pred.orig) ##(3)
```

### Comparison of weight-length relationship {.tabset}

#### Across years

Include year as a factor.  

```{r does weigth length relationship for smelt change across yrs, message = FALSE, warning = FALSE}
smelt$fyear <- factor(smelt$Year)
lm2 <- lm(logW~logL*fyear, data = smelt)
```

The analysis of variable table is constructed by submitting the saved lm object to anova() as such: 

```{r anova lm2, message = FALSE, warning = FALSE}
anova(lm2)
```

These results indicate that the interaction terms are significant (p = 2.2e-16). There is evidence to conclude that there is a difference in slopes in the length-weight relationship between years. The p-value for the indicator variable suggests that there is a difference in intercepts between the three years (p = 2.2e-16).

Plots and confidence intervals should be constructed for the model with the interaction term, as it was significant. The confidence intervals, constructed with:

```{r confint of model, message = FALSE, warning = FALSE}
confint(lm2)
par(mfrow=c(1,1))
fitPlot(lm2,xlab="Log Length (mm)",ylab="Log Weight (g)", legend = "topleft",main = "", col = adjustcolor(c("red","blue","grey"), alpha.f = .5))
```


#### Across stations

Include stations as a factor.  

```{r does weigth length relationship for smelt change across stations, message = FALSE, warning = FALSE}
smelt$fstations <- factor(smelt$Station)
lm3 <- lm(logW~logL*fstations, data = smelt)
``` 

The analysis of variable table is constructed by submitting the saved lm object to anova() as such:

```{r anova lm3, message = FALSE, warning = FALSE}
anova(lm3)
```

These results indicate that the interaction terms is significant (p = 2.2e-16). There is evidence to conclude that there is difference in slopes in the length-weight relationship between years. The p-value for the indicator variable suggests that there is a difference in intercepts between the three sites (p = 2.2e-16). 

Plots and confidence intervals should be constructed for the model with the interaction term, as it was significant. The confidence intervals, constructed with:
```{r confint model 3, message = FALSE, warning = FALSE}
confint(lm3)
par(mfrow=c(1,1))
fitPlot(lm3, xlab="Log Length (mm)",ylab="Log Weight (g)", legend = "topleft",main = "", col = adjustcolor(c("red","blue","grey"), alpha.f = .5))
```

## Conclusion: a and b parameters
In <a href=http://www.dnr.state.mi.us/publications/pdfs/ifr/manual/smii%20chapter17.pdf> Schneider et al 2010 report for Michigan Department of Natural Resources</a>, growth parameter are reported for rainbow smelt:    
  *  a = -5.12117 <br/>
  *  b =  2.96408 <br/>
We found (see results for lm1):  <br/>
  *  a = -5.05001<br/>
  *  b =  2.88230<br/>
There are some discrepancies between the two, but they're fairly close.  <br/>

To convert them to the initial equation W = aL<sup>b</sup>:  
```{r conclusion a and b parameters for rainbow smelt, message = FALSE, warning = FALSE}
(a = exp(lm1$coefficients[1]))
(b = lm1$coefficients[2])
```

Therefore, the intial equation is W = 0.000009405 x L<sup>2.88230</sup>

***

## Weight-length relationships for specific groups of rainbow smelt {.tabset}

### Age-0 (for Lake Champlain fish.xlsx)

The calculations for a and b for age-0 and age-1 rainbow smelt are for the purpose of transcribing rainbow smelt density to biomass values in Lake Champlain fish.xlsx in our Dropbox!

n = 89

Because most rainbow smelt of age-0 or with a year class of "YOY" don't have a weight, we will calculate an average weight for them based on the average length of rainbow smelt of age-0 (n = 89). We will use the weight-length equation calculated earlier for all rainbow smelt in Lake Champlain. Again, the equation from earlier was: <br/><br/> 
W = 0.000009405 x L<sup>2.88230</sup> 
<br/><br/>
Therefore, if we use the average length of 41 mm for the 89 fish with a classification of "YOY", our equation is: <br/><br/> 
W = 0.000009405 x 41<sup>2.88230</sup> 

```{r W and L for smelt YOY}
mean(smelt$Length[smelt$YearClass == "YOY"], na.rm = TRUE)
length(smelt$Length[smelt$YearClass == "YOY"])
(weight_age0 <- 0.000009405 * (41^2.8830))
```

The average weight of age-0 smelt in Lake Champlain is around 0.42 grams. 

### Age-1 (for Lake Champlain fish.xlsx)

n = 9,754

Using data from 9,754 rainbow smelt of age-1, we can find an average weight.

```{r W and L for smelt Age-1}
mean(smelt$Weight[smelt$Age == 1], na.rm = TRUE)
mean(smelt$Length[smelt$Age == 1], na.rm = TRUE)
length(smelt$Weight[smelt$Age == 1])
```

The average weight of age-0 rainbow smelt in Lake Champlain is around 8.59 grams. Their average length is 114 mm.

### <4 years old

n = 10,987

```{r W and L for smelt Age2-4}
lm_4years <- lm(logW~logL,data = smelt[as.numeric(smelt$Age) <= 4,])
fitPlot(lm_4years, xlab = "Log Total Length (mm)", ylab = "Log Weight (g)", main = "")
summary(lm_4years)
```

a = -5.19480 <br/>
b =  2.96750 <br/>

W = 0.0000064 x L<sup>2.96750</sup>

### <5 years old

n = 19,412

```{r W and L for smelt Age4-5}
lm_5years <- lm(logW~logL,data = smelt[as.numeric(smelt$Age) <= 5,])
fitPlot(lm_5years, xlab = "Log Total Length (mm)", ylab = "Log Weight (g)", main = "")
summary(lm_5years)
```

a = -5.12426 <br/>
b =  2.93121 <br/>

W = 0.0000075 x L<sup>2.93121</sup>

### <6 years old

n = 23,296

```{r W and L for smelt Age6-7}
lm_6years <- lm(logW~logL,data = smelt[as.numeric(smelt$Age) <= 6,])
fitPlot(lm_6years, xlab = "Log Total Length (mm)", ylab = "Log Weight (g)", main = "")
summary(lm_6years)
```

a = -5.14460 <br/>
b =  2.90205 <br/>

W = 0.0000086 x L<sup>2.90205</sup>

### At or below 200 mm

n = 24,816

```{r W and L for smelt below 200mm}
lm_below200mm <- lm(logW~logL,data = smelt[as.numeric(smelt$Length) <= 200,])
fitPlot(lm_below200mm, xlab = "Log Total Length (mm)", ylab = "Log Weight (g)", main = "")
summary(lm_below200mm)
```

a = -5.02208 <br/>
b =  2.88082 <br/>

W = 0.0000095 x L<sup>2.880817</sup>

### Above 200 mm

n = 180

```{r W and L for smelt above 200 mm}
lm_above200mm <- lm(logW~logL,data = smelt[as.numeric(smelt$Length) > 200,])
fitPlot(lm_above200mm, xlab = "Log Total Length (mm)", ylab = "Log Weight (g)", main = "")
summary(lm_above200mm)
```

a = -5.3648 <br/>
b =  3.0303 <br/>

W = 0.0000043 x L<sup>3.0303</sup>

## Summary of a and b

```{r summary a and b for smelt}
# Table which includes the age/size group and their respective a and b parameters
age_group <- c("All", "<4 years", "<5 years", "<6 years", "<=200 mm", ">200 mm")
a <- c(-5.05001, -5.19480, -5.12426, -5.14460, -5.02208, -5.3648)
b <- c(2.88230, 2.96750, 2.93121, 2.90205, 2.88082, 3.0303)

a_and_b_summary <- data.frame(age_group, a, b)
kable(a_and_b_summary)
```

## Get smelt biomass evolution

First, select fish sampled in the main lake. Juniper Island is a location with consistent sampling. The site is 70-90 m deep. In deeper sites, trawl was lowered to approximately 35 m depth. The net was towed at the maximum depth for 10 minutes, allowing it to stabilize. The net was then raised about 3 m and towed for an additional 5 minutes. This step is repeated until the net was 10 m below the surface, and then it is hauled back to the boat. Thus, in deep-water sites such as Juniper Island, each trawl consisted of nine steps and lasted for 55 minutes. Four trawls per night were conducted at each sites. <br>
CPUE is expressed in terms of catch per 55-minutes of trawling (catch X 55 min/actual trawling time). A sample of 50 fish was randomly selected for each haul and frozen for later otolith extraction. In the laboratory, smelt were thawed, measured, weighed, and otoliths were extracted. 

```{r smelt subset main lake}
smelt_sub <- smelt[smelt$Station=="Juniper Island",]
smelt_sub_summ<-as.data.frame(with(smelt_sub, tapply(rep(1,nrow(smelt_sub)),list("Year#"=Year), sum)))
colnames(smelt_sub_summ) = "CPUE"
smelt_sub_summ$Year <- as.numeric(paste(rownames(smelt_sub_summ)))

ggplot(smelt_sub_summ, aes(Year, CPUE)) + geom_point() + geom_line()
```



# Alewife


```{r read alewife data, message=FALSE, warning=FALSE, include=FALSE}
alewife <- read.csv(file = paste0(getpath4data(),"data_from_Ellen/MasterFileSmeltBio1984-2015-2-24-16.csv"))
alewife <- alewife[alewife$Species == "Alewife",]
alewife$Station <- gsub("-"," ",alewife$Station)
head(alewife)
```

The data were provided by Ellen Marsden. It contains `r nrow(alewife)` observations of smelt for years `r paste(unique(alewife$Year), sep="", collapse="-")`. In fact, most data are for 2012. Each row has some metadata associated (date and basin sampled) as well as individuals characteristics (length, weight, year class, condition).


## Get alewife biomass evolution

Same logic, but if we select alewife && Juniper Island here, we only have data for 2012

```{r alewife subset main lake}
alwf_sub <- alewife[alewife$Station=="Juniper Island",]
alwf_sub_summ<-as.data.frame(with(alwf_sub, tapply(rep(1,nrow(alwf_sub)),list("Year#"=Year), sum)))
colnames(alwf_sub_summ) = "CPUE"
alwf_sub_summ$Year <- as.numeric(paste(rownames(alwf_sub_summ)))

ggplot(alwf_sub_summ, aes(Year, CPUE)) + geom_point() + geom_line()
```



# Whitefish


# Cisco
CiscoTL dataset as part of the FSAdata

CPUE (catch per 55 minutes trawl) of cisco collected, 1999-2014. Data from the VTFWS 2015 annual report (see doc given by Ellen).

Note that in Valcour Island (2013) and in Mallets Bay (2000,2002,2008), CPUE reported are in fact <1.

```{r extract data from VTFWS 2015 report table 12}
cisco <- data.frame("Year"=1999:2014)
cisco$BarberPoint   <- c(15,7,12,6,15,16,11,4,2,4,2,1,1,3,0,0)
cisco$JuniperIsland <- c(2,2,3,24,11,9,8,3,0,3,2,2,4,2,0,0)
cisco$ValcourIsland <- c(4,4,19,35,15,NA,8,4,7,2,3,5,9,3,1,0)
cisco$MalletsBay    <- c(1,1,1,1,0,0,1,0,0,1,0,0,0,0,0,0)
```

```{r}
cisco2 <- cbind(melt(cisco[,-1]), "Year"=rep(cisco$Year,4))
colnames(cisco2) <- c("Station","CPUE","Year")
ggplot(cisco2, aes(Year,CPUE,col=Station)) + geom_point() + geom_line()
```
<br> `r fig_cap("cisco CPUE", "CPUE (catch per 55 minutes trawl) of cisco collected, 1999-2014")`


# Lake Trout

__Data:__ <br>
Data for 2016-2018 field seasons provided by Pascal Wilkins.
Displaying some summary data, including number of wild vs. stocked fish per year and basins, and number of fish collected per year (lot more in 2018 than the previous years).
```{r read data LT, include=FALSE}
LTcond <- read.delim(paste0(getpath4data(),"data_from_Pascal/LT_ConditionOverall.txt"))
LTcond <- LTcond[-c(2065),] # outlier detected by fultons

#head(LTcond)
with(LTcond, tapply(rep(1,nrow(LTcond)),list("Year#"=Year, "Clipped#"=Clipped), sum))
with(LTcond, tapply(rep(1,nrow(LTcond)),list("Site"=Local, "Clipped#"=Clipped), sum))
with(LTcond, tapply(rep(1,nrow(LTcond)),list("Year#"=Year, "Site"=Local), sum))

```

## Simple approach: estimate biomass from number of juvenile stocked

```{r simple approach for LT biomass estimate}
n_lt_stocked = 85000 #number lake trout stocked
w_lt_juv     = 1     #weight juvenile lake trout in g
```


## Read and explore effort data

First, import the data. They are all kept in different files, so that need to be processed a little bit.  
```{r read data effort LT, include=FALSE}
# Read data
LTeff16 <- read.delim(paste0(getpath4data(),"data_from_Pascal/LT_2016_effort2.txt"))
LTeff17 <- read.delim(paste0(getpath4data(),"data_from_Pascal/LT_2017_effort2.txt"))
LTeff18 <- read.delim(paste0(getpath4data(),"data_from_Pascal/LT_2018_effort2.txt"))

# Check which columns match and which don't
colnames(LTeff16)[!colnames(LTeff16) %in% colnames(LTeff17)]
colnames(LTeff16)[!colnames(LTeff16) %in% colnames(LTeff18)]
colnames(LTeff18)[!colnames(LTeff18) %in% colnames(LTeff16)]
colnames(LTeff18)[!colnames(LTeff18) %in% colnames(LTeff17)]

# Vector to reordinate columns at the end following LTeff18 template
order_name <- names(LTeff18)

# Order so all columns are in the same order
LTeff16 <- LTeff16[,c(order(colnames(LTeff16)))]
LTeff17 <- LTeff17[,c(colnames(LTeff16)[order(colnames(LTeff16))],colnames(LTeff17)[!colnames(LTeff17) %in% colnames(LTeff16)])]
LTeff18 <- LTeff18[,c(colnames(LTeff17)[order(colnames(LTeff17))],colnames(LTeff18)[!colnames(LTeff18) %in% colnames(LTeff17)])]

# merge dataframe
LTeff <- merge(t(LTeff17),t(LTeff16), by = "row.names", all = T)
  rownames(LTeff) <- LTeff[,grep("Row.names", colnames(LTeff))]
  LTeff <- LTeff[,-grep("Row.names", colnames(LTeff))]
LTeff <- merge(t(LTeff18),LTeff, by = "row.names", all = T)
  rownames(LTeff) <- LTeff[,grep("Row.names", colnames(LTeff))]
  LTeff <- LTeff[,-grep("Row.names", colnames(LTeff))]
LTeff <- as.data.frame(t(LTeff))
# Check that we have the correct number of rows - two extra, because rownames were added as columns (and then transformed in rows, so two extra rows)
dim(LTeff)
sum(nrow(LTeff16)+nrow(LTeff17)+nrow(LTeff18))

LTeff <- LTeff[,c(order(order_name))]

head(LTeff)

LTeff$AvDepth_m <- (as.numeric(paste(LTeff$start.depth.m)) + as.numeric(paste(LTeff$end.depth.m)))/2
# Just checking Pascal also calculated the average depth by averaging start and end depth:
plot(LTeff$AvDepth_m, as.numeric(paste(LTeff$Average.Depth..m.)))
abline(a=0, b=1)

names(LTeff)
```

Number of fish collected per year and per sites: Main Lake is mainly targeted.

```{r number of LT collected per year per basin, echo=FALSE}
sites = unique(LTcond$Local)
plot(c(unique(LTcond$Year)), rep(1,3),
     cex=sqrt(summary(as.factor(LTcond$Year[LTcond$Local==sites[1]]))/15),
     xlim=c(2015,2019),ylim=c(0,4), axes=F, xlab="", ylab="", main="Relative number of data per year and per site", pch=20)
points(c(unique(LTcond$Year)), rep(2,3),
       cex=sqrt(summary(as.factor(LTcond$Year[LTcond$Local==sites[2]])))/15, pch=20)
points(c(unique(LTcond$Year)), rep(3,3),
       cex=sqrt(summary(as.factor(LTcond$Year[LTcond$Local==sites[3]])))/15, pch=20)
axis(1, at=c(1000,3000));axis(1, at = unique(LTcond$Year))
text(x=rep(2015,3), y=seq(1.5,3.5,1), labels = paste(sites, "lake"), adj = 0)

```


When where the trips done? There are two x-scales here (on top of the depth): year and hour of the day.

```{r when were the trips for LT done, echo=FALSE, message=FALSE, warning=FALSE}
LTeff$date <- as.POSIXlt(substr(parse_date_time(x = LTeff$date,orders = c("%d/%m/%Y", "%d/%m/%y", "%Y-%m-%d")),1,10), format = "%Y-%m-%d")
LTeff$yday <- LTeff$date$yday

LTeff$fyear <- as.factor(LTeff$year)
LTeff$year <- as.numeric(paste(LTeff$year))

LTeff$year_hour <- LTeff$year + as.numeric(paste0(substr(LTeff$start.time, 1,2),substr(LTeff$start.time, 4,5)))/5000-.24

LTeff$netID2 <- paste(LTeff$year,as.numeric(paste(LTeff$netID)),sep="_")
LTeff <- LTeff[order(LTeff$date),]


LTeff <- LTeff[!is.na(LTeff$year),]

ggplot(LTeff, aes(x=year_hour, y=-yday, color=LTeff$AvDepth_m)) + 
  geom_point() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  scale_x_discrete(name ="Year", limits=c(2016,2017,2018)) +
  ylab("Julian Day") +
  annotate(geom="text",x=2016:2018, y=c(rep(-80,3)), label=paste("n=",summary(LTeff$fyear))) + 
  labs(color='Average depth (m)') + scale_fill_continuous(guide = guide_legend()) +
    theme(legend.position="bottom") +
  scale_colour_gradient(high = "grey10", low = "#56B1F7",
  space = "Lab", na.value = "blue", guide = "colourbar",
  aesthetics = "colour")

```

## Map trips
```{r map trip LT, message=FALSE, warning=FALSE, include=FALSE, cache=TRUE}
bathy<-readOGR(paste0(getpath4data(),"GIS/LakeChamplain_Shapefile/LakeChamplain.shp"))
lcgroupmap=c("lake",rep("island", 84562))
bathy2 <- raster(paste0(getpath4data(),"GIS/LCbathy.tif"))

world <- ne_countries(scale = "medium", returnclass = "sf")

class(bathy)
crs(bathy)
crs(world)
bathy <- spTransform(bathy,crs(world))
crs(bathy)
bathy2 <- projectRaster(bathy2, crs = crs(world))

res(bathy2)
#aggregate by factor = 3 to increase plotting speed
bathy2 <- aggregate(bathy2, fact=3)


LTeff$start.lat.dd <- as.numeric(paste(LTeff$start.lat.dd))
LTeff$start.lon.dd <- as.numeric(paste(LTeff$start.lon.dd))
LTeff$end.lat.dd <- as.numeric(paste(LTeff$end.lat.dd))
LTeff$end.lon.dd <- as.numeric(paste(LTeff$end.lon.dd))

# Step not necessary at the moment but make sure we're in the right projection.
LTeff_coord <- LTeff[,c("date","start.lon.dd","start.lat.dd","end.lon.dd","end.lat.dd")]
LTeff_coord <- LTeff_coord[complete.cases(LTeff_coord$start.lon.dd),]
LTeff_coord <- LTeff_coord[complete.cases(LTeff_coord$end.lon.dd),]
class(LTeff_coord)
## [1] "data.frame"
LTeff_start <- LTeff_coord[,c("start.lon.dd","start.lat.dd")]
coordinates(LTeff_start)<-~start.lon.dd+start.lat.dd
class(LTeff_start)
## [1] "SpatialPointsDataFrame"
## attr(,"package")
## [1] "sp"

# does it have a projection/coordinate system assigned?
proj4string(LTeff_start) # nope
## [1] NA

proj4string(bathy)

# we know that the coordinate system is NAD83 so we can manually
# tell R what the coordinate system is
proj4string(LTeff_start)<-CRS("+proj=longlat +datum=WGS84")

# now we can use the spTransform function to project. We will project
# the mapdata and for coordinate reference system (CRS) we will
# assign the projection from counties

LTeff_start<-spTransform(LTeff_start, CRS(proj4string(bathy)))

# double check that they match
identical(proj4string(LTeff_start),proj4string(bathy))

# Do the same for end coords
LTeff_end <- LTeff_coord[,c("end.lon.dd","end.lat.dd")]
coordinates(LTeff_end)<-~end.lon.dd+end.lat.dd
proj4string(LTeff_end)<-CRS("+proj=longlat +datum=WGS84")
LTeff_end<-spTransform(LTeff_end, CRS(proj4string(bathy)))

# double check that they match
identical(proj4string(LTeff_end),proj4string(bathy))


# Go back to dataframe
LTeff_start_df <- as.data.frame(LTeff_start)
LTeff_end_df <- as.data.frame(LTeff_end)
# Include to initial dataframe
LTeff_coord$start.lon.dd <- LTeff_start_df$start.lon.dd
LTeff_coord$start.lat.dd <- LTeff_start_df$start.lat.dd
LTeff_coord$end.lon.dd   <- LTeff_end_df$end.lon.dd
LTeff_coord$end.lat.dd   <- LTeff_end_df$end.lat.dd

```


There are some outliers, so deleting all longitude < -73.48 (2 points) and thus > -73.1.

```{r remove outliers from LT trawls coord}
LTeff_coord <- LTeff_coord[LTeff_coord$start.lon.dd<(-73.1) & LTeff_coord$start.lon.dd > (-73.48),]
```

Map visualising all the trips

```{r map visualize all trips, echo=FALSE, message=FALSE, warning=FALSE}
p2 <- ggplot() +  geom_polygon(data=bathy, aes(x=long, y=lat, group=group, fill=lcgroupmap), show.legend = FALSE) +
  scale_fill_manual(values = c("white","darkgrey")) +
  geom_point(data=LTeff_coord, aes(x=start.lon.dd, y=start.lat.dd), color=NA) +
  xlab("Longitude") + ylab("Latitude") + theme_minimal()  +
  theme_set(theme_bw()) +
  coord_equal(ratio=1) # square plot to avoid the distortion

  

p2 <- p2 + geom_segment(aes(x = LTeff_coord$start.lon.dd, y = LTeff_coord$start.lat.dd, xend = LTeff_coord$end.lon.dd, yend = LTeff_coord$end.lat.dd))

p2


#ggsave(paste0(getwd(),"/Output/Figures/1-descriptive/Trawl_trips_",min(LTeff$year, na.rm=T),"-",max(LTeff$year, na.rm=T),".pdf"), p2)

## Scale on map varies by more than 10%, scale bar may be inaccurate

```

## Explore growth parameters

### Weight-length relationship

Based on <a href=http://derekogle.com/fishR/examples/oldFishRVignettes/LengthWeight.pdf> this tutorial by Derek Ogle</a>, using FSA package.
The relationship between length and weigth is not linear, because length is a linear measure and weight is related to volume
```{r W by L plot for LT}
par(mfrow=c(1,2))
plot(LTcond$weight ~ LTcond$frozen.tl,xlab="total length (mm)",ylab="weight (g)",main="", pch=20)

LTcond$logW <- log(LTcond$weight)
LTcond$logL <- log(LTcond$frozen.tl)

lm1 <- lm(logW~logL,data=LTcond)
fitPlot(lm1,xlab="log total length (mm)",ylab="log weight (g)",main="")
abline(h=0, lwd=.5)
text(x=min(LTcond$logL), y=max(LTcond$logW)*.9, pos=4, label=paste0("y = ", round(lm1$coefficients[1],1)," + x * ", round(lm1$coefficients[2],1)))

```

The relationship is indeed not linear, but we can get the coefficients by log-transforming the data. The coefficients are given below:
```{r summary W by L for LT}
summary(lm1)
```


Try the quantile regression to fit the non-tranformed data
```{r quantile regression, message=FALSE, warning=FALSE}
library(quantreg)
LTcond$weight ~ LTcond$frozen.tl
m1 <- rq(weight~poly(frozen.tl,2), data=LTcond,tau=0.9)
m2 <- rq(weight~poly(frozen.tl,3), data=LTcond,tau=0.9)
m3 <- rq(weight~poly(frozen.tl,4), data=LTcond,tau=0.9)
AIC(m1)
AIC(m2)
AIC(m3)
# geom_quantile uses rq()
ggplot(LTcond, aes(frozen.tl,weight))+ geom_point()+
    geom_quantile(formula=y~poly(x,3),quantiles=0.9)

```


### Test whether LT exhibit isometric or allometric growth

A test of whether the fish in a population exhibit isometric growth or not can be obtained by noting that b is the estimated slope from fitting the transformed length-weight model. The slope is generically labeled with β such that the test for allometry can be translated into the following statistical hypotheses:
* H0: β=3 ⇒ H0 :"Isometric growth"
* HA: β≠3 ⇒ HA :"Allometric growth"
(All taken from Derek Ogle tutorial, go back there for more details).

A test, and confidence interval for b, of whether Lake Trout from Lake Champlain exhibited allometric growth or not is constructed with
```{r iso or allometric growth for LT}
hoCoef(lm1,2,3)
confint(lm1)
```
These results show that LT exhibit allometric growth (p < 0.0000001) with an exponent parameter (b) between 3.22 and 3.24, with 95% confidence.

### Prediction on original scale
Again, from Derek Ogle tutorial: "Predictions of the mean value of the response variable given a value of the explanatory variable can be made with predict(). In the length-weight regression, the value predicted is the mean log of weight. Most often, of course, the researcher is interested in predicting the mean weight on the original scale. An intuitive and common notion is that the log result can simply be back-transformed to the original scale by exponentiation. However, back-transforming the mean value on the log scale in this manner underestimates the mean value on the original scale. This observation stems from the fact that the back-transformed mean value from the log scale is equal to the geometric mean of the values on the original scale. The geometric mean is always less than the arithmetic mean and, thus, the back-transformed mean always underestimates the arithmetic mean from the original scale."

We want to extract the sigma, and then get the correction factor:
```{r sigma for LT growth}
syx <- summary(lm1)$sigma
( cf <- exp((syx^2)/2) )
```

(1) Predict log weight of a LT of size 200 mm
(2) Biased prediction on original scale
(3) Corrected prediction on original scale
```{r check bias for LT growth parameters}
( pred.log <- predict(lm1,data.frame(logL=log(200)),interval="c") ) ##(1)
( bias.pred.orig <- exp(pred.log) ) ##(2)
( pred.orig <- cf*bias.pred.orig ) ##(3)
```

### Comparison of Weight-Length relationship {.tabset}

#### Across years

Include year as a factor.  
```{r condition LT across years}
LTcond$fyear <- factor(LTcond$Year)
lm2 <- lm(logW~logL*fyear,data=LTcond)
```

The analysis of variable table is constructed by submitting the saved lm object to anova() as such
```{r anova on condition LT across years}
anova(lm2)
```
These results indicate that the interaction terms is significant (p = 1.850e-10). There is evidence to conclude that there is difference in slopes in the length-weight relationship between years. The p-value for the indicator variable suggests that there is a difference in intercepts between the three years (p = 1.086e-07). 

Plots and confidence intervals should be constructed for the model with the interaction term, as it was significant. The confidence intervals, constructed with:
```{r confint for condition LT across years}
confint(lm2)
par(mfrow=c(1,1))
fitPlot(lm2,xlab="log frozen length (mm)",ylab="log weight (g)",legend="topleft",main="", col=adjustcolor(c("red","blue","grey"), alpha.f = .5))
```

The difference is not so obvious once plotted


#### Across sites

Include sites as a factor.  
```{r condition LT across sites}
LTcond$fsites <- factor(LTcond$Local)
lm3 <- lm(logW~logL*fsites,data=LTcond)
```

The analysis of variable table is constructed by submitting the saved lm object to anova() as such
```{r anova condition LT across sites}
anova(lm3)
```

These results indicate that the interaction terms is significant (p = 
0.008697). There is evidence to conclude that there is difference in slopes in the length-weight relationship between years. The p-value for the indicator variable suggests that there is a difference in intercepts between the three sites (p = 1.193e-13). 

Plots and confidence intervals should be constructed for the model with the interaction term, as it was significant. The confidence intervals, constructed with:
```{r confint condition LT across sites}
confint(lm3)
par(mfrow=c(1,1))
fitPlot(lm3,xlab="log frozen length (mm)",ylab="log weight (g)",legend="topleft",main="", col=adjustcolor(c("red","blue","grey"), alpha.f = .5))
```

### Conclusion: a and b parameters
In <a href=http://www.dnr.state.mi.us/publications/pdfs/ifr/manual/smii%20chapter17.pdf> Schneider et al 2010 report for Michigan Department of Natural Resources</a>, growth parameter are reported for Lake Trout:  
  *  a= -5.519  
  *  b= 3.17882  
_Warning:_ These parameters are not on original scale.  
We found (see results for lm1):  <br>
  * a= -5.578692 
  * b= 3.225599  
So not too far off.  

To convert them to the initial equation W = aL<sup>b</sup>:  
```{r conclusion a and b parameters for LT}
(a= exp(lm1$coefficients[1]))
(b= lm1$coefficients[2])
```

Year and sites were significant when we tested for interaction: I should probably use parameter for one year and one site? Or on the contrary, we're aware there's some differences but it's more relevant to average them out by using several years.

## Growth equations

### Coding functions

#### Fulton's condition factor
Because of assumed isometric growth, comparisons should be limited to fish of similar length (small size range) so slope is zero 
(in other words, condition does not increase with age)

It means we should calculate a and b for smaller bin sizes, get the K for the ideal fish (K=1), then compare fish from the same bin sizes to that K

Below, I wrote a function to compute Fulton's factor ($K = W/L^3$) for different bin sizes. The function is called *Compute_FultonK*.

#### Relative condition factor

I also wrote a function to compute relative condition factor.
$K_n = W/W'$
W' is the predicted weight for a population (using a and b factors). It compensates for allometric growth. The average for a given species is 1.0.


The function is called *RelCond_Fisheries*.

#### Relative weigth

Below, I wrote a function to compute relative weight.
$Wr = W/Ws * 100$
Ws is a species-specific standard based on W = aLb


The function is called* RelWeight_Fisheries*. The argument *size_limits* allows to calculate the factor only for a range of the population (user must enter a vector of 2 values).

### Fulton's condition factor K

```{r use function Compute_FultonK, echo=FALSE, message=FALSE, warning=FALSE}

Fult1 <- Compute_FultonK(pop_ID = LTcond$Trawl.Id,
                pop_weight = LTcond$weight,
                pop_length = LTcond$tl.mm/10,
                bin_limits = seq(0,800, by = 100))
Fult2 <- Compute_FultonK(pop_ID = LTcond$Trawl.Id,
                pop_weight = LTcond$weight,
                pop_length = LTcond$tl.mm/10,
                bin_limits = seq(0,800, by = 800))
Fult3 <- Compute_FultonK(pop_ID = LTcond$Trawl.Id,
                pop_weight = LTcond$weight,
                pop_length = LTcond$tl.mm/10,
                bin_limits = seq(0,800, by = 400))
grid.arrange(Fult1$plot_KL,Fult2$plot_KL,Fult3$plot_KL, nrow=1)
# Fult4 <- Compute_FultonK(pop_ID = LTcond$Trawl.Id[which(LTcond$tl.mm>100 & LTcond$tl.mm <400)],
#                 pop_weight = LTcond$weight[which(LTcond$tl.mm>100 & LTcond$tl.mm <400)],
#                 pop_length = LTcond$tl.mm[which(LTcond$tl.mm>100 & LTcond$tl.mm <400)]/10,
#                 bin_limits = seq(0,800, by = 20))
# Fult4$plot_KL
# Fult4$Growth_parameters
Fult5 <- Compute_FultonK(pop_ID = LTcond$Trawl.Id[which(LTcond$tl.mm>100 & LTcond$tl.mm <400)],
                pop_weight = LTcond$weight[which(LTcond$tl.mm>100 & LTcond$tl.mm <400)],
                pop_length = LTcond$tl.mm[which(LTcond$tl.mm>100 & LTcond$tl.mm <400)]/10,
                bin_limits = seq(0,800, by =800), 
                outliers = 1865)
#Fult5$plot_KL
Fult6 <- Compute_FultonK(pop_ID = LTcond$Trawl.Id[which(LTcond$tl.mm>100 & LTcond$tl.mm <400)],
                pop_weight = LTcond$weight[which(LTcond$tl.mm>100 & LTcond$tl.mm <400)],
                pop_length = LTcond$tl.mm[which(LTcond$tl.mm>100 & LTcond$tl.mm <400)]/10,
#                bin_limits = c(100,120,150, 190,280,400), 
                bin_limits = c(100,120,150, 220,280,400), 
                outliers = 1865)
#Fult6$plot_KL
#Fult6$plot_WL
grid.arrange(
  Fult5$plot_WL,
  Fult5$plot_WL_log,
  Fult5$plot_KL,
  Fult6$plot_WL,
  Fult6$plot_WL_log,
  Fult6$plot_KL, ncol=3,widths=c(1,1,1.5))

# grid.arrange(
#   Fult5$plot_WL,
#   Fult5$plot_KL,
#   Fult6$plot_WL,
#   Fult6$plot_KL, ncol=2)


```

### Relative condition factor

Get relative condition factor:
```{r use function RelCondLT}
RelCondLT <- RelCond_Fisheries(pop_ID = LTcond$Trawl.Id,
                pop_weight = LTcond$weight,
                pop_length = LTcond$tl.mm/10)
grid.arrange(RelCondLT$plot_WL_log, RelCondLT$plot_KnL, ncol=2, widths=c(1,1.5))

# RelCondLT$Data[which(RelCondLT$Data$Kn>1.6),]
# RelCondLT$a
# RelCondLT$b

```
<br>`r fig_cap("relative cond LT", "Weight:length relationship (log10 scale) and relative condition factor for lake trout collected in Lake Champlain, over the 2016-2018 field seasons")`<br>

### Relative weigth

Get relative weight:
```{r use function RelWeightLT}
RelWeightLT <- RelWeight_Fisheries(pop_ID = LTcond$Trawl.Id,
                pop_weight = LTcond$weight,
                pop_length = LTcond$tl.mm/10,
                a=0.00741,
                b=3.04,
                source="https://www.fishbase.se/Summary/SpeciesSummary.php?ID=248&AT=lake+trout")
RelWeightLT$plot_WrL + geom_hline(yintercept = mean(RelWeightLT$Data$Wr), colour="coral", lty=2) 

```
<br>`r fig_cap("relative weight LT", "Relative weigth for lake trout collected in Lake Champlain, over the 2016-2018 field seasons, compared to growth parameters reported on fishbase for Salvelinus namaycush in North America.")`

a and b parameters have been collected on fishbase, <a href="`r RelWeightLT$source_a_b_param`">link</a>.


## Target catch and bycatch

### Read and explore data {.tabset}

First, import the data. They are all kept in different files, so that need to be processed a little bit. <br> 
[code hidden to save room on the .html] 
```{r read data catch, message=FALSE, warning=FALSE, include=FALSE}
# Read data
# target catch
tg16 <- read.delim(paste0(getpath4data(), "data_from_Pascal/LT_2016_targetcatch.txt"));tg16$notes <- rep(NA, nrow(tg16))
tg17 <- read.delim(paste0(getpath4data(), "data_from_Pascal/LT_2017_targetcatch.txt"))
tg18 <- read.delim(paste0(getpath4data(), "data_from_Pascal/LT_2018_targetcatch.txt"))
# bycatch
byc16 <- read.delim(paste0(getpath4data(),"data_from_Pascal/LT_2016_bycatch.txt"))
byc17 <- read.delim(paste0(getpath4data(),"data_from_Pascal/LT_2017_bycatch.txt"))
byc18 <- read.delim(paste0(getpath4data(),"data_from_Pascal/LT_2018_bycatch.txt"))

# merge dataframe
tg <- merge(t(tg17),t(tg16), by = "row.names", all = T)
  rownames(tg) <- tg[,grep("Row.names", colnames(tg))]
  tg <- tg[,-grep("Row.names", colnames(tg))]
tg <- merge(t(tg18),tg, by = "row.names", all = T)
  rownames(tg) <- tg[,grep("Row.names", colnames(tg))]
  tg <- tg[,-grep("Row.names", colnames(tg))]
tg <- as.data.frame(t(tg))
tg <- mutate_all(tg, tolower)
# Check that we have the correct number of rows - two extra, because rownames were added as columns (and then transformed in rows, so two extra rows)
dim(tg)
sum(nrow(tg16)+nrow(tg17)+nrow(tg18))

tg$fyear <- tg$year
tg$year <- as.numeric(paste(tg$year))
tg$netID2 <- paste(tg$year,as.numeric(paste(tg$netID)),sep="_")

date <- as.POSIXlt(tg$date, format = "%d/%m/%Y")
tg$yday <- date$yday
tg <- tg[order(date),]
head(tg)

# Do the same for bycatch
byc <- merge(t(byc17),t(byc16), by = "row.names", all = T)
rownames(byc) <- byc[,grep("Row.names", colnames(byc))]
byc <- byc[,-grep("Row.names", colnames(byc))]
byc <- merge(t(byc18),byc, by = "row.names", all = T)
rownames(byc) <- byc[,grep("Row.names", colnames(byc))]
byc <- byc[,-grep("Row.names", colnames(byc))]
byc <- as.data.frame(t(byc))
byc <- mutate_all(byc, tolower)
# Check that we have the correct number of rows - two extra, because rownames were added as columns (and then transformed in rows, so two extra rows)
dim(byc)
sum(nrow(byc16)+nrow(byc17)+nrow(byc18))

byc$fyear <- byc$year
byc$year <- as.numeric(paste(byc$year))
byc$netID2 <- paste(byc$year,as.numeric(paste(byc$netID)),sep="_")

byc$date <- as.POSIXlt(byc$date, format = "%d/%m/%Y", tz = "")
byc$yday <- byc$date$yday
byc$date <- as.Date(byc$date )
byc <- byc[order(date),]
head(byc)



```


Summary of the target catch and bycatch. Note that for bycatch, a count doesn't represent one individual necesseraly, because sometimes presence was recorded in tote fullness.

```{r summary target and by catch, message=FALSE, warning=FALSE, include=FALSE}
summ_tg <- with(tg, tapply(rep(1,nrow(tg)),list("species#"=species,"Year#"=year), sum))

summ_by <- with(byc, tapply(rep(1,nrow(byc)),list("species#"=species,"Year#"=year), sum))

```

#### Target catch
```{r view target catch, echo=FALSE}
summ_tg
```

#### Bycatch
```{r view bycatch, echo=FALSE}
summ_by
```

### Look more in details at target catch data

The distribution of length is relatively similar one year to the other, with a spreader distribution in 2017. Most fish caught are less than 50 cm long.  

```{r plot target catch length density, echo=FALSE, message=FALSE, warning=FALSE}
tg$tl_mm <- tg$tl_mm  %>% paste() %>% as.numeric()

# mean 
mu <- ddply(tg, "fyear", summarise, grp.mean=mean(tl_mm, na.rm=T))
# Density plot
p <- ggplot(tg, aes(x=tl_mm, fill=fyear)) +
  geom_density()+
  geom_vline(data=mu, aes(xintercept=grp.mean, color=fyear),
             linetype="dashed") +
  geom_density(alpha=0.2) +
  xlab("total length (mm)") + 
  labs(fill='Year')
 ggplotly(p)

```

The variation in length is linked to the capture day of the year, but probably because some species with really different size were caught only then. Year is also a significant explanatory variable. However, there is a lot of noise (very low R<sup>2</sup>).  
```{r explain variation in max total length, echo=FALSE}
lm4a <- lm(tl_mm ~ yday, data=tg)
lm4b <- lm(tl_mm ~ yday*fyear, data=tg)
lm4c <- lm(tl_mm ~ yday*fyear*species, data=tg)
lm4d <- lm(tl_mm ~ yday*species, data=tg)
AIC(lm4a,lm4b,lm4c,lm4d)[order(AIC(lm4a,lm4b,lm4c,lm4d)$AIC),]

```

All AIC are high. Species explain the most, which doesn't come as a surprise: not all species grow as big as the others. 
We're looking at the results of the first model that doesn't include the species (we do not necesseraly want to see how some species are bigger than other here, the idea is to know whether the sampling day in the season or the year have targetted different catch.
There are no strong trend, if larger species were collected on some days, it may have to do with whe zone of the lake that was surveyed instead of actual in-year variation.
The only species with a real trend is sea lamprey. Average size increases by about 200 mm over the season.

```{r summary on model explaining best total length,  echo=FALSE, message=FALSE, warning=FALSE}
summary(lm4b)

ggplot(tg[tg$species=="lake trout"|tg$species=="alewife"|tg$species=="lake whitefish"|tg$species=="rainbow smelt"|tg$species=="sea lamprey"|tg$species=="burbot",], aes(x=yday,y=tl_mm,color=fyear)) + geom_point() + stat_smooth() + facet_wrap(~species)

```

## Extract mortality for LT from only wild data

```{r try looking at mortality in wild LT, include=FALSE}
# create a dataframe with lake trout wild
ltw <- tg[tg$species == "lake trout",]
ltw <- ltw[ltw$fin_clip == "na" | ltw$fin_clip == "nc" ,]
nrow(ltw)

par(mfrow=c(3,1))
hist(ltw$tl_mm[ltw$year==2016], col=adjustcolor("yellow", alpha.f = .4), xlim=c(0,max(ltw$tl_mm, na.rm = T)), xlab="length (mm)", main="2016")
hist(ltw$tl_mm[ltw$year==2017], col=adjustcolor("grey", alpha.f = .4), xlim=c(0,max(ltw$tl_mm, na.rm = T)), xlab="length (mm)", main="2017")
hist(ltw$tl_mm[ltw$year==2018], col=adjustcolor("pink", alpha.f = .4), xlim=c(0,max(ltw$tl_mm, na.rm = T)), xlab="length (mm)", main="2018")
par(mfrow=c(1,1))



```


## Lake Trout diet analysis

Helping Alex with some code, but he'll do the bulk of it (see other .Rmd on diet analysis).  
An example with the size, but he will do the same to look at stomach content. Here, we learn that 40% of our sample of fish smaller than 100mm were sampled in 2016. (but he will repeat it to learn whether size class impact stomach emptyness)

```{r LT diet analysis create size class, echo=FALSE}

# Create the size class in a new column
tg <- tg  %>% mutate(size_class = case_when(
                      tl_mm <= 100 ~ "[0,100]",
        tl_mm > 100 & tl_mm <= 200 ~ "]100,200]", 
        tl_mm > 200 & tl_mm <= 300 ~ "]200,300]",
        tl_mm > 300 & tl_mm <= 400 ~ "]300,400]",
        tl_mm > 400                ~ "]400,∞]"))

# This is a way around since we're not going to use the histogram function (that deals with counts), but histogram instead (we're getting the count another way). Look up the difference between histogram and barplots and let me know if you don't understand it.
# First step, I'm getting here the number of row that correspond to these two factors. You would want to replace 'Year' by 'empty_stomach'. Note that the two columns (size_class and fyear) are factors.
(summ_sc <- with(tg, tapply(rep(1,nrow(tg)),list("Size class"=size_class, "Year"=fyear), sum)))

# Then, to get the percentage of fish with empty stomach per size class, you would do the following:
(summ_sc <- as.data.frame(summ_sc/rowSums(summ_sc, na.rm=T)))
# Here, we learn that 40% of our sample of fish smaller than 100mm were sampled in 2016.

# You can plot the output in a histogram (here I'm only plotting the proportion that was sampled in 2016, you will only plot the proportion were empty_stomach = F).
ggplot(data=summ_sc, aes(x=rownames(summ_sc),y=summ_sc[,1])) +
  geom_bar(stat="identity") + xlab("Size class") + ylab("Percentage") 

```

### Stomach full (Y/N) per size

```{r LT stomach full Y or N, message=FALSE, warning=FALSE, include=FALSE}
LTdiet <- read.delim(paste0(getpath4data(),"data_from_Pascal/LT_diet_2016-2018.txt"))
LTdiet$Total.Length <- as.numeric(paste(LTdiet$Total.Length..mm.))
LTdiet$Start.Depth <- as.numeric(paste(LTdiet$Start.Depth..m.))
LTdiet$Food.in.Stomach..Y.N.
colnames(LTdiet) <- str_replace(colnames(LTdiet), "Food.in.Stomach..Y.N.", "Food.in.Stomach")
summary(LTdiet$Food.in.Stomach)
LTdiet$Food.in.Stomach[LTdiet$Food.in.Stomach=="no"] <- "N"
LTdiet$Food.in.Stomach[LTdiet$Food.in.Stomach=="yes"|LTdiet$Food.in.Stomach=="T"] <- "Y"
summary(LTdiet$Food.in.Stomach)
LTdiet$stock.wild <- ifelse(LTdiet$Clip.Location == "NC", "Wild", "Stocked")

# Create the size class in a new column
LTdiet <- LTdiet  %>% mutate(size_class = case_when(
  Total.Length <= 100 ~ "[0,100]",
  Total.Length > 100 & Total.Length <= 200 ~ "]100,200]", 
  Total.Length > 200 & Total.Length <= 300 ~ "]200,300]",
  Total.Length > 300 & Total.Length <= 400 ~ "]300,400]",
  Total.Length > 400 & Total.Length <= 500 ~ "]400,500]",
  Total.Length > 500 ~ "]500,∞]"))

# First step, I'm getting here the number of row that correspond to these two factors. 
summ_sc <- with(LTdiet, tapply(rep(1,nrow(LTdiet)),list("Size class"=size_class, "Food"=Food.in.Stomach), sum))
(summ_sc <- summ_sc[,colSums(summ_sc, na.rm = T)>0])
summ_sc2 <- melt(summ_sc[,c('Y','N')],id.vars = 1)

# Then, to get the percentage of fish with empty stomach per size class, you would do the following:
(summ_sc_per <- as.data.frame(summ_sc/rowSums(summ_sc, na.rm=T)))

```

Fish above 400 mm have less food in their stomach but thats also the class with the least catch.

```{r percent fish with food in stomach, echo=FALSE, message=FALSE, warning=FALSE}
# You can plot the output in a histogram (here I'm only plotting the proportion that was sampled in 2016, you will only plot the proportion were empty_stomach = F).
p1 <- ggplot(summ_sc2,aes(x = `Size class`,y = value)) + 
    geom_bar(aes(fill = Food),stat = "identity",position = "dodge") + 
     xlab("Size class") + ylab("Number of fish per size class")  +
    theme(legend.position="bottom")
ggplotly(p1)

p2 <- ggplot(data=summ_sc_per, aes(x=rownames(summ_sc_per),y=summ_sc_per[,"Y"])) +
  geom_bar(stat="identity") + xlab("Size class") + ylab("Percentage of individuals with \nat least some food in their stomach") 
ggplotly(p2)
```


### Stomach full (Y/N) per time of sampling

```{r LT stomach fullness per time of sampling create data, message=FALSE, warning=FALSE, include=FALSE}
head(LTeff)
tail(LTdiet)
#load lubridate to handle several date/time format
LTdiet$Capture.Date2 <- parse_date_time(x = LTdiet$Capture.Date,orders = c("d-b-y", "m/d/y", "m/d/Y"))
# 6 without actual sampling date written as "unknown 2016"
```

Here, we need to link diet data to the sampling info (effort dataset).
Some fish (`r length(LTdiet$Capture.Date2[is.na(LTdiet$Capture.Date2)])
`) don't have the sample date available (e.g., line 451-456, it says "unknown 2016"). I'm removing these from the analysis.  

Here, we could match every fish to the effort, using first day of sampling, then starting depth.

```{r LT stomach fullness per time of sampling, message=FALSE, warning=FALSE, include=FALSE}
# Removing the unknown date we can't link back to the effort
LTdiet <- LTdiet[!is.na(LTdiet$Capture.Date2),]

# Match
LTeff$Capture.Date2 <- parse_date_time(x = LTeff$date,orders = c("%Y-%m-%d"))

LTdiet$Hour.sampled <- rep(NA, nrow(LTdiet))
for (i in 1:nrow(LTdiet)) {
  if(i==1) {n1=0;n2=0}
  narrow2day <- LTeff[LTeff$Capture.Date2==LTdiet$Capture.Date2[i],]
  effortdate <- narrow2day[narrow2day$start.depth.m==LTdiet$Start.Depth[i],]
  if(nrow(effortdate)==1) {
    LTdiet$Hour.sampled[i] <- substr(effortdate$start.time,1,2)
    n1 <- n1+1 # for info message
  } else {
      n <- n2+1 # for info message
      }
  # Info message
  if(i==nrow(LTdiet)) message(paste0(" ✓ Found single sampling event for ", n1," of the individuals","\n ✕ No or several sampling events were found for ", n2, " of the individuals" ))
}

LTdiet$Hour.sampled <- as.numeric(paste(LTdiet$Hour.sampled))

# Create class for hour in the day in a new column
min(LTdiet$Hour.sampled, na.rm=T)
max(LTdiet$Hour.sampled, na.rm=T)
LTdiet <- LTdiet  %>% mutate(hour_class = case_when(
  Hour.sampled >= 5 & Hour.sampled <= 6 ~ "5am-6am", 
  Hour.sampled >= 7 & Hour.sampled <= 8 ~ "7am-8am",
  Hour.sampled >= 9 & Hour.sampled <= 10 ~ "9am-10am",
  Hour.sampled >= 10 & Hour.sampled <= 10 ~ "10am-11am", 
  Hour.sampled >= 12 & Hour.sampled <= 13 ~ "12pm-1pm", 
  Hour.sampled >= 14 & Hour.sampled <= 15 ~ "2pm-3pm", 
  Hour.sampled >= 16 & Hour.sampled <= 17 ~ "4pm-5pm"))

# Actually I won't even do it by hour class but just by hour

# First step, I'm getting here the number of row that correspond to these two factors. 
summ_hc <- with(LTdiet, tapply(rep(1,nrow(LTdiet)),list("Hour class"=Hour.sampled, "Food"=Food.in.Stomach), sum, na.rm=T))
(summ_hc <- summ_hc[,colSums(summ_hc, na.rm = T)>0])

# Then, to get the percentage of fish with empty stomach per size class, you would do the following:
(summ_hc <- as.data.frame(summ_hc/rowSums(summ_hc, na.rm=T)))


```

```{r plot percentage of individuals with food in stomach per time of the day, echo=FALSE, message=FALSE, warning=FALSE}
# You can plot the output in a histogram (here I'm only plotting the proportion that was sampled in 2016, you will only plot the proportion were empty_stomach = F).
p1 <- qplot(LTdiet$Hour.sampled,geom="histogram",binwidth = 1,  main = "Number of observations", xlab = "Hour of the day",col=I("white"))
ggplotly(p1)

p1 <- ggplot(data=summ_hc, aes(x=as.numeric(rownames(summ_hc)),y=summ_hc[,"Y"])) +
  geom_bar(stat="identity") + xlab("Time of the day") + ylab("Percentage of individuals with \nat least some food in their stomach") 
ggplotly(p1)


```


### Stomach contents for all Lake Trout

Here is a distribution of the food items preyed upon by lake trout. Some of the most common food items include alewife, mysus, and daphnia, which make up over 58.3% of their diet. The next sections will break down their diet for each length class. The "other" category contributes over 35.6% of the lake trout's diet, so there's a large portion of their diet that was unidentified.

```{r as numeric for stomach content and visualisation, echo=FALSE, message=FALSE, warning=FALSE}
LTdiet$Smelt <- as.numeric(LTdiet$Smelt)
LTdiet$Smelt.YOY <- as.numeric(LTdiet$Smelt.YOY)
LTdiet$Alewife <- as.numeric(LTdiet$Alewife)
LTdiet$Alewife.YOY <- as.numeric(LTdiet$Alewife.YOY)
LTdiet$Sculpin <- as.numeric(LTdiet$Sculpin)
LTdiet$Sculpin.YOY <- as.numeric(LTdiet$Sculpin.YOY)
LTdiet$YOY <- as.numeric(LTdiet$YOY)
LTdiet$Mysis <- as.numeric(LTdiet$Mysis)
LTdiet$Yellow.Perch <- as.numeric(LTdiet$Yellow.Perch)
LTdiet$Unidentifiable.Fish <- as.numeric(LTdiet$Unidentifiable.Fish)
LTdiet$Daphnia <- as.numeric(LTdiet$Daphnia)
LTdiet$Copepod <- as.numeric(LTdiet$Copepod)
LTdiet$zoops <- as.numeric(LTdiet$zoops)
LTdiet$Spiny.Water.Flea <- as.numeric(LTdiet$Spiny.Water.Flea)
LTdiet$Trout.perch <- as.numeric(LTdiet$Trout.perch)
LTdiet$Tess..Darter <- as.numeric(LTdiet$Tess..Darter)
LTdiet$Macroinvert <- as.numeric(LTdiet$Macroinvert)
LTdiet$fishes <- as.numeric(LTdiet$fishes)
LTdiet$Other <- as.numeric(LTdiet$Other)

sumFoods <- colSums(LTdiet[, 21:39], na.rm = FALSE, dims = 1)
sumFoods <- sumFoods[!is.na(sumFoods)]
contents.all <- as.data.frame(sumFoods)
ID <- rownames(contents.all)
propFoods <- sumFoods / sum(sumFoods) * 100

contents.all <- data.frame(sumFoods, propFoods, ID)

diet.total <- ggplot(data = contents.all, mapping = aes(x = "", y = propFoods, fill = contents.all$ID)) +
  geom_bar(stat = "identity") +
  theme(axis.title.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(y = "Proportion of Total Diet", fill = "Food Item")

ggplotly(diet.total)
```

### Stomach Contents for Each Length Class

I'm creating six different bar plots to show the distribution of feeding patterns for each of the size classes we created earlier. From a quick glance at the outputs, it appears that lake trout of larger size tend to consume more alewife. In addition, longer lake trout tend to utilize smelt more than smaller lake trout. LT of smaller sizes utilize mysis and zooplankton much more than lake trout of larger size as well. It appears their diet shifts from small items such as mysis and zooplankton early on in their life to larger prey items such as smelt and alewife. 

And just a note for all of us, there are much more efficient ways of obtaining what I wanted here. However, I'm not that advanced! Rosalie has created code with a loop that is much more concise and accomplishes the same thing, but for now, this will do.

<span style="color:red">Note from Rosalie: Hey, it's impressive you've learned so much in 7 months! 80% of being a researcher seems to be about not giving up based on my experience.</span>


```{r contents.by.size_class, echo=FALSE}
# I'm first creating column sums for each of the prey items. 
sumFoods_0_100 <- colSums(LTdiet[which(LTdiet$size_class == "[0,100]"), 21:39], na.rm = FALSE, dims = 1)
sumFoods_100_200 <- colSums(LTdiet[which(LTdiet$size_class == "]100,200]"), 21:39], na.rm = FALSE, dims = 1)
sumFoods_200_300 <- colSums(LTdiet[which(LTdiet$size_class == "]200,300]"), 21:39], na.rm = FALSE, dims = 1)
sumFoods_300_400 <- colSums(LTdiet[which(LTdiet$size_class == "]300,400]"), 21:39], na.rm = FALSE, dims = 1)
sumFoods_400_500 <- colSums(LTdiet[which(LTdiet$size_class == "]400,500]"), 21:39], na.rm = FALSE, dims = 1)
sumFoods_500_up <- colSums(LTdiet[which(LTdiet$size_class == "]500,∞]"), 21:39], na.rm = FALSE, dims = 1)

# I'm placing these sums in a data frame for now, so I can obtain row names for the next little code chunk. 
contents.all.SC <- data.frame(sumFoods_0_100, sumFoods_100_200, sumFoods_200_300, sumFoods_300_400, sumFoods_400_500, sumFoods_500_up)

# Now I'm obtaining the prey ID's so I can eventually use them as a filter in my barplots. 
Food.ID <- rownames(contents.all.SC)

# I'm now calculating percentages of each of the prey items for each size class. 
propFoods_0_100 <- sumFoods_0_100 / sum(sumFoods_0_100, na.rm = TRUE) * 100
propFoods_100_200 <- sumFoods_100_200 / sum(sumFoods_100_200, na.rm = TRUE) * 100
propFoods_200_300 <- sumFoods_200_300 / sum(sumFoods_200_300, na.rm = TRUE) * 100
propFoods_300_400 <- sumFoods_300_400 / sum(sumFoods_300_400, na.rm = TRUE) * 100
propFoods_400_500 <- sumFoods_400_500 / sum(sumFoods_400_500, na.rm = TRUE) * 100
propFoods_500_up <- sumFoods_500_up / sum(sumFoods_500_up, na.rm = TRUE) * 100

# Tossing in the proportions as well as prey ID into the data frame. 
contents.all.SC <- data.frame(sumFoods_0_100, propFoods_0_100, sumFoods_100_200, propFoods_100_200, sumFoods_200_300, propFoods_200_300, sumFoods_300_400, propFoods_300_400, sumFoods_400_500, propFoods_400_500, sumFoods_500_up, propFoods_500_up, Food.ID)

# Making sure all of the NAs in the data frame are 0, because ggplot doesn't like creating graphs with NAs.
contents.all.SC[is.na(contents.all.SC)] <- 0

# Barplot for 0-100 size class.
barplot_0_100 <- ggplot(data = contents.all.SC, mapping = aes(x = "", y = propFoods_0_100, fill = contents.all.SC$Food.ID)) +
  geom_bar(stat = "identity") +
  theme(axis.title.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(y = "Proportion of Total Diet", fill = "Food Item", title = "[0-100]")

# Barplot for 101-200 size class.
barplot_100_200 <- ggplot(data = contents.all.SC, mapping = aes(x = "", y = propFoods_100_200, fill = contents.all.SC$Food.ID)) +
  geom_bar(stat = "identity") +
  theme(axis.title.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(y = "Proportion of Total Diet", fill = "Food Item", title = "]100-200]")

# Barplot for 201-300 size class. 
barplot_200_300 <- ggplot(data = contents.all.SC, mapping = aes(x = "", y = propFoods_200_300, fill = contents.all.SC$Food.ID)) +
  geom_bar(stat = "identity") +
  theme(axis.title.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(y = "Proportion of Total Diet", fill = "Food Item", title = "]200-300]")

# Barplot for 301-400 size class. 
barplot_300_400 <- ggplot(data = contents.all.SC, mapping = aes(x = "", y = propFoods_300_400, fill = contents.all.SC$Food.ID)) +
  geom_bar(stat = "identity") +
  theme(axis.title.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(y = "Proportion of Total Diet", fill = "Food Item", title = "]300-400]")

# Barplot for 401-500 size class. 
barplot_400_500 <- ggplot(data = contents.all.SC, mapping = aes(x = "", y = propFoods_400_500, fill = contents.all.SC$Food.ID)) +
  geom_bar(stat = "identity") +
  theme(axis.title.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(y = "Proportion of Total Diet", fill = "Food Item", title = "]400-500]")

# Barplot for 501 and up size class. 
barplot_500_up <- ggplot(data = contents.all.SC, mapping = aes(x = "", y = propFoods_500_up, fill = contents.all.SC$Food.ID)) +
  geom_bar(stat = "identity") +
  theme(axis.title.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(y = "Proportion of Total Diet", fill = "Food Item", title = "]500-∞]")

# Converting all of my ggplot outputs to ggplotly outputs so the users can be interactive with the outputs. 
ggplotly(barplot_0_100)
ggplotly(barplot_100_200)
ggplotly(barplot_200_300)
ggplotly(barplot_300_400)
ggplotly(barplot_400_500)
ggplotly(barplot_500_up)
```


### Stomach Contents in the Main Lake

Share of the diet for main lake. <br>
/!\\ no age class distinction here.

```{r contents.by.location}
# Creating a new variable called Lake.Segment based on the capture location. Segments determined from epa.gov document. Hill Bay and Central Lake, NY locations were unidentified on Google Maps, and were therefore placed in the "Unknown" lake segment. 
LTdiet <- LTdiet %>% mutate(Lake.Segment = case_when(Capture.Location == "Au Sable Point" ~ "Main Lake",
                                    Capture.Location == "Boquet Bay" ~ "Main Lake",
                                    Capture.Location == "Boquet Delta to Essex" ~ "Main Lake",
                                    Capture.Location == "Boquet River" ~ "Main Lake",
                                    Capture.Location == "Boquet River Delta" ~ "Main Lake",
                                    Capture.Location == "Burlington Bay" ~ "Main Lake",
                                    Capture.Location == "Burlington Bay Far" ~ "Main Lake",
                                    Capture.Location == "Burlington Far" ~ "Main Lake",
                                    Capture.Location == "Essex" ~ "Main Lake",
                                    Capture.Location == "Essex  " ~ "Main Lake",
                                    Capture.Location == "Essex to Bouquet Delta" ~ "Main Lake",
                                    Capture.Location == "Essex, NY" ~ "Main Lake",
                                    Capture.Location == "Essex, NY to Whallon" ~ "Main Lake",
                                    Capture.Location == "Jackson Point" ~ "Main Lake",
                                    Capture.Location == "Port Kent" ~ "Main Lake",
                                    Capture.Location == "Providence Island" ~ "Main Lake",
                                    Capture.Location == "Rockwell" ~ "Main Lake",
                                    Capture.Location == "Rockwell Bay" ~ "Main Lake",
                                    Capture.Location == "South Hero- Jackson Pt" ~ "Main Lake",
                                    Capture.Location == "South Hero- Rockwell Bay" ~ "Main Lake",
                                    Capture.Location == "South Hero-Jackson Pt" ~ "Main Lake",
                                    Capture.Location == "Trembleau Point" ~ "Main Lake",
                                    Capture.Location == "Valcour" ~ "Main Lake",
                                    Capture.Location == "Whallon" ~ "Main Lake",
                                    Capture.Location == "Whallon Bay" ~ "Main Lake",
                                    Capture.Location == "Whallon-Essex" ~ "Main Lake",
                                    Capture.Location == "Wilcox" ~ "Main Lake",
                                    Capture.Location == "Willsboro Bay, NY" ~ "Main Lake",
                                    Capture.Location == "Winooski" ~ "Main Lake"))

# Summing up food items for each lake segment.
sumFoods_MainLake <- colSums(LTdiet[which(LTdiet$Lake.Segment == "Main Lake"), 21:39], na.rm = FALSE, dims = 1)

# Converting sums to proportions of total diet.
propFoods_MainLake <- sumFoods_MainLake / sum(sumFoods_MainLake, na.rm = TRUE) * 100

# Creating a data frame with all sums and proportions.
contents.lake.segment <- data.frame(sumFoods_MainLake, propFoods_MainLake)

# Making sure all of my NAs are 0's again.
contents.lake.segment[is.na(contents.lake.segment)] <- 0

# Barplot for diet distribution in Main Lake. 
barplot_MainLake <- ggplot(data = contents.lake.segment, mapping = aes(x = "", y = propFoods_MainLake, fill = contents.all.SC$Food.ID)) +
  geom_bar(stat = "identity") +
  theme(axis.title.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(y = "Proportion of Total Diet", fill = "Food Item", title = "Main Lake")

# Converting ggplot outputs to ggplotly outputs. 
ggplotly(barplot_MainLake)

```

# Atlantic Salmon

# Burbot

# Lamprey

Note: some data are available in FSAdata -- SLampreyGL: Stock and recruitment data for Sea Lamprey in the Great Lakes, 1997-2007.

```{r read sea lamprey data, include=FALSE}
trap <- read.delim(paste0(getpath4data(), "data_from_Brad_Young/sea_lamprey_trapping_data.txt"))
trap_annual_summ <- read.delim(paste0(getpath4data(), "data_from_Brad_Young/annual_trapping_summary.txt"))
wound <- read.delim(paste0(getpath4data(), "data_from_Brad_Young/wounding_LAT_LAS.txt"))
annual_survey <- read.delim(paste0(getpath4data(), "data_from_Brad_Young/sea_lamprey_survey_data.txt"))
annual_survey[,2] <- as.numeric(paste(annual_survey[,2]))
annual_survey[,3] <- as.numeric(paste(annual_survey[,3]))
annual_survey[,4] <- as.numeric(paste(annual_survey[,4]))
```

## Wounding 
Lake Champlain salmonids presented a high rate of woundings which motivated the long-term sea lamprey control program, operated by VTFWS. The figure below shows the evolution of wounding per 100 fish.
```{r wounding by sea lamprey, echo=FALSE, message=FALSE, warning=FALSE}
p <- ggplot(melt(wound, id.vars='Year'), aes(Year, value, fill=variable)) +
  geom_bar(stat="identity", position=position_dodge()) + 
  geom_text(aes(label=value), vjust=1.6, hjust=c(rep(1.3,16), rep(-0.3,16)), color="black", size=2.7) + 
  ylab("Sea Lamprey wounds per 100 fish") + 
  scale_fill_manual(values=c('#999999','#5ab4ac'),name = "Species",  labels = c("Lake Trout \n(533-633 mm)", "Atlantic Salmon \n(432-533 mm)")) +
  theme_bw() +
  scale_x_continuous("Year", labels = as.character(wound$Year), breaks = wound$Year)

p + 
  geom_hline(aes(yintercept = 25, colour = "Lake Trout goal"), color='black', lty=2,show.legend = F) + 
  geom_hline(aes(yintercept = 15, colour = "Atlantic Salmon goal"), color='black', lty=3,show.legend = F) +
  geom_text(aes(x=2003,y=25, label="Lake Trout goal"),hjust = 0, vjust=1.2) +
  geom_text(aes(x=2003,y=15, label="Atlantic Salmon goal"),hjust = 0, vjust=1.2)

```
<br>`r fig_cap("woundings per salmonid", "Lake Champlain Sea Lamprey Wounds per 100 fish")`

## Management of sea lamprey population
Work on trapping data.
`r fig_cap("total catch vs. number surveys", "Total number of lamprey caught versus the total number of year surveyed. Note that sometimes sites with a same name were sampled twice a year (for example, Lewis Stream was sampled twice in 2005, 2009, 2013. Reach 1: Mouth to falls in North Ferrisburgh, Reach 2: falls to Scott Dam). The actual number of surveys may be higher.", display= FALSE)`

```{r working on trapping data, echo=FALSE}
trap$Date <- as.Date(trap$Date, format="%d-%b-%Y")
summary(trap)

trap_summ <- as.data.frame(with(trap, tapply(rep(1,nrow(trap)),list("Year"=Year, "River"=River), sum, na.rm=T)))


trap_summ2 <- data.frame("Total.catch"=colSums(trap_summ,na.rm = T),
                         "Number.surveys"=sapply(trap_summ, function(x) sum(!is.na(x))))
trap_summ2 <- trap_summ2[order(trap_summ2$Total.catch, decreasing = TRUE),]

ggplot(trap_summ2, aes(rownames(trap_summ2),Total.catch, fill=trap_summ2$Number.surveys)) +
  geom_bar(stat="identity", position=position_dodge()) +  labs(x="Stations", y="Number lamprey caught", fill = "Number of years surveyed") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1),legend.position="bottom",
        panel.background = element_rect(fill = "white", colour = "grey50"))

```
<br>`r fig_cap("total catch vs. number surveys", display="full")`


Great Chazy river (NY) and Lewis stream (VT) are the two sites with the most lampreys `r fig_cap("total catch vs. number surveys", display="cite")`.

```{r visualize total catch versus number of surveys}
ggplot(trap_summ2, aes(Number.surveys, Total.catch)) + 
  labs(x="Total number of years surveyed", y="Total number of lamprey caught") + 
  geom_label(aes(Number.surveys, Total.catch), label=rownames(trap_summ2), nudge_y = -150,alpha=.8) + 
  geom_point(alpha=.5) + 
  theme_classic() 
```
<br>`r fig_cap("total catch vs. number surveys", display="full")`

## Lamprey weigth-length relationship from trap data

### Fit linear model
```{r fit linear model sea lamprey}
par(mar=c(5.1,4.1,4.1,4.1))
hist(trap$length)
trap$logL <- log10(trap$length/10)
trap$logW <- log10(trap$weight)
lm1 <- lm(logW~logL,data = trap)
fitPlot(lm1,xlab="Log total length", ylab = "Log weight", main="")
axis(3, at=log10(c(0:10,seq(10,90,10),seq(100,500,100))), labels = c(0:10,seq(10,90,10),seq(100,500,100)))
mtext("Total length (cm)", side=3, line = 2.5)
axis(4, at=log10(c(0:9,seq(10,100,10),seq(0,1000,100))), labels=c(0:9,seq(10,100,10),seq(0,1000,100)))
mtext("Total weight (g)", side=4, line = 2.5)
summary(lm1)
```
<br>`r fig_cap("lm w l relationship sea lamprey", caption="Weight-length relationship (log scale) for sea lamprey.")`

### Conclusion a and b parameters
To convert them to the initial equation W = aL<sup>b</sup>:  
```{r conclusion a and b parameters for sea lamprey, message = FALSE, warning = FALSE}
(a = 10^(lm1$coefficients[1]))
(b = lm1$coefficients[2])
```

Therefore, the initial equation is W = 0.01726149 x L<sup>2.429350</sup>


## Survey data

```{r}
head(annual_survey)

ggplot(annual_survey, aes(year_surveyed, num_ammocoetes)) +
  geom_bar(stat="identity", position=position_dodge()) +  labs(x="Stations", y="Number lamprey caught", fill = "Number of years surveyed") + facet_wrap(~river_trib_system) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),legend.position="bottom",
        panel.background = element_rect(fill = "white", colour = "grey50"))
```
<br>`r fig_cap("num_ammocoetes per year", caption="Number of ammocoetes caught per year, and per sites")`


## Get biomass for model

The mortality between the ammocoetes and transformers stage is really high, so using the number of ammocoetes per streams may not be the best approach. 
However, trapping of adults is really efficient (~95% said Ellen), so backtracking and estimating the number of individuals at the parasitoid stage. Sea lamprey will spend 12-18 months in the parasitoid stage.

"Estimates of the lake-wide out-migrating transformer population for the 2002 and 2003 parasitic-phase cohorts were 269,139 ± 55,610 (SD) and 111,807 ± 23,511 (SD)" (Howe et al, 2006^[Howe, E.A., Ellen Marsden, J., Bouffard, W., 2006. Movement of Sea Lamprey in the Lake Champlain Basin. Journal of Great Lakes Research 32, 776–787. https://doi.org/10.3394/0380-1330(2006)32[776:MOSLIT]2.0.CO;2]).

# Stocking

## VTFWS long-term data 
Data communicated by Ellen on September 23rd, 2019. She presents them in Marsden et al (2018)^[].

```{r read long term stocking data VT, include=FALSE}
stklkt <- read.delim(paste0(getpath4data(),"data_from_Ellen/LC_LKT_stocking_history_1972_2018.txt"))
head(stklkt)
#stklkt_summ <- read.delim(paste0(getpath4data(),"data_from_Ellen/LC_LKT_stocking_history_1972_2018_summ.txt"))

# create a summary
stklkt_summ <- as.data.frame(with(stklkt, tapply(NB_EQUIV,list("Year"=YEAR,"Stocking_site"=STOCKING_SITE), sum, na.rm=T)))
stklkt_summ$TOTAL <- rowSums(stklkt_summ, na.rm = T)

```

```{r plot stoking lkt data long term, echo=FALSE, message=FALSE, warning=FALSE}
p1 <- ggplot(stklkt_summ, aes(x=as.numeric(paste(rownames(stklkt_summ))),y=TOTAL)) +
  geom_bar(stat="identity") + xlab("Year") + ylab("Number of yearling-equivalent lake trout") + theme_bw()
p1 + scale_y_continuous(labels = comma)
#ggplotly(p1)
```


## VTFWS online database

### Data

Right now I'm including just data from VT. Work more on all the stocking Rmd before including it to this synthesis.

Stocking data were obtained from the <a href= https://vtfishandwildlife.com/fishvt>VTFWD website</a> by clicking on the link 'Trout Stocking Report'.

```{r read stocking data VTFWD, include=FALSE}
stkvt <- read.delim(paste0(getpath4data(),'Stocking/VTFWD_Stocking_2009-2019.txt'))
nrow(stkvt)
length(stkvt$ACTUAL.NUMBER.STOCKED[!is.na(stkvt$ACTUAL.NUMBER.STOCKED)])
names(stkvt)

stkvt$DATE.STOCKED <- parse_date_time(x = stkvt$DATE.STOCKED,orders = c("m/d/y", "m/d/Y"))
```

Over the `r min(stkvt$Year)`-`r max(stkvt$Year)` period, `r nrow(stkvt)` stocking events were scheduled, and `r length(stkvt$ACTUAL.NUMBER.STOCKED[!is.na(stkvt$ACTUAL.NUMBER.STOCKED)])` stocking events took place.

```{r create summary data vtfws, echo=FALSE, message=FALSE, warning=FALSE}

summ_stkvt <- stkvt %>% group_by(as.factor(Year)) %>%
  summarise(mean=mean(ACTUAL.NUMBER.STOCKED, na.rm=T),
            sum=sum(ACTUAL.NUMBER.STOCKED, na.rm=T),
            scheduled=length(ACTUAL.NUMBER.STOCKED),
            went_through=length(ACTUAL.NUMBER.STOCKED[!is.na(ACTUAL.NUMBER.STOCKED)]),
            percent_done=went_through/scheduled)
summ_stkvt <- as.data.frame(summ_stkvt)

p1 <- ggplot(data=summ_stkvt, aes(x=summ_stkvt[,1],y=as.numeric(summ_stkvt[,2]))) +
  geom_bar(stat="identity") + xlab("Year") + ylab("mean # stocked") 

p2 <- ggplot(data=summ_stkvt, aes(x=summ_stkvt[,1],y=as.numeric(summ_stkvt[,3]))) +
  geom_bar(stat="identity") + xlab("Year") + ylab("sum # stocked") 

## convert plots to gtable objects
g1 <- ggplotGrob(p1)
g2 <- ggplotGrob(p2)
g <- rbind(g1, g2, size="first") # stack the two plots
g$widths <- unit.pmax(g1$widths, g2$widths) # use the largest widths
# center the legend vertically
g$layout[grepl("guide", g$layout$name),c("t","b")] <- c(1,nrow(g))
grid.newpage()
grid.draw(g)

colnames(summ_stkvt) <- c("Year","Mean #fish stocked", "Total #fish stocked", "Schedule events", "Stocking that went through", "% stocking done")
summ_stkvt

```
<br>`r fig_cap("vtfws species stocked number", caption = "Mean and total number of fish stocked by VTFWS from 2009 to 2019.")`


In 2019, there are still many stocking events that were not carried out. We're removing this year from the analysis.

```{r remove 2019 from the dataframe}
stkvt <- stkvt[stkvt$Year!=2019,]
```

### Species stocked

What are the species stocked by VTFWD? We're only looking at the effective stocking now.

```{r look at the species stocked by vtfws, echo=FALSE, message=FALSE, warning=FALSE}

sp_stkvt <- stkvt %>% group_by(as.factor(SPECIES)) %>%
  summarise(mean=mean(ACTUAL.NUMBER.STOCKED, na.rm=T),
            sum=sum(ACTUAL.NUMBER.STOCKED, na.rm=T),
            scheduled=length(ACTUAL.NUMBER.STOCKED),
            went_through=length(ACTUAL.NUMBER.STOCKED[!is.na(ACTUAL.NUMBER.STOCKED)]),
            percent_done=went_through/scheduled)

ggplot(data=stkvt, aes(x=SPECIES,y=ACTUAL.NUMBER.STOCKED, fill=as.factor(Year))) +
  geom_bar(stat="identity") + xlab("Species") + ylab("total # stocked") +
  labs(fill="Year") + ggtitle("All Vermont")

ggplot(data=stkvt[grep("Champ",stkvt$WATER),], aes(x=SPECIES,y=ACTUAL.NUMBER.STOCKED, fill=as.factor(Year))) +
  geom_bar(stat="identity") + xlab("Species") + ylab("total # stocked") +
  labs(fill="Year") + ggtitle("Lake Champlain waters")



```
<br>`r fig_cap("vtfws species stocked", caption = "Species stocked in Lake Champlain waters by VTFWS.")`

## NYDEC online database

### Read data and basic stats

Stocking data were obtained from the <a href= "https://data.ny.gov/Recreation/Fish-Stocking-Lists-Actual-Beginning-2011/9hpx-asd8">NYDEC website</a>.

```{r read the stocking data from nydec, include=FALSE}
stkny <- read.delim(paste0(getpath4data(),"Stocking/NYDEC_Stocking_2011-2019.txt"))
nrow(stkny)
names(stkny)

#QAQC some data
stkny$Town[stkny$Town=="plattsburgh"] <- "Plattsburgh"

```

Over the `r min(stkny$Year)`-`r max(stkny$Year)` period, `r nrow(stkny)` stocking events took place.

```{r summary stocking data nydec, echo=FALSE, message=FALSE, warning=FALSE}

summ_stkny <- stkny %>% group_by(as.factor(Year)) %>%
  summarise(mean=mean(Number, na.rm=T),
            sum=sum(Number, na.rm=T),
            went_through=length(Number))
summ_stkny <- as.data.frame(summ_stkny)

p1 <- ggplot(data=summ_stkny, aes(x=summ_stkny[,1],y=as.numeric(summ_stkny[,2]))) +
  geom_bar(stat="identity") + xlab("Year") + ylab("mean # stocked") 

p2 <- ggplot(data=summ_stkny, aes(x=summ_stkny[,1],y=as.numeric(summ_stkny[,3]))) +
  geom_bar(stat="identity") + xlab("Year") + ylab("sum # stocked") 

## convert plots to gtable objects
g1 <- ggplotGrob(p1)
g2 <- ggplotGrob(p2)
g <- rbind(g1, g2, size="first") # stack the two plots
g$widths <- unit.pmax(g1$widths, g2$widths) # use the largest widths
# center the legend vertically
g$layout[grepl("guide", g$layout$name),c("t","b")] <- c(1,nrow(g))
grid.newpage()
grid.draw(g)

colnames(summ_stkny) <- c("Year","Mean #fish stocked", "Total #fish stocked", "Stocking events")
summ_stkny

```


### Species stocked

What are the species stocked by VTFWD? We're only looking at the effective stocking now.

```{r look at the species stocked by nydec, echo=FALSE, message=FALSE, warning=FALSE}

sp_stkny <- stkny %>% group_by(as.factor(Species)) %>%
  summarise(mean=mean(Number, na.rm=T),
            sum=sum(Number, na.rm=T),
            went_through=length(Number))

ggplot(data=stkny, aes(x=Species,y=Number, fill=as.factor(Year))) +
  geom_bar(stat="identity") + xlab("Species") + ylab("total # stocked") +
  labs(fill="Year") + ggtitle("All NY")

ggplot(data=stkny[grep("Champ",stkny$Waterbody),], aes(x=Species,y=Number, fill=as.factor(Year))) +
  geom_bar(stat="identity") + xlab("Species") + ylab("total # stocked") +
  labs(fill="Year") + ggtitle("Lake Champlain waters")


```


# Summary of data availability

The plot below summarizes which data are available, for which period for Lake Champlain.

```{r coarse summary of data availability, echo=FALSE, message=FALSE, warning=FALSE}
summary_data <- as.data.frame(matrix(
  c(
c("Phytoplankton",        min(year(phyto$VisitDate)), max(year(phyto$VisitDate)), "VTDEC"),
c("Zooplankton",          min(year(zoo$VisitDate)), max(year(zoo$VisitDate)), "VTDEC"),
c("Mysis",                min(mysis$Year), max(mysis$Year), "Stockwell et al."),
c("Benthic invertebrates",NA,NA,NA),
c("Sculpin",              NA,NA,NA),
c("Trout-perch",          min(tps$year), max(tps$year), "Lake trout recruitment survey"),
c("Smelt",                min(smelt$Year), max(smelt$Year), "Forage fish survey"),
c("Whitefish",            min(tps$year), max(tps$year), "Lake trout recruitment survey"),
c("Cisco",                NA,NA,NA),
c("Lake Trout",           min(tps$year), max(tps$year), "Lake trout recruitment survey"),
c("Atlantic Salmon",      NA,NA,NA),
c("Burbot",               NA,NA,"Great Lakes data?"),
c("Sea lamprey",          NA,NA,"VTFWS")
), ncol=4, byrow = T))

colnames(summary_data) <- c("Species","MinYear","MaxYear","Source")
summary_data$MaxYear <- as.numeric(paste(summary_data$MaxYear))
summary_data$MinYear <- as.numeric(paste(summary_data$MinYear))

# color palette
mycol <- wes_palette("Darjeeling1", length(unique(summary_data$Source[!is.na(summary_data$Source)])), type="continuous")
source_data <- unique(summary_data$Source[!is.na(summary_data$Source)])

par(mar=c(5,6,2,2))
plot(c(min(summary_data$MinYear, na.rm = T),max(summary_data$MinYear, na.rm = T)), c(1,length(unique(summary_data$Species))), pch=NA, xlab="Year", ylab="", axes=F, main="Not updated since 09/09/19 - see next plot instead \n (colors refer to different data collection program)")
axis(1, at=1970:2020, labels=NA, lwd = .5); axis(1,at=1970:2020, labels=NA, lwd.ticks = 0)
axis(1)
for (i in 1:length(unique(summary_data$Species))) {
  msubset = summary_data[summary_data$Species==unique(summary_data$Species)[i],]
  for (j in 1:nrow(msubset)) {
    lines(msubset[,c("MinYear","MaxYear")],
        rep(c(i+(j/100)),2), col=mycol[source_data==msubset$Source], lwd=3)
  }
  par(xpd=T)
  text(x=min(summary_data$MinYear, na.rm = T),y=i,msubset$Species[1],pos=2)
  par(xpd=F)
}


```

<br>`r fig_cap("summary biomass coarse", caption = "Data availability -- stopped working on this plot, because at the moment I think the option below conveys more information.")`

```{r better summary of data availability, echo=FALSE, message=FALSE, warning=FALSE}
summary_ls <- list(
  "Phytoplankton" = data.frame("Year"=c(min(dtlcm$Year):max(dtlcm$Year)),"Biovolume" = as.vector(with(dtlcm, tapply(Net.phytoplankton..total.biovolume,list("Year"=year(as.Date(dtlcm$VisitDate, format="%d/%m/%Y"))), mean, na.rm=T)))),
  "Zooplankton" = data.frame("Year"=zoo19_month_Predator$Year, "Biomass"=rowSums(zoo19_month_Predator[,-1], na.rm = T) + rowSums(zoo19_month_Grazer_sm[,-1], na.rm = T) +rowSums(zoo19_month_Grazer_lr[,-1], na.rm = T)),
  "Mysids" = mysis_summ[,c("Year","mean_w")],
  "Zebra mussel" = data.frame("Year"=rownames(ZMvel_summ), "Densities"=ZMvel_summ$STA19),
  "Sculpin" = NA,
  "Trout-perch" = data.frame("Year"=c(2016:2018),"Density"=rep(0.5,length(2016:2018))),
  "Smelt" = data.frame("Year"=smelt_sub_summ$Year, "CPUE"=smelt_sub_summ$CPUE),
  "Whitefish" = data.frame("Year"=c(1982:1998,2016:2018),"Density"=rep(0.5,length(c(1982:1998,2016:2018)))),
  "Cisco" = data.frame("Year"=cisco$Year,"CPUE"=cisco$JuniperIsland),
  "Alewife" = data.frame("Year"=c(2016:2018),"Density"=rep(0.5,length(c(2016:2018)))),
  "Walleye" = NA,
  "Stocked juvenile lake trout" = data.frame("Year"=1972:2016, "Stocked"=as.vector(t(stklkt_summ["TOTAL",as.numeric(paste(colnames(stklkt_summ))) %in% 1972:2016]))),
  "Lake trout" = data.frame("Year"=c(1982:1998,2016:2018),"Density"=rep(0.5,length(c(1982:1998,2016:2018)))),
  "Atlantic salmon" = NA,
  "Burbot" = data.frame("Year"=c(1982:1998),"Density"=rep(0.5,length(c(1982:1998)))),
  "Sea lamprey" = data.frame("Year"=c(1990:2018),"Density"=rep(0.5,length(1990:2018)))
)

# Replacing NA of phytoplankton by 0.5 because we have the data -- just no estimate of biomass right now
summary_ls$Phytoplankton[is.na(summary_ls$Phytoplankton$Biovolume),2] <- max(summary_ls$Phytoplankton$Biovolume,na.rm=T)

if(!R_U_KNITTING) pdf(paste0(getwd(),"/Output/Figures/5-Summary/summary_biomass2.pdf"),width = 9, height = 5)
layout(matrix(c(1,2), nrow=1), widths=c(3.5,1.2))
par(mar=c(5,10,4,0.5))
plot(c(1971,2019), c(1,length(summary_ls)), pch=NA, xlab="Year", ylab="", axes=F)
axis(1, at=1960:2020, labels=NA, lwd = .5); axis(1,at=1960:2020, labels=NA, lwd.ticks = 0)
axis(1)

axis(3, at=1960:2020, labels=NA, lwd = .5); axis(3,at=1960:2020, labels=NA, lwd.ticks = 0)
axis(3)

for (i in 1:length(summary_ls)) {
  abline(h=i,lty=2, col=adjustcolor("grey", alpha.f = .4))
  msubset = summary_ls[[i]]
  if(any(!is.na(msubset))) {
    # Set which year we actually have all data
    #default
    whichyear= msubset[,1]
    if(names(summary_ls[i])=="Phytoplankton") whichyear= msubset$Year[msubset$Year %in% 2006:2015]
    if(names(summary_ls[i])%in% c("Trout-perch", "Whitefish", "Lake trout", "Sea lamprey","Burbot")) whichyear= NULL
    if(names(summary_ls[i])=="Smelt") whichyear= msubset$Year[msubset$Year %in% 1998:2015]
    if(names(summary_ls[i])=="stocked lake trout") whichyear=as.numeric(paste(summ_stkvt$Year))
    
    msubset[,1] <- as.numeric(paste(msubset[,1]))
    msubset[,2] <- as.numeric(paste(msubset[,2]))
    # Add rows without data to visualize on the plot
    # If data are continuous already, msubset_2 will just be a copy of msubset
    msubset_2 <- rbind(data.frame("x" = msubset[,1],
                                  "y" = msubset[,2]),
                       data.frame("x" = c(min(msubset[,1]):max(msubset[,1]))[!min(msubset[,1]):max(msubset[,1])%in%msubset[,1]],
                                  "y" = rep(NA,length(which(!min(msubset[,1]):max(msubset[,1])%in%msubset[,1])))))
    msubset_2 <- msubset_2[order(msubset_2$x),]
    lines(msubset_2[,1],ifelse(!is.na(msubset_2[,2]),i,NA), col="grey", lwd=1.5)
    x <- msubset_2[,1]
    w <- msubset_2[,2]/max(msubset_2[,2], na.rm=T)
    w[is.na(w)] <- 0
    polygon(x = c(x,rev(x)), y=c((i+w/2),rev(i-w/2)), border = NA, col=adjustcolor("grey", alpha.f = .7))
    if(names(summary_ls[i])=="Burbot") TeachingDemos::shadowtext(mean(1982:1998),i, labels = "uncertainties about \ndata availability", col = "black",bg = "white", theta = seq(pi/4, 2 * pi, length.out = 8), r = 0.1, cex=.9)
    x <- msubset_2[msubset_2[,1] %in% whichyear,1]
    w <- msubset_2[msubset_2[,1] %in% whichyear,2]/max(msubset_2[msubset_2[,1] %in% whichyear,2], na.rm=T)
    w[is.na(w)] <- 0
    # if(names(summary_ls[i])%in%c("Alewife", "Zebra Mussel")) {
    #   polygon(x = c(x,rev(x)), y=c((i+w/2),rev(i-w/2)), border = "black", col=adjustcolor("pink", alpha.f = .6))
    # } else {
       polygon(x = c(x,rev(x)), y=c((i+w/2),rev(i-w/2)), border = NA, col=adjustcolor("black", alpha.f = .8))
    # }
    if(names(summary_ls[i])=="Smelt") {
     for (w in 1:2) {
       if (w==1) whichyear = 1990:1996
       if (w==2) whichyear = 1984:1985
    x <- msubset_2[msubset_2[,1] %in% whichyear,1]
    w <- msubset_2[msubset_2[,1] %in% whichyear,2]/max(msubset_2[msubset_2[,1] %in% whichyear,2], na.rm=T)
    w[is.na(w)] <- 0
     polygon(x = c(x,rev(x)), y=c((i+w/2),rev(i-w/2)), border = NA, col=adjustcolor("black", alpha.f = .7)) 
     }
    }

  }
  par(xpd=T)
  #text(x=min(summary_data$MinYear, na.rm = T),y=i,names(summary_ls[i]),pos=2)
  TeachingDemos::shadowtext(x=1971,y=i, labels = names(summary_ls[i]), col = "black",bg = "white", theta = seq(pi/4, 2 * pi, length.out = 8), r = 0.1, pos=2)
  par(xpd=F)
  
  # Add a different point if invasion
  if(names(summary_ls[i])=="Alewife") points(2003,i,pch=17, col="red")
  if(names(summary_ls[i])=="Zebra mussel") points(1993,i,pch=17, col="red")
  if(names(summary_ls[i])=="Zooplankton") points(c(2014,2018),rep(i,2),pch=17, col="red")
  
  # Add comment
  #if(names(summary_ls[i])=="Alewife") TeachingDemos::shadowtext(2004,i, labels = "probably more data available?", col = "black",bg = "white", theta = seq(pi/4, 2 * pi, length.out = 8), r = 0.1, cex=.9, pos=4)
  
  # Add a different point if work still in progress
  if(names(summary_ls[i])%in%c("Burbot","Atlantic Salmon", "Sculpin","Walleye")) points(max(summary_data$MinYear, na.rm = T),i,pch=4, col="red")
}
rect(2000,0.7,2002,i+0.3, lty=2, lwd=.5)

par(mar=c(5,0,2,1))
plot(c(1,10),c(1,10),axes=F,xlab="",ylab="", pch=NA)
legend(x=1,y=9.5, legend = c("Relative biomass","Biomass still \nbeing estimated"),
       fill=c("black",adjustcolor("grey", alpha.f = .7)), bty="n", border = NULL)
legend(x=1,y=7, legend = c("Species invasion", "No data yet", "Period chosen for \nmodel initialization"),
       pch=c(17,4,NA), lty=c(NA,NA,2), col=c("red","red","black"), bty="n", border = NULL)

if(!R_U_KNITTING) dev.off()
par(mfrow=c(1,1))


```
<br>`r fig_cap("summary biomass", caption = "Species selected for the initial model, and relative evolution of their biomass (width of polygon). We are still collecting data for some species (Burbot, Atlantic Salmon, Sculpin), and converting survey data to biomass (e.g., phytoplankton counts to biovolume before biovolumes started being reported by the state). Asterisks indicate invasive species, and triangles on the timeline indicates the time of invasion.")`

```{r summary of environmental data availability, echo=FALSE, message=FALSE, warning=FALSE}
# Similar figure than before, but with drivers / environmental variables
weather <- read.delim(paste0(getpath4data(), "Weather/allWeatherBTV_join.txt"))
weather$date <- as.Date(weather$date, format="%d/%m/%Y")
weather$Year <- year(weather$date)
weather$Month <- month(weather$date)
# plot(with(weather[weather$Month %in% c(12:3),], tapply(AvgApparentTemperatureCelsius,list("Year"=Year), mean, na.rm=T)), type="b")

summary_env_var <- list(
  "Summer average air temperature (degC)" = data.frame("Year"=unique(weather$Year),"SumAirTemp" = as.vector(with(weather[weather$Month %in% c(6:9),], tapply(AvgApparentTemperatureCelsius,list("Year"=Year), mean, na.rm=T)))),
  
  "Winter average air temperature (degC)" = data.frame("Year"=unique(weather$Year),"WintAirTemp" = as.vector(with(weather[weather$Month %in% c(12,1:3),], tapply(AvgApparentTemperatureCelsius,list("Year"=Year), mean, na.rm=T)))),
  
  "Average air temperature (degC)" = data.frame("Year"=unique(weather$Year),"AvgAirTemp" = as.vector(with(weather, tapply(AvgApparentTemperatureCelsius,list("Year"=Year), mean, na.rm=T)))),
  
  "Water temperature epilimnion (degC)" = data.frame("Year"=c(min(dtlcm$Year):max(dtlcm$Year)),"WatTemp" = as.vector(with(dtlcm[dtlcm$StationID==21 & month(as.Date(dtlcm$VisitDate, format="%d/%m/%Y")) %in% c(4:10),], tapply(Temperature_E,list("Year"=year(as.Date(dtlcm$VisitDate[dtlcm$StationID==21 & month(as.Date(dtlcm$VisitDate, format="%d/%m/%Y")) %in% c(4:10)], format="%d/%m/%Y"))), mean, na.rm=T)))),
  "Dissolved phosphorus epilimnion (mug/l)" = data.frame("Year"=c(min(dtlcm$Year):max(dtlcm$Year)),"DissolvedP" = as.vector(with(dtlcm[dtlcm$StationID==21 & month(as.Date(dtlcm$VisitDate, format="%d/%m/%Y")) %in% c(4:10),], tapply(Dissolved.Phosphorus_E,list("Year"=year(as.Date(dtlcm$VisitDate[dtlcm$StationID==21 & month(as.Date(dtlcm$VisitDate, format="%d/%m/%Y")) %in% c(4:10)], format="%d/%m/%Y"))), mean, na.rm=T)))),
  "Secchi depth (m)" = data.frame("Year"=c(min(dtlcm$Year):max(dtlcm$Year)),"Secchi.Depth" = as.vector(with(dtlcm[dtlcm$StationID==21,], tapply(Secchi.Depth,list("Year"=year(as.Date(dtlcm$VisitDate[dtlcm$StationID==21], format="%d/%m/%Y"))), mean, na.rm=T))))
)


if(!R_U_KNITTING) pdf(paste0(getwd(),"/Output/Figures/5-Summary/summary_env_variables.pdf"),width = 12, height = 8)
layout(matrix(c(1,0,2,0,3,4), nrow=3, byrow = T), widths=c(3.5,1), heights=c(1.2,.8,2.5))
par(mar=c(0,10,5,3.5))
plot(summary_env_var$`Summer average air temperature (degC)`, type="l", xlab="", ylab="Temperature (deg C)", axes=F, xlim=c(1971,2019),
     ylim=c(min(summary_env_var$`Winter average air temperature (degC)`[,2], na.rm=T)-1, max(summary_env_var$`Summer average air temperature (degC)`[,2], na.rm=T)+1))
lines(summary_env_var$`Average air temperature (degC)`,lwd=2)
lines(summary_env_var$`Winter average air temperature (degC)`)
lines(summary_env_var$`Water temperature epilimnion (degC)`[-c(1:3),], col="darkcyan")
TeachingDemos::shadowtext(summary_env_var$`Summer average air temperature (degC)`[3,], labels = "summer (June-September)", col = "black",bg = "white", theta = seq(pi/4, 2 * pi, length.out = 8), r = 0.1, pos=3)
TeachingDemos::shadowtext(summary_env_var$`Winter average air temperature (degC)`[3,], labels = "winter (December-March)", col = "black",bg = "white", theta = seq(pi/4, 2 * pi, length.out = 8), r = 0.1, pos=1)
TeachingDemos::shadowtext(summary_env_var$`Average air temperature (degC)`[1,], labels = "mean annual temperature", col = "black",bg = "white", theta = seq(pi/4, 2 * pi, length.out = 8), r = 0.1, pos=1, font=2)
TeachingDemos::shadowtext(summary_env_var$`Water temperature epilimnion (degC)`[5,], labels = "epilimnion temperature (April-October)", col = "darkcyan",bg = "white", theta = seq(pi/4, 2 * pi, length.out = 8), r = 0.1, pos=1)

axis(3, at=1960:2020, labels=NA, lwd = .5); axis(3,at=1960:2020, labels=NA, lwd.ticks = 0)
axis(3)
axis(2)
axis(2, at=c(-20,30), labels=NA)
mtext("Year", side = 3, line = 2.2)

#2nd plot
par(mar=c(4,10,0,3.5))
plot(summary_env_var$`Dissolved phosphorus epilimnion (mug/l)`, type="l", xlab="Year", ylab="", axes=F, xlim=c(1971,2019), col="darkgreen")
points(summary_env_var$`Dissolved phosphorus epilimnion (mug/l)`,pch=16, col="darkgreen")
axis(1, at=1960:2030, labels=NA, lwd = .5); axis(1,at=1960:2030, labels=NA, lwd.ticks = 0)
axis(1)
axis(4, col="darkgreen", col.ticks = "darkgreen", col.axis = "darkgreen")
axis(4, at=c(0,20), labels=NA, col="darkgreen")
mtext("Dissolved phosphorus\nepilimnion (µg/l)", side = 4, line = 3.2, cex=.7, col="darkgreen")

#3rd plot
par(mar=c(5,10,4,3.5))
plot(c(1971,2019), c(1,length(summary_ls)), pch=NA, xlab="Year", ylab="", axes=F)
axis(1, at=1960:2020, labels=NA, lwd = .5); axis(1,at=1960:2020, labels=NA, lwd.ticks = 0)
axis(1)

axis(3, at=1960:2020, labels=NA, lwd = .5); axis(3,at=1960:2020, labels=NA, lwd.ticks = 0)
axis(3)
par(mar=c(5,10,4,0.5))

for (i in 1:length(summary_ls)) {
  abline(h=i,lty=2, col=adjustcolor("grey", alpha.f = .4))
  msubset = summary_ls[[i]]
  if(any(!is.na(msubset))) {
    # Set which year we actually have all data
    #default
    whichyear= msubset[,1]
    if(names(summary_ls[i])=="Phytoplankton") whichyear= msubset$Year[msubset$Year %in% 2006:2015]
    if(names(summary_ls[i])%in% c("Trout-perch", "Whitefish", "Lake trout", "Sea lamprey","Burbot")) whichyear= NULL
    if(names(summary_ls[i])=="Smelt") whichyear= msubset$Year[msubset$Year %in% 1998:2015]
    if(names(summary_ls[i])=="stocked lake trout") whichyear=as.numeric(paste(summ_stkvt$Year))
    
    msubset[,1] <- as.numeric(paste(msubset[,1]))
    msubset[,2] <- as.numeric(paste(msubset[,2]))
    # Add rows without data to visualize on the plot
    # If data are continuous already, msubset_2 will just be a copy of msubset
    msubset_2 <- rbind(data.frame("x" = msubset[,1],
                                  "y" = msubset[,2]),
                       data.frame("x" = c(min(msubset[,1]):max(msubset[,1]))[!min(msubset[,1]):max(msubset[,1])%in%msubset[,1]],
                                  "y" = rep(NA,length(which(!min(msubset[,1]):max(msubset[,1])%in%msubset[,1])))))
    msubset_2 <- msubset_2[order(msubset_2$x),]
    lines(msubset_2[,1],ifelse(!is.na(msubset_2[,2]),i,NA), col="grey", lwd=1.5)
    x <- msubset_2[,1]
    w <- msubset_2[,2]/max(msubset_2[,2], na.rm=T)
    w[is.na(w)] <- 0
    polygon(x = c(x,rev(x)), y=c((i+w/2),rev(i-w/2)), border = NA, col=adjustcolor("grey", alpha.f = .4))
    if(names(summary_ls[i])=="Burbot") TeachingDemos::shadowtext(mean(1982:1998),i, labels = "uncertainties about \ndata availability", col = "black",bg = "white", theta = seq(pi/4, 2 * pi, length.out = 8), r = 0.1, cex=.9)
    x <- msubset_2[msubset_2[,1] %in% whichyear,1]
    w <- msubset_2[msubset_2[,1] %in% whichyear,2]/max(msubset_2[msubset_2[,1] %in% whichyear,2], na.rm=T)
    w[is.na(w)] <- 0
    if(names(summary_ls[i])%in%c("Alewife", "Zebra Mussel")) {
      polygon(x = c(x,rev(x)), y=c((i+w/2),rev(i-w/2)), border = "black", col=adjustcolor("pink", alpha.f = .6))
    } else {
      polygon(x = c(x,rev(x)), y=c((i+w/2),rev(i-w/2)), border = NA, col=adjustcolor("black", alpha.f = .8))
    }
    if(names(summary_ls[i])=="Smelt") {
     for (w in 1:2) {
       if (w==1) whichyear = 1990:1996
       if (w==2) whichyear = 1984:1985
    x <- msubset_2[msubset_2[,1] %in% whichyear,1]
    w <- msubset_2[msubset_2[,1] %in% whichyear,2]/max(msubset_2[msubset_2[,1] %in% whichyear,2], na.rm=T)
    w[is.na(w)] <- 0
     polygon(x = c(x,rev(x)), y=c((i+w/2),rev(i-w/2)), border = NA, col=adjustcolor("black", alpha.f = .8)) 
     }
    }

  }
  par(xpd=T)
  #text(x=min(summary_data$MinYear, na.rm = T),y=i,names(summary_ls[i]),pos=2)
  TeachingDemos::shadowtext(x=1971,y=i, labels = names(summary_ls[i]), col = "black",bg = "white", theta = seq(pi/4, 2 * pi, length.out = 8), r = 0.1, pos=2)
  par(xpd=F)
  
  # Add a different point if invasion
  if(names(summary_ls[i])=="Alewife") points(2003,i,pch=17, col="red")
  if(names(summary_ls[i])=="Zebra mussel") points(1993,i,pch=17, col="red")
  if(names(summary_ls[i])=="Zooplankton") points(c(2014,2018),rep(i,2),pch=17, col="red")
  
  # Add comment
  if(names(summary_ls[i])=="Alewife") TeachingDemos::shadowtext(2004,i, labels = "probably more data available?", col = "black",bg = "white", theta = seq(pi/4, 2 * pi, length.out = 8), r = 0.1, cex=.9, pos=4)
  
  # Add a different point if work still in progress
  if(names(summary_ls[i])%in%c("Burbot","Atlantic Salmon", "Sculpin","Walleye")) points(max(summary_data$MinYear, na.rm = T),i,pch=4, col="red")
}
rect(2000,0.7,2002,i+0.3, lty=2, lwd=.5)

par(mar=c(5,0,2,1))
plot(c(1,10),c(1,10),axes=F,xlab="",ylab="", pch=NA)
legend(x=1,y=9.5, legend = c("Relative biomass","Biomass still \nbeing estimated","Invasive species"),
       fill=c("black",adjustcolor("grey", alpha.f = .4), adjustcolor("pink", alpha.f = .6)), bty="n", border = NULL)
legend(x=1,y=7, legend = c("Species invasion", "No data yet", "Period chosen for \nmodel initialization"),
       pch=c(17,4,NA), lty=c(NA,NA,2), col=c("red","red","black"), bty="n", border = NULL)

if(!R_U_KNITTING) dev.off()
par(mfrow=c(1,1))


```
<br>`r fig_cap("summary biomass and env var", caption = "Upper graph: Environmental parameters (summer, winter, and annual average temperature), epilimnion mean temperature (April-October), and dissolved phosphorus concentration. Lower graph: Species selected for the initial model, and relative evolution of their biomass (width of polygon). We are still collecting data for some species (Burbot, Atlantic Salmon, Sculpin), and converting survey data to biomass (e.g., phytoplankton counts to biovolume before biovolumes started being reported by the state). Asterisks indicate invasive species, and triangles on the timeline indicates the time of invasion.")`


# Get P/B for each group

```{r read growth parameters }
gp <- read.delim(paste0(getpath4data(), "Growth_parameters.txt"))
```


Production rate P/B is defined at steady state as equal to total mortality (Z), the sum of fishing mortality (F) and natural mortality (M) (Pauly et al. 2000^[Pauly, D., Christensen, V., Walters, C., 2000. Ecopath, Ecosim, and Ecospace as tools for evaluating ecosystem impact of fisheries. ICES J. Mar. Sci. 57, 697–706. https://doi.org/10.1006/jmsc.2000.0726]).

In Lake Champlain, fishing mortality is really low (F ~ 0).  M, in fishes, is positively correlated to their value of K (from the von Bertalanffy growth formula), followed by the temperature of their environment, and in a lesser extent, inversely correlated to their asymptotic length (Pauly 1980). We calculated M following the equation below, that was established from 175 marine fish stocks (Pauly 1980^[Pauly, D., 1980. On the interrelationships between natural mortality, growth parameters, and mean environmental temperature in 175 fish stocks. J. Const. int. Explor. Mer 39, 175–192.]).

$log(M)= -0.00066 - 0.279 * log(L_∞) + 0.6543*log(K) + 0.4634*log(T)$

T for now is defined as 14°C because we don't have data for winter in Lake Champlain, so I just need to get a data.

```{r get p/b}
# using some parameters from growth parameter
Tenv = 14

getM <- function(Lasympt, K, Tenv) {
  10^(-0.00066-0.279*log10(Lasympt)+0.6543*log10(K)+0.4634*log10(Tenv))
}
  
gp$Group.name
M_sea_lamprey = getM(Lasympt = 69, K = 0.16, Tenv = Tenv)
M_lake_trout_ad = getM(Lasympt = 74.6, K = 0.232, Tenv = Tenv) # K was obtained from regression done for Lake Champlain fish (Grace and Ellen). L∞=746mm
M_lake_trout_juv = getM(Lasympt = 23, K = 0.232, Tenv = Tenv)
M_lake_trout_stk = getM(Lasympt = 5, K = 0.232, Tenv = Tenv)
M_atlantic_salmon = getM(Lasympt = 150, K = 0.15, Tenv = Tenv)
M_burbot = getM(Lasympt = 117, K = 0.05, Tenv = Tenv)
M_walleye = getM(Lasympt = 107, K = 0.2, Tenv = Tenv)
M_cisco = getM(Lasympt = 57.000, K = 0.16, Tenv = Tenv)
M_whitefish = getM(Lasympt = 79.000, K = 0.12, Tenv = Tenv)
M_trout_perch = getM(Lasympt = 15, K = 0.05, Tenv = Tenv)
M_alewife = getM(Lasympt = 38.100, K = 0.2, Tenv = Tenv)
M_smelt = getM(Lasympt = 33, K = 0.31, Tenv = Tenv)
M_sculpin = getM(Lasympt = 12.7, K = NA, Tenv = Tenv)

M_fish <- data.frame("sea_lamprey"= c(M_sea_lamprey),
            "lake_trout_ad" = M_lake_trout_ad,
            "lake_trout_juv" = M_lake_trout_juv,
            "lake_trout_stk" = M_lake_trout_stk,
            "atlantic_salmon"= M_atlantic_salmon,
            "burbot" = M_burbot,
            "walleye" = M_walleye,
            "cisco" = M_cisco,
            "whitefish" = M_whitefish,
            "trout_perch"= M_trout_perch,
            "alewife"=M_alewife,
            "smelt"=M_smelt,
            "sculpin"=M_sculpin)

if(!R_U_KNITTING) write.table(M_fish, file=paste0(getpath4data(),"EwE_params/M_fish.txt"), sep="\t")

M_fish
```


```{r get P/B for various temperatures, echo=FALSE}
# using some parameters from growth parameter
Tenvt = seq(12,16,by=0.1)

for (i in 1:length(Tenvt)) {
  Tenv <- Tenvt[i]
  
  M_sea_lamprey = getM(Lasympt = 69, K = 0.16, Tenv = Tenv)
  M_lake_trout_ad = getM(Lasympt = 74.6, K = 0.232, Tenv = Tenv) # K was obtained from regression done for Lake Champlain fish (Grace and Ellen). L∞=746mm
  M_lake_trout_juv = getM(Lasympt = 23, K = 0.232, Tenv = Tenv)
  M_lake_trout_stk = getM(Lasympt = 5, K = 0.232, Tenv = Tenv)
  M_atlantic_salmon = getM(Lasympt = 150, K = 0.15, Tenv = Tenv)
  M_burbot = getM(Lasympt = 117, K = 0.05, Tenv = Tenv)
  M_walleye = getM(Lasympt = 107, K = 0.2, Tenv = Tenv)
  M_cisco = getM(Lasympt = 57.000, K = 0.16, Tenv = Tenv)
  M_whitefish = getM(Lasympt = 79.000, K = 0.12, Tenv = Tenv)
  M_trout_perch = getM(Lasympt = 15, K = 0.05, Tenv = Tenv)
  M_alewife = getM(Lasympt = 38.100, K = 0.2, Tenv = Tenv)
  M_smelt = getM(Lasympt = 33, K = 0.31, Tenv = Tenv)
  M_sculpin = getM(Lasympt = 12.7, K = NA, Tenv = Tenv)
  
  if (i==1) M_fish2 <- data.frame("T_env" = Tenv,
              "sea_lamprey"= M_sea_lamprey,
              "lake_trout_ad" = M_lake_trout_ad,
              "lake_trout_juv" = M_lake_trout_juv,
              "lake_trout_stk" = M_lake_trout_stk,
              "atlantic_salmon"= M_atlantic_salmon,
              "burbot" = M_burbot,
              "walleye" = M_walleye,
              "cisco" = M_cisco,
              "whitefish" = M_whitefish,
              "trout_perch"= M_trout_perch,
              "alewife"=M_alewife,
              "smelt"=M_smelt,
              "sculpin"=M_sculpin) else
                 M_fish2 <- rbind(M_fish2,
                                c(Tenv,M_sea_lamprey,M_lake_trout_ad, M_lake_trout_juv, M_lake_trout_stk, M_atlantic_salmon,M_burbot, M_walleye,M_cisco,M_whitefish,M_trout_perch,M_alewife, M_smelt,M_sculpin))

}

M_fish3 <- cbind("T_env"=M_fish2[,1],data.frame(lapply(M_fish2[,-1], function(X) X/X[1])))

ggplot(M_fish2, aes(T_env, lake_trout_ad/REco2$BB[REco2$Group=="Adult lake trout"])) + geom_point() +
  geom_point(aes(T_env, alewife/REco2$BB[REco2$Group=="Alewife"]), col="blue") +
  geom_point(aes(T_env, atlantic_salmon/REco2$BB[REco2$Group=="Atlantic salmon"]), col="pink") + labs(x="T environment",y="Mortality/Biomass") + theme_bw()

```
`r fig_cap("varying T env impact on PB fish", caption="Mortality (pondered by biomass) as a function of water temperature, for 3 groups in the food web.")`

## P/B for zooplankton (Stockwell and Johannsson 1997^[Stockwell, J.D., Johannsson, O.E., 1997. Temperature-dependent allometric models to estimate zooplankton production in temperate freshwater lakes. Can. J. Fish. Aquat. Sci. 54, 2350–2360. https://doi.org/10.1139/f97-141])

Found the method to get a better estimate of P/B in the supplementary materials B of Zhang et al 2016 paper on Lake Erie^[Zhang, H., Rutherford, E.S., Mason, D.M., Breck, J.T., Wittmann, M.E., Cooke, R.M., Lodge, D.M., Rothlisberger, J.D., Zhu, X., Johnson, T.B., 2016. Forecasting the Impacts of Silver and Bighead Carp on the Lake Erie Food Web. Transactions of the American Fisheries Society 145, 136–162. https://doi.org/10.1080/00028487.2015.1069211].

To quote them:
"P/B values were calculated based on published relationships between temperature and production (Shuter and Ing 1997; Stockwell and Johannsson 1997). Specifically, for non-predatory cladocerans, P/B=0.162/d when temperature was >10°C, and 0.042/d when temperature was <10°C (Stockwell and Johannsson 1997); for predatory cladocerans, P/B is a function of body weight and water temperature:
$log(dailyP/B) = -0.23*log(dry_{wt}(µg))-0.73$, when mean seasonal temperature > 10, and $log(dailyP/B) = -0.26*log(dry_{wt}(µg))-1.36$ when mean seasonal temperature < 10  ̊C. For copepods and rotifer, P/B is calculated as: $log(median_{daily}P/B) = A + 0.04336*(medianTemperature(°C))$, where A = -1.844 for cyclopoids, -2.294 for calanoids, and -1.631 for rotifers (Shuter and Ing 1997). We used water temperature values provided from output of a 1-dimensional model of Lake Erie water temperature by Rucinski et al. (2010). Q/B values were calculated based on gross growth efficiency reported by Straile (1997). Diet for most zooplankton groups was obtained from Vanderploeg (1994), but for Bythotrephes spp was obtained from Vanderploeg et al. (1993)."

```{r get P/B for for zooplankton}
dailyPB <- function(drywt, mean_seasonal_temperature) {
  # dry weight in µg
  # mean seasonal temperature in °C
  if(mean_seasonal_temperature>10) {
    return(10^(-0.23*log10(drywt)-0.73))
    } else {
    return(10^(-0.26*log10(drywt)-1.36))
  }
}

dailyPB_test <- data.frame("drywt" = seq(0.01,2,0.01),
                           "temp11"= dailyPB(seq(0.01,2,0.01),11),
                           "temp9" = dailyPB(seq(0.01,2,0.01),9)) 
ggplot(dailyPB_test) + 
  geom_line(aes(drywt,temp11, colour=">10"), show.legend = T) +
  geom_line(aes(drywt,temp9, colour="<=10"), show.legend = T) +
  labs(x="Dry weight (µg)", y="daily P/B") +
  theme_bw() + 
  theme(
    legend.position = c(.95, .95),
    legend.justification = c("right", "top"),
    legend.box.just = "right",
    legend.margin = margin(6, 6, 6, 6)
    ) +
  scale_colour_manual(name="Mean seasonal temperature (°C)", values=c(">10"="coral1", "<=10"="black"))


```


## Export P/B

```{r export P/B}
# if(!R_U_KNITTING) write.table(x, file=paste0(getpath4data(),"EwE_params/P_B.txt"), sep="\t")
```


# Get Q/B for each group


Consumption rate Q/B is a function of the asymptotic length (W∞, g), the mean annual temperature of the water body (T’=1000/T, with T in Kelvin, Tcelsius + 273.15), the aspect ratio of the caudal fin (A), h and d. <br>

$log(Q⁄B)= 7.964-0.204 log(W_∞ )-1.065 T'+0.0873 A+0.532 h + 0.398 d$ <br>
with $A=  h^2⁄s$ <br>
and $T'=  1000⁄(Tcelsius+273.15)$ <br>


A is an approximation of the swimming and metabolic activity of the fish (Palomares and Pauly 1998^[Palomares, M.L.D., Pauly, D., 1998. Predicting food consumption of fish populations as functions of mortality, food type, morphometrics, temperature and salinity. Mar. Freshwater Res. 49, 447. https://doi.org/10.1071/MF98015]).

<a href="https://www.fishbase.se/manual/fishbasethe_swimming_and_speed_tables.htm">About aspect ratio</a>.

```{r get q/b}
# using some parameters from growth parameter
Tk = 1000/(Tenv+273.15)


getQB <- function(Wasympt, Tk, A, h, d) {
  7.964-0.204 * log(Wasympt)-1.065 * Tk + 0.0873 * A+0.532 *h+0.398*d
}
  
gp$Group.name
QB_sea_lamprey = getQB(Wasympt = 2.5, Tk=Tk, A = 0.81, h = 0, d = 0)
QB_lake_trout_ad = getQB(Wasympt = 32, Tk=Tk, A = 2.11, h =0, d =  0)
QB_lake_trout_juv = getQB(Wasympt = 1.2, Tk=Tk, A = 2.11, h =0, d = 0)
QB_lake_trout_stk = getQB(Wasympt = 0.2, Tk=Tk, A = 2.11, h =0, d = 0)
QB_atlantic_salmon = getQB(Wasympt = 35, Tk=Tk, A = 2.02, h =0, d =0)
QB_walleye = getQB(Wasympt = 11.3, Tk=Tk, A = 1.29, h = 0, d = 0)
QB_burbot = getQB(Wasympt = 34, Tk=Tk, A = 0.73, h = 0, d = 0)
QB_cisco = getQB(Wasympt = 3.6, Tk=Tk, A = 2.73, h = 0, d = 0)
QB_whitefish = getQB(Wasympt = 19, Tk=Tk, A = 1.80, h = 0, d = 0)
QB_trout_perch = getQB(Wasympt = .058, Tk=Tk, A = 1.35, h = 0, d = 0)
QB_alewife = getQB(Wasympt = 0.2, Tk=Tk, A = 1.76, h = 0, d = 0)
QB_smelt = getQB(Wasympt = 0.363, Tk=Tk, A = 2.34, h = 0, d = 0)
QB_sculpin = getQB(Wasympt = 0.029, Tk=Tk, A = 0.89, h = 0, d = 0)

QB_fish <- data.frame("sea_lamprey"= c(QB_sea_lamprey),
            "lake_trout_ad" = QB_lake_trout_ad,
            "lake_trout_juv" = QB_lake_trout_juv,
            "lake_trout_stk" = QB_lake_trout_stk,
            "atlantic_salmon"= QB_atlantic_salmon,
            "burbot" = QB_burbot,
            "walleye" = QB_walleye,
            "cisco" = QB_cisco,
            "whitefish" = QB_whitefish,
            "trout_perch"= QB_trout_perch,
            "alewife"=QB_alewife,
            "smelt"=QB_smelt,
            "sculpin"=QB_sculpin)

if(!R_U_KNITTING) write.table(QB_fish, file=paste0(getpath4data(),"EwE_params/QB_fish.txt"), sep="\t")

QB_fish
```



